{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 522 entries, 0 to 521\n",
      "Data columns (total 24 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   tweet_id                  522 non-null    int64  \n",
      " 1   reply_reply_count         522 non-null    float64\n",
      " 2   reply_like_count          522 non-null    float64\n",
      " 3   reply_retweet_count       522 non-null    float64\n",
      " 4   reply_quote_count         522 non-null    float64\n",
      " 5   reply_possibly_sensitive  522 non-null    float64\n",
      " 6   reply_has_url             522 non-null    float64\n",
      " 7   reply_mentioned_url_num   522 non-null    float64\n",
      " 8   reply_id_num              522 non-null    float64\n",
      " 9   reply_isweekday           522 non-null    float64\n",
      " 10  reply_senti_score         522 non-null    float64\n",
      " 11  reply_count               522 non-null    float64\n",
      " 12  like_count                522 non-null    float64\n",
      " 13  retweet_count             522 non-null    float64\n",
      " 14  quote_count               522 non-null    float64\n",
      " 15  possibly_sensitive        522 non-null    float64\n",
      " 16  has_url                   522 non-null    float64\n",
      " 17  mentioned_url_num         522 non-null    float64\n",
      " 18  id_num                    522 non-null    float64\n",
      " 19  isweekday                 522 non-null    float64\n",
      " 20  followers_count           522 non-null    float64\n",
      " 21  tweet_count               522 non-null    float64\n",
      " 22  verified                  522 non-null    float64\n",
      " 23  senti_score               522 non-null    float64\n",
      "dtypes: float64(23), int64(1)\n",
      "memory usage: 98.0 KB\n"
     ]
    }
   ],
   "source": [
    "train_stat = pd.read_csv('./data/train_stat_feat_df.csv')\n",
    "dev_stat = pd.read_csv('./data/dev_stat_feat_df.csv')\n",
    "dev_stat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 522 entries, 0 to 521\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweet_id    522 non-null    int64 \n",
      " 1   text        522 non-null    object\n",
      " 2   created_at  522 non-null    object\n",
      " 3   user_id     522 non-null    int64 \n",
      " 4   tweet_id.1  522 non-null    int64 \n",
      " 5   label       522 non-null    int64 \n",
      " 6   reply       522 non-null    object\n",
      " 7   reply_text  518 non-null    object\n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 32.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_tweet = pd.read_csv('./data/train_tweet_df.csv')\n",
    "dev_tweet = pd.read_csv('./data/dev_tweet_df.csv')\n",
    "dev_tweet.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, data_type, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        # read pre-processed data\n",
    "        self.tweet_df = pd.read_csv(f'./data/{data_type}_tweet_df.csv', usecols=['text', 'reply_text', 'label'])\n",
    "        self.statistic_df = pd.read_csv(f'./data/{data_type}_stat_feat_df.csv')\n",
    "        self.tweet_df['text'] = self.tweet_df['text'].replace(np.nan, '')\n",
    "        self.tweet_df['reply_text'] = self.tweet_df['reply_text'].replace(np.nan, '')\n",
    "        # define tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "    def __len__(self):\n",
    "        return self.tweet_df.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #Selecting the sentence and label at the specified index in the data frame\n",
    "        sentence = ['[CLS]'] + + self.tweet_df.iloc[index]['text'] + ['[SEP]'] + self.tweet_df.iloc[index]['reply_text']\n",
    "        label = self.tweet_df.loc[index, 'label']\n",
    "\n",
    "        #Preprocessing the text to be suitable for BERT\n",
    "        tokens = self.tokenizer.tokenize(sentence) #Tokenize the sentence\n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
    "\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
    "\n",
    "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attn_mask = [1 if token != '[PAD]' else 0 for token in tokens]\n",
    "        return tokens_ids_tensor, attn_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 208k/208k [00:02<00:00, 85.7kB/s] \n",
      "Downloading: 100%|██████████| 426k/426k [00:03<00:00, 109kB/s]  \n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(TweetDataset('train', 200), shuffle=True, batch_size=20, drop_last=True)\n",
    "dev_loader = DataLoader(TweetDataset('dev', 200), shuffle=True, batch_size=20, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RumorClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RumorClassifier, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        self.ffnn = nn.Sequential(nn.Linear(791,128),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3),\n",
    "                                 nn.Linear(128,64),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3),\n",
    "                                  nn.Linear(64,1),\n",
    "                                  nn.Sigmoid()\n",
    "                                 )\n",
    "\n",
    "    def forward(self, seq, attn_masks, seg, stats):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "        \n",
    "        x = torch.cat((cls_rep,stats),dim=1)\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.ffnn(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = logits.unsqueeze(-1)\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "def evaluate(net, criterion, dataloader, device):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "    #[FOR TPU] Using ParalellLoader\n",
    "    dataloader = pl.ParallelLoader(dataloader, [device])\n",
    "    dataloader = dataloader.per_device_loader(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, labels in dataloader:\n",
    "            #[FOR GPU] Converting these to cuda tensors\n",
    "            #seq, attn_masks, labels = seq.cuda(device), attn_masks.cuda(device), labels.cuda(device)\n",
    "            logits = net(seq, attn_masks)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(net, criterion, opti, train_loader, dev_loader, max_eps, device):\n",
    "\n",
    "    best_acc = 0\n",
    "    st = time.time()\n",
    "    for ep in range(max_eps):\n",
    "        \n",
    "        net.train()\n",
    "        # [FOR TPU] Using ParalellLoader\n",
    "        train_loader = DataLoader(TweetDataset('train', 200), shuffle=True, batch_size=20, drop_last=True)\n",
    "        dev_loader2 = pl.ParallelLoader(dev_loader, [device])\n",
    "        dev_loader2 = dev_loader2.per_device_loader(device)\n",
    "        \n",
    "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
    "            #Clear gradients\n",
    "            opti.zero_grad()  \n",
    "\n",
    "            #[FOR GPU] Converting these to cuda tensors\n",
    "            #seq, attn_masks, labels = seq.cuda(device), attn_masks.cuda(device), labels.cuda(device)\n",
    "\n",
    "            #Obtaining the logits from the model\n",
    "            logits = net(seq, attn_masks)\n",
    "\n",
    "            #Computing loss\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "\n",
    "            #Backpropagating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #[FOR GPU] Optimization step\n",
    "            #opti.step()\n",
    "\n",
    "            #[FOR TPU] Optimization step\n",
    "            xm.optimizer_step(opti)\n",
    "\n",
    "            if it % 10 == 0:\n",
    "                #Please remove [xla:{}] and xm.get_ordinal() if you want to run with GPU\n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                print(\"[xla:{}] Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(xm.get_ordinal(), it, ep, loss.item(), acc, (time.time()-st)))\n",
    "                st = time.time()\n",
    "     \n",
    "        dev_acc, dev_loss = evaluate(net, criterion, dev_loader2, device)\n",
    "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
    "        if dev_acc > best_acc:\n",
    "            print(\"Best development accuracy improved from {} to {}, saving model...\".format( best_acc, dev_acc))\n",
    "            best_acc = dev_acc\n",
    "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41027df0d37d918892da4b3bd3742540160af8137c6b072c04b94ef4f57ef28f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
