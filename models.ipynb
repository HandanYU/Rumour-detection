{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\81520\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\81520\\anaconda3\\lib\\site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\81520\\anaconda3\\lib\\site-packages (from transformers) (4.47.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 5.2 MB/s eta 0:00:00\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
      "     -------------------------------------- 895.2/895.2 KB 9.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\81520\\anaconda3\\lib\\site-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\81520\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\81520\\anaconda3\\lib\\site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: requests in c:\\users\\81520\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
      "     ---------------------------------------- 77.9/77.9 KB ? eta 0:00:00\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     -------------------------------------- 40.8/40.8 KB 649.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\81520\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\81520\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\81520\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\81520\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\81520\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: click in c:\\users\\81520\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\81520\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: six in c:\\users\\81520\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Installing collected packages: tokenizers, typing-extensions, sacremoses, packaging, huggingface-hub, transformers\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.2\n",
      "    Uninstalling typing-extensions-3.7.4.2:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.2\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.4\n",
      "    Uninstalling packaging-20.4:\n",
      "      Successfully uninstalled packaging-20.4\n",
      "Successfully installed huggingface-hub-0.5.1 packaging-21.3 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0 typing-extensions-4.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DistilBertTokenizerFast, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 522 entries, 0 to 521\n",
      "Data columns (total 24 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   tweet_id                  522 non-null    int64  \n",
      " 1   reply_reply_count         522 non-null    float64\n",
      " 2   reply_like_count          522 non-null    float64\n",
      " 3   reply_retweet_count       522 non-null    float64\n",
      " 4   reply_quote_count         522 non-null    float64\n",
      " 5   reply_possibly_sensitive  522 non-null    float64\n",
      " 6   reply_has_url             522 non-null    float64\n",
      " 7   reply_mentioned_url_num   522 non-null    float64\n",
      " 8   reply_id_num              522 non-null    float64\n",
      " 9   reply_isweekday           522 non-null    float64\n",
      " 10  reply_senti_score         522 non-null    float64\n",
      " 11  reply_count               522 non-null    float64\n",
      " 12  like_count                522 non-null    float64\n",
      " 13  retweet_count             522 non-null    float64\n",
      " 14  quote_count               522 non-null    float64\n",
      " 15  possibly_sensitive        522 non-null    float64\n",
      " 16  has_url                   522 non-null    float64\n",
      " 17  mentioned_url_num         522 non-null    float64\n",
      " 18  id_num                    522 non-null    float64\n",
      " 19  isweekday                 522 non-null    float64\n",
      " 20  followers_count           522 non-null    float64\n",
      " 21  tweet_count               522 non-null    float64\n",
      " 22  verified                  522 non-null    float64\n",
      " 23  senti_score               522 non-null    float64\n",
      "dtypes: float64(23), int64(1)\n",
      "memory usage: 98.0 KB\n"
     ]
    }
   ],
   "source": [
    "train_stat = pd.read_csv('./data/train_stat_feat_df.csv')\n",
    "dev_stat = pd.read_csv('./data/dev_stat_feat_df.csv')\n",
    "dev_stat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 522 entries, 0 to 521\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweet_id    522 non-null    int64 \n",
      " 1   text        522 non-null    object\n",
      " 2   created_at  522 non-null    object\n",
      " 3   user_id     522 non-null    int64 \n",
      " 4   tweet_id.1  522 non-null    int64 \n",
      " 5   label       522 non-null    int64 \n",
      " 6   reply       522 non-null    object\n",
      " 7   reply_text  518 non-null    object\n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 32.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_tweet = pd.read_csv('./data/train_tweet_df.csv')\n",
    "dev_tweet = pd.read_csv('./data/dev_tweet_df.csv')\n",
    "dev_tweet.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, data_type, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # read pre-processed data\n",
    "        self.tweet_df = pd.read_csv(f'./data/{data_type}_tweet_df.csv', usecols=['text', 'reply_text', 'label'])\n",
    "        self.statistic_df = pd.read_csv(f'./data/{data_type}_stat_feat_df.csv')\n",
    "        self.tweet_df['text'] = self.tweet_df['text'].replace(np.nan, '')\n",
    "        self.tweet_df['reply_text'] = self.tweet_df['reply_text'].replace(np.nan, '')\n",
    "        # define tokenizer\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    def __len__(self):\n",
    "        return self.tweet_df.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        source_token_mask = self.tokenizer(self.tweet_df.iloc[idx]['text'], truncation=True, padding='max_length', max_length=self.max_seq_len)\n",
    "        source_token, source_mask = torch.tensor(source_token_mask['input_ids']), torch.tensor(source_token_mask['attention_mask'])\n",
    "        pair_token_mask = self.tokenizer(self.tweet_df.iloc[idx]['text'], self.tweet_df.iloc[idx]['reply_text'], truncation='only_second', padding='max_length', max_length=self.max_seq_len)\n",
    "        pair_tokens_tensor, pair_mask_tensor = torch.tensor(pair_token_mask['input_ids']), torch.tensor(pair_token_mask['attention_mask'])\n",
    "        return source_token, source_mask, pair_tokens_tensor, pair_mask_tensor, self.tweet_df.iloc[idx]['label'], torch.tensor(self.statistic_df.iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 4.64kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 253kB/s]  \n",
      "Downloading: 100%|██████████| 455k/455k [00:01<00:00, 402kB/s]  \n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 70.7kB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(TweetDataset('train', 200), shuffle=True, batch_size=20, drop_last=True)\n",
    "dev_loader = DataLoader(TweetDataset('dev', 200), shuffle=True, batch_size=20, drop_last=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41027df0d37d918892da4b3bd3742540160af8137c6b072c04b94ef4f57ef28f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
