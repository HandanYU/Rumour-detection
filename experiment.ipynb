{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a00ed975-afc9-4cbe-89e9-651cd7187fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# import sampler as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108ce5df-3d00-451b-8a38-48e2bf0a8e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reply_contributors', 'reply_possibly_sensitive_appealable', 'reply_favorited', 'reply_retweeted', 'reply_protected', 'reply_contributors_enabled', 'reply_is_translator', 'reply_following', 'reply_follow_request_sent', 'reply_notifications', 'possibly_sensitive_appealable', 'favorited', 'retweeted', 'protected', 'contributors_enabled', 'is_translator', 'following', 'follow_request_sent', 'notifications']\n",
      "['reply_contributors', 'reply_possibly_sensitive_appealable', 'reply_favorited', 'reply_retweeted', 'reply_protected', 'reply_contributors_enabled', 'reply_is_translator', 'reply_following', 'reply_follow_request_sent', 'reply_notifications', 'possibly_sensitive_appealable', 'favorited', 'retweeted', 'protected', 'contributors_enabled', 'is_translator', 'following', 'follow_request_sent', 'notifications']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>reply_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1240727985491193862</td>\n",
       "      <td>covid viru transmit area hot humid world healt...</td>\n",
       "      <td>humid good demonstr virus surviv temperatur hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>634943791934406657</td>\n",
       "      <td>marilyn monro jame dean smoke new york citi http</td>\n",
       "      <td>icon [SEP] http [SEP] yo puedo demostrar que e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1243967693297987584</td>\n",
       "      <td>symptom covid jcinigeria</td>\n",
       "      <td>symptom usual mild gradual common [SEP] infect...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1233175449980874752</td>\n",
       "      <td>coronaviru wear mask protect covid</td>\n",
       "      <td>thank [SEP]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1245592346344841216</td>\n",
       "      <td>symptom covid let watch new episod q covid know</td>\n",
       "      <td>infect peopl around world believ togeth stop [...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1240727985491193862  covid viru transmit area hot humid world healt...   \n",
       "1   634943791934406657   marilyn monro jame dean smoke new york citi http   \n",
       "2  1243967693297987584                           symptom covid jcinigeria   \n",
       "3  1233175449980874752                 coronaviru wear mask protect covid   \n",
       "4  1245592346344841216    symptom covid let watch new episod q covid know   \n",
       "\n",
       "                                          reply_text  label  \n",
       "0  humid good demonstr virus surviv temperatur hi...      0  \n",
       "1  icon [SEP] http [SEP] yo puedo demostrar que e...      1  \n",
       "2  symptom usual mild gradual common [SEP] infect...      0  \n",
       "3                                       thank [SEP]       0  \n",
       "4  infect peopl around world believ togeth stop [...      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stat = pd.read_csv('./sep_data/train_scaled_stat_feat_df.csv')\n",
    "dev_stat = pd.read_csv('./sep_data/dev_scaled_stat_feat_df.csv')\n",
    "\n",
    "train_tweet = pd.read_csv('./sep_data/train_tweet_df.csv')\n",
    "dev_tweet = pd.read_csv('./sep_data/dev_tweet_df.csv')\n",
    "\n",
    "# train_stat.info()\n",
    "\n",
    "# dev_stat.info()\n",
    "\n",
    "train_stat.drop(columns=['Unnamed: 0','label'], inplace=True)\n",
    "train_stat.head()\n",
    "\n",
    "dev_stat.drop(columns=['Unnamed: 0','label'], inplace=True)\n",
    "dev_stat.head()\n",
    "\n",
    "train_stat.sum()\n",
    "\n",
    "dev_zero = []\n",
    "for column in dev_stat.columns:\n",
    "    if dev_stat[column].sum() == 0:\n",
    "        dev_zero.append(column)\n",
    "print(dev_zero)\n",
    "\n",
    "train_zero = []\n",
    "for column in train_stat.columns:\n",
    "    if train_stat[column].sum() == 0:\n",
    "        train_zero.append(column)\n",
    "print(train_zero)\n",
    "\n",
    "train_stat.drop(columns=train_zero, inplace=True)\n",
    "dev_stat.drop(columns=dev_zero, inplace=True)\n",
    "\n",
    "dev_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d9d7b-b4cc-4144-8a0b-ac2e1f50d586",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fill NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9ddbb0-fe88-4ff9-9fc3-ea6ee1e85a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 536 entries, 0 to 535\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweet_id    536 non-null    int64 \n",
      " 1   text        536 non-null    object\n",
      " 2   reply_text  536 non-null    object\n",
      " 3   label       536 non-null    int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 16.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# dev_tweet.info()\n",
    "\n",
    "# train_tweet.info()\n",
    "\n",
    "dev_tweet.reply_text.fillna('', inplace=True)\n",
    "train_tweet.reply_text.fillna('', inplace=True)\n",
    "dev_tweet.text.fillna('', inplace=True)\n",
    "train_tweet.text.fillna('', inplace=True)\n",
    "\n",
    "dev_tweet.info()\n",
    "\n",
    "# dev_tweet.iloc[0].reply_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d11469-56b1-4ba0-b50f-96208aabf747",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a78af90-9ac2-4e12-9d87-03aae7cdf566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/536 [00:00<?, ?it/s]C:\\Users\\trist\\AppData\\Local\\Temp\\ipykernel_10556\\3296011167.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dev_tweet.text.iloc[i] = '[CLS] ' + str(dev_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(dev_tweet.reply_text.iloc[i]).strip() + ' [SEP]'\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 536/536 [00:08<00:00, 62.30it/s]\n",
      "  0%|                                                                                         | 0/1579 [00:00<?, ?it/s]C:\\Users\\trist\\AppData\\Local\\Temp\\ipykernel_10556\\3296011167.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_tweet.text.iloc[i] = '[CLS] ' + str(train_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(train_tweet.reply_text.iloc[i]).strip() + ' [SEP]'\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1579/1579 [00:21<00:00, 74.92it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(dev_tweet))):\n",
    "    dev_tweet.text.iloc[i] = '[CLS] ' + str(dev_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(dev_tweet.reply_text.iloc[i]).strip() + ' [SEP]'\n",
    "\n",
    "for i in tqdm(range(len(train_tweet))):\n",
    "    train_tweet.text.iloc[i] = '[CLS] ' + str(train_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(train_tweet.reply_text.iloc[i]).strip() + ' [SEP]'\n",
    "\n",
    "dev_tweet.iloc[0].text\n",
    "\n",
    "from transformers import BertModel\n",
    "# bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee25759-322a-420b-8a7d-84b2be197b6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aadc52e-0850-448b-8282-6b35952d9d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 522/522 [02:50<00:00,  3.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1542/1542 [08:05<00:00,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 256\n",
    "dev_tokens = []\n",
    "train_tokens = []\n",
    "\n",
    "for i in tqdm(range(len(dev_tweet))):\n",
    "    txt = dev_tweet.text.iloc[i]\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    if len(tokens) < max_len:\n",
    "         padded_tokens = tokens + ['[PAD]' for _ in range(max_len - len(tokens))]\n",
    "    else:\n",
    "        padded_tokens = tokens[:max_len-1] + ['[SEP]']\n",
    "    attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "    seg_ids = [1 if token == '[SEP]' else 0 for token in padded_tokens]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "    token_ids_t = torch.tensor(token_ids).unsqueeze(0) #Shape : [1, 12]\n",
    "    attn_mask_t = torch.tensor(attn_mask).unsqueeze(0) #Shape : [1, 12]\n",
    "    seg_ids_t   = torch.tensor(seg_ids).unsqueeze(0) #Shape : [1, 12]\n",
    "    outputs = bert_model(token_ids_t, attention_mask = attn_mask_t,\\\n",
    "                                  token_type_ids = seg_ids_t, return_dict=True)\n",
    "    cont_reps = outputs.last_hidden_state\n",
    "    cls_rep = cont_reps[:, 0]\n",
    "    dev_tokens.append(cls_rep.detach().numpy())\n",
    "\n",
    "for i in tqdm(range(len(train_tweet))):\n",
    "    txt = train_tweet.text.iloc[i]\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    if len(tokens) < max_len:\n",
    "         padded_tokens = tokens + ['[PAD]' for _ in range(max_len - len(tokens))]\n",
    "    else:\n",
    "        padded_tokens = tokens[:max_len-1] + ['[SEP]']\n",
    "    attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "    seg_ids = [1 if token == '[SEP]' else 0 for token in padded_tokens]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "    token_ids_t = torch.tensor(token_ids).unsqueeze(0)\n",
    "    attn_mask_t = torch.tensor(attn_mask).unsqueeze(0)\n",
    "    seg_ids_t   = torch.tensor(seg_ids).unsqueeze(0)\n",
    "    outputs = bert_model(token_ids_t, attention_mask = attn_mask_t,\\\n",
    "                                  token_type_ids = seg_ids_t, return_dict=True)\n",
    "    cont_reps = outputs.last_hidden_state\n",
    "    cls_rep = cont_reps[:, 0]\n",
    "    train_tokens.append(cls_rep.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6fad0a-794c-4b6c-8958-07e9a4da423d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BERT seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9adfbd5-67e1-4334-8ccf-4cb9af1b7580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 536/536 [00:02<00:00, 266.95it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1579/1579 [00:05<00:00, 308.33it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 256\n",
    "dev_seq = []\n",
    "train_seq = []\n",
    "dev_mask = []\n",
    "train_mask = []\n",
    "dev_seg = []\n",
    "train_seg = []\n",
    "\n",
    "for i in tqdm(range(len(dev_tweet))):\n",
    "    txt = dev_tweet.text.iloc[i]\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    if len(tokens) < max_len:\n",
    "         padded_tokens = tokens + ['[PAD]' for _ in range(max_len - len(tokens))]\n",
    "    else:\n",
    "        padded_tokens = tokens[:max_len-1] + ['[SEP]']\n",
    "    attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "    seg_ids = []\n",
    "    seg_idx = 0\n",
    "    for token in padded_tokens:\n",
    "        seg_ids.append(seg_idx)\n",
    "        if token == '[SEP]':\n",
    "            seg_idx += 1\n",
    "    # seg_ids = [1 if token == '[SEP]' else 0 for token in padded_tokens]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "    \n",
    "    dev_seq.append(token_ids)\n",
    "    dev_mask.append(attn_mask)\n",
    "    dev_seg.append(seg_ids)\n",
    "\n",
    "for i in tqdm(range(len(train_tweet))):\n",
    "    txt = train_tweet.text.iloc[i]\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    if len(tokens) < max_len:\n",
    "         padded_tokens = tokens + ['[PAD]' for _ in range(max_len - len(tokens))]\n",
    "    else:\n",
    "        padded_tokens = tokens[:max_len-1] + ['[SEP]']\n",
    "    attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "    seg_ids = []\n",
    "    seg_idx = 0\n",
    "    for token in padded_tokens:\n",
    "        seg_ids.append(seg_idx)\n",
    "        if token == '[SEP]':\n",
    "            seg_idx += 1\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "    \n",
    "    train_seq.append(token_ids)\n",
    "    train_mask.append(attn_mask)\n",
    "    train_seg.append(seg_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c85e1-9dd7-467c-a3f4-3d49fdbc43a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### test BERT adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14e6939-428d-4eb8-b857-e85c5999baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdTweetDataset(Data.Dataset):\n",
    "    def __init__(self, seq, mask, seg, y):\n",
    "        self.seq = torch.tensor(seq)\n",
    "        self.mask = torch.tensor(mask)\n",
    "        self.seg = torch.tensor(seg)\n",
    "        self.y = torch.tensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx],self.mask[idx],self.seg[idx], self.y[idx], idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c982771-f7de-4e1b-b6ca-25dc0b33d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_tweet['label']\n",
    "y_dev = dev_tweet['label']\n",
    "w_nonr = len(y_train)/(len(y_train)-y_train.sum())\n",
    "w_r = len(y_train)/(y_train.sum())\n",
    "weights = []\n",
    "for l in y_train:\n",
    "    if l == 0:\n",
    "        weights.append(w_nonr)\n",
    "    else:\n",
    "        weights.append(w_r)\n",
    "weights = torch.FloatTensor(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23186e80-a34a-4888-b065-20a04b64826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_set = AdTweetDataset(train_seq, train_mask, train_seg,y_train)\n",
    "dev_set = AdTweetDataset(dev_seq, dev_mask, dev_seg, y_dev)\n",
    "\n",
    "# sampler_s = sp.StratifiedSampler(class_vector=torch.from_numpy(np.array(y_train)), batch_size=64)\n",
    "train_sampler = Data.WeightedRandomSampler(weights, len(train_set), replacement=True)\n",
    "train_loader = Data.DataLoader(train_set, sampler=train_sampler,batch_size=64)\n",
    "dev_loader = Data.DataLoader(dev_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac4ee5f3-9a21-4305-9296-146e7ae607a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RumorClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RumorClassifier, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        self.ffnn = nn.Sequential(nn.Linear(811,128),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3),\n",
    "                                 nn.Linear(128,64),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3),\n",
    "                                  nn.Linear(64,1),\n",
    "                                  nn.Sigmoid()\n",
    "                                 )\n",
    "\n",
    "    def forward(self, seq, attn_masks, seg, stats):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_mask,\\\n",
    "                                  token_type_ids = seg, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "        \n",
    "        x = torch.cat((cls_rep,stats),dim=1)\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.ffnn(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a15bb73-449a-438b-9037-7e7db3321991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.empty_cache ()\n",
    "net = RumorClassifier()\n",
    "# net = net.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99dd698a-ed70-46b8-a860-bb755cba759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = logits.unsqueeze(-1)\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "def get_f1_from_logits(logits, labels):\n",
    "    preds = (logits > 0.5).astype(int)\n",
    "    p, r, f, _ = precision_recall_fscore_support(labels, preds, pos_label=1, average=\"binary\")\n",
    "    return f\n",
    "\n",
    "def evaluate(net, criterion, dataloader, device):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "    all_log = np.array([])\n",
    "    all_labels = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for seq, mask, seg, labels, idx in dataloader:\n",
    "            # seq, labels = seq.to(device), labels.to(device)\n",
    "            stats = np.array(train_stat)\n",
    "            stats = torch.tensor(stats[idx]).float()\n",
    "            #Obtaining the logits from the model\n",
    "            logits = net(seq, mask, seg, stats)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            # mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            \n",
    "            all_log = np.hstack((all_log, logits.squeeze()))\n",
    "            all_labels = np.hstack((all_labels, labels.numpy()))\n",
    "            count += 1\n",
    "        \n",
    "        f = get_f1_from_logits(all_log, all_labels)\n",
    "    return f, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9666cce-201a-4823-ba27-db02fc97e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "best_acc = 0\n",
    "st = time.time()\n",
    "eps = []\n",
    "t_loss = []\n",
    "d_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebeadff2-f94c-405b-a565-edc35d3313c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#Obtaining the logits from the model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 15\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#Computing loss\u001b[39;00m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39msqueeze(), labels\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mRumorClassifier.forward\u001b[1;34m(self, seq, attn_masks, seg, stats)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03mInputs:\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    -seq : Tensor of shape [B, T] containing token ids of sequences\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#Feeding the input to BERT model to obtain contextualized representations\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m cont_reps \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#Obtaining the representation of [CLS] head (the first token)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:969\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    965\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    967\u001b[0m \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m extended_attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_extended_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\modeling_utils.py:559\u001b[0m, in \u001b[0;36mModuleUtilsMixin.get_extended_attention_mask\u001b[1;34m(self, attention_mask, input_shape, device)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03mMakes broadcastable attention and causal masks so that future and masked tokens are ignored.\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    `torch.Tensor` The extended attention mask, with a the same dtype as `attention_mask.dtype`.\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[1;32m--> 559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    560\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m attention_mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attention_mask\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;66;03m# Provided a padding mask of dimensions [batch_size, seq_length]\u001b[39;00m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;66;03m# - if the model is a decoder, apply a causal mask in addition to the padding mask\u001b[39;00m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;66;03m# - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "for ep in range(20):\n",
    "    eps.append(ep)\n",
    "    net.train()\n",
    "    for it, (seq, mask, seg, labels,idx) in enumerate(train_loader):\n",
    "        \n",
    "        #Clear gradients\n",
    "        opti.zero_grad()\n",
    "        #Converting these to cuda tensors\n",
    "        # seq, mask, seg, labels = seq.to(device), mask.to(device), seg.to(device), labels.to(device)\n",
    "\n",
    "        stats = np.array(train_stat)\n",
    "        stats = torch.tensor(stats[idx]).float()\n",
    "        #Obtaining the logits from the model\n",
    "        print(mask.shape)\n",
    "        logits = net(seq, mask, seg, stats)\n",
    "        #Computing loss\n",
    "        loss = criterion(logits.squeeze(), labels.float())\n",
    "\n",
    "        #Backpropagating the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #Optimization step\n",
    "        opti.step()\n",
    "\n",
    "        if it % 10 == 0:\n",
    "\n",
    "            acc = get_accuracy_from_logits(logits, labels)\n",
    "            print(\"Iteration {} of epoch {} complete. \\n Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "        \n",
    "    dev_acc, dev_loss = evaluate(net, criterion, dev_loader, 'cpu')\n",
    "    t_loss.append(loss.item())\n",
    "    d_loss.append(dev_loss)\n",
    "    print(\"Development F1: {}; Development Loss: {}\".format(dev_acc, dev_loss))\n",
    "    torch.save(net.state_dict(), 'D:\\\\bertcls_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06fd0ef9-7d68-4f85-9619-5a712cf7468a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 20 complete. \n",
      " Loss: 0.019885603338479996; Accuracy: 1.0; Time taken (s): 246.2210726737976\n",
      "Iteration 10 of epoch 20 complete. \n",
      " Loss: 0.015068178065121174; Accuracy: 1.0; Time taken (s): 364.4286153316498\n",
      "Iteration 20 of epoch 20 complete. \n",
      " Loss: 0.014043761417269707; Accuracy: 1.0; Time taken (s): 456.5346965789795\n",
      "Development F1: 0.8034934497816594; Development Loss: 0.32976017230086857\n",
      "Iteration 0 of epoch 21 complete. \n",
      " Loss: 0.02222348563373089; Accuracy: 1.0; Time taken (s): 248.7010898590088\n",
      "Iteration 10 of epoch 21 complete. \n",
      " Loss: 0.020274026319384575; Accuracy: 1.0; Time taken (s): 303.29566860198975\n",
      "Iteration 20 of epoch 21 complete. \n",
      " Loss: 0.015691112726926804; Accuracy: 1.0; Time taken (s): 309.9355397224426\n",
      "Development F1: 0.8127853881278538; Development Loss: 0.2758317864499986\n",
      "Iteration 0 of epoch 22 complete. \n",
      " Loss: 0.08303241431713104; Accuracy: 0.984375; Time taken (s): 304.21868419647217\n",
      "Iteration 10 of epoch 22 complete. \n",
      " Loss: 0.015413559041917324; Accuracy: 1.0; Time taken (s): 457.6564712524414\n",
      "Iteration 20 of epoch 22 complete. \n",
      " Loss: 0.05906928703188896; Accuracy: 0.984375; Time taken (s): 304.9207911491394\n",
      "Development F1: 0.8270042194092827; Development Loss: 0.28376719179666704\n",
      "Iteration 0 of epoch 23 complete. \n",
      " Loss: 0.08762548118829727; Accuracy: 0.984375; Time taken (s): 247.13635396957397\n",
      "Iteration 10 of epoch 23 complete. \n",
      " Loss: 0.014156580902636051; Accuracy: 1.0; Time taken (s): 303.74406909942627\n",
      "Iteration 20 of epoch 23 complete. \n",
      " Loss: 0.01193959265947342; Accuracy: 1.0; Time taken (s): 390.6635046005249\n",
      "Development F1: 0.8255319148936171; Development Loss: 0.30597007233235574\n",
      "Iteration 0 of epoch 24 complete. \n",
      " Loss: 0.015071961097419262; Accuracy: 1.0; Time taken (s): 373.2922513484955\n",
      "Iteration 10 of epoch 24 complete. \n",
      " Loss: 0.16163405776023865; Accuracy: 0.96875; Time taken (s): 314.5331063270569\n",
      "Iteration 20 of epoch 24 complete. \n",
      " Loss: 0.014783065766096115; Accuracy: 1.0; Time taken (s): 304.26830315589905\n",
      "Development F1: 0.8154506437768241; Development Loss: 0.3153880031572448\n",
      "Iteration 0 of epoch 25 complete. \n",
      " Loss: 0.012907586991786957; Accuracy: 1.0; Time taken (s): 247.12117624282837\n",
      "Iteration 10 of epoch 25 complete. \n",
      " Loss: 0.08658713102340698; Accuracy: 0.984375; Time taken (s): 388.495600938797\n",
      "Iteration 20 of epoch 25 complete. \n",
      " Loss: 0.08797440677881241; Accuracy: 0.984375; Time taken (s): 444.62221026420593\n",
      "Development F1: 0.8220338983050848; Development Loss: 0.3235528651210997\n",
      "Iteration 0 of epoch 26 complete. \n",
      " Loss: 0.009531870484352112; Accuracy: 1.0; Time taken (s): 245.6775255203247\n",
      "Iteration 10 of epoch 26 complete. \n",
      " Loss: 0.10084151476621628; Accuracy: 0.984375; Time taken (s): 303.85618233680725\n",
      "Iteration 20 of epoch 26 complete. \n",
      " Loss: 0.014829369261860847; Accuracy: 1.0; Time taken (s): 303.41104912757874\n",
      "Development F1: 0.8070175438596492; Development Loss: 0.3228878262970183\n",
      "Iteration 0 of epoch 27 complete. \n",
      " Loss: 0.011221525259315968; Accuracy: 1.0; Time taken (s): 252.37632632255554\n",
      "Iteration 10 of epoch 27 complete. \n",
      " Loss: 0.011448283679783344; Accuracy: 1.0; Time taken (s): 304.37904691696167\n",
      "Iteration 20 of epoch 27 complete. \n",
      " Loss: 0.0925203412771225; Accuracy: 0.984375; Time taken (s): 302.5675551891327\n",
      "Development F1: 0.808695652173913; Development Loss: 0.3628538300593694\n",
      "Iteration 0 of epoch 28 complete. \n",
      " Loss: 0.010461912490427494; Accuracy: 1.0; Time taken (s): 245.39943170547485\n",
      "Iteration 10 of epoch 28 complete. \n",
      " Loss: 0.10049627721309662; Accuracy: 0.984375; Time taken (s): 302.95568680763245\n",
      "Iteration 20 of epoch 28 complete. \n",
      " Loss: 0.013972880318760872; Accuracy: 1.0; Time taken (s): 303.3659427165985\n",
      "Development F1: 0.8135593220338982; Development Loss: 0.355592323674096\n",
      "Iteration 0 of epoch 29 complete. \n",
      " Loss: 0.012208727188408375; Accuracy: 1.0; Time taken (s): 248.34082889556885\n",
      "Iteration 10 of epoch 29 complete. \n",
      " Loss: 0.009290837682783604; Accuracy: 1.0; Time taken (s): 304.03312253952026\n",
      "Iteration 20 of epoch 29 complete. \n",
      " Loss: 0.01629195362329483; Accuracy: 1.0; Time taken (s): 309.3891339302063\n",
      "Development F1: 0.8211382113821138; Development Loss: 0.354914559258355\n",
      "Iteration 0 of epoch 30 complete. \n",
      " Loss: 0.009985179640352726; Accuracy: 1.0; Time taken (s): 246.72114539146423\n",
      "Iteration 10 of epoch 30 complete. \n",
      " Loss: 0.08295148611068726; Accuracy: 0.984375; Time taken (s): 302.49599862098694\n",
      "Iteration 20 of epoch 30 complete. \n",
      " Loss: 0.010746770538389683; Accuracy: 1.0; Time taken (s): 304.997713804245\n",
      "Development F1: 0.8; Development Loss: 0.3106331398917569\n",
      "Iteration 0 of epoch 31 complete. \n",
      " Loss: 0.012927914038300514; Accuracy: 1.0; Time taken (s): 245.40931391716003\n",
      "Iteration 10 of epoch 31 complete. \n",
      " Loss: 0.01420043595135212; Accuracy: 1.0; Time taken (s): 303.50465416908264\n",
      "Iteration 20 of epoch 31 complete. \n",
      " Loss: 0.058363381773233414; Accuracy: 0.984375; Time taken (s): 304.5951814651489\n",
      "Development F1: 0.8138528138528138; Development Loss: 0.33783677385913\n",
      "Iteration 0 of epoch 32 complete. \n",
      " Loss: 0.01577906683087349; Accuracy: 1.0; Time taken (s): 246.53348898887634\n",
      "Iteration 10 of epoch 32 complete. \n",
      " Loss: 0.0076201907359063625; Accuracy: 1.0; Time taken (s): 303.21191334724426\n",
      "Iteration 20 of epoch 32 complete. \n",
      " Loss: 0.019136643037199974; Accuracy: 1.0; Time taken (s): 303.1140329837799\n",
      "Development F1: 0.8075117370892018; Development Loss: 0.3337654744585355\n",
      "Iteration 0 of epoch 33 complete. \n",
      " Loss: 0.009034951217472553; Accuracy: 1.0; Time taken (s): 247.39248204231262\n",
      "Iteration 10 of epoch 33 complete. \n",
      " Loss: 0.01830347068607807; Accuracy: 0.984375; Time taken (s): 309.5242328643799\n",
      "Iteration 20 of epoch 33 complete. \n",
      " Loss: 0.007760205306112766; Accuracy: 1.0; Time taken (s): 303.16456937789917\n",
      "Development F1: 0.8105726872246696; Development Loss: 0.36395369635687935\n",
      "Iteration 0 of epoch 34 complete. \n",
      " Loss: 0.006267976947128773; Accuracy: 1.0; Time taken (s): 247.36873483657837\n",
      "Iteration 10 of epoch 34 complete. \n",
      " Loss: 0.006056802812963724; Accuracy: 1.0; Time taken (s): 303.10969710350037\n",
      "Iteration 20 of epoch 34 complete. \n",
      " Loss: 0.007883936166763306; Accuracy: 1.0; Time taken (s): 304.3833019733429\n",
      "Development F1: 0.8209606986899564; Development Loss: 0.3815513253211975\n",
      "Iteration 0 of epoch 35 complete. \n",
      " Loss: 0.006764204241335392; Accuracy: 1.0; Time taken (s): 246.40640783309937\n",
      "Iteration 10 of epoch 35 complete. \n",
      " Loss: 0.008328605443239212; Accuracy: 1.0; Time taken (s): 303.22836923599243\n",
      "Iteration 20 of epoch 35 complete. \n",
      " Loss: 0.009615111164748669; Accuracy: 1.0; Time taken (s): 303.521719455719\n",
      "Development F1: 0.812227074235808; Development Loss: 0.3704833785692851\n",
      "Iteration 0 of epoch 36 complete. \n",
      " Loss: 0.007990188896656036; Accuracy: 1.0; Time taken (s): 245.58361196517944\n",
      "Iteration 10 of epoch 36 complete. \n",
      " Loss: 0.007527108769863844; Accuracy: 1.0; Time taken (s): 304.3862864971161\n",
      "Iteration 20 of epoch 36 complete. \n",
      " Loss: 0.007162157911807299; Accuracy: 1.0; Time taken (s): 302.77595829963684\n",
      "Development F1: 0.8214285714285714; Development Loss: 0.3426625066333347\n",
      "Iteration 0 of epoch 37 complete. \n",
      " Loss: 0.007065384648740292; Accuracy: 1.0; Time taken (s): 247.03540992736816\n",
      "Iteration 10 of epoch 37 complete. \n",
      " Loss: 0.009799385443329811; Accuracy: 1.0; Time taken (s): 304.599214553833\n",
      "Iteration 20 of epoch 37 complete. \n",
      " Loss: 0.0058921510353684425; Accuracy: 1.0; Time taken (s): 309.83567070961\n",
      "Development F1: 0.8230088495575221; Development Loss: 0.392632813917266\n",
      "Iteration 0 of epoch 38 complete. \n",
      " Loss: 0.006193756125867367; Accuracy: 1.0; Time taken (s): 244.7565999031067\n",
      "Iteration 10 of epoch 38 complete. \n",
      " Loss: 0.0037228900473564863; Accuracy: 1.0; Time taken (s): 303.10095715522766\n",
      "Iteration 20 of epoch 38 complete. \n",
      " Loss: 0.004564584232866764; Accuracy: 1.0; Time taken (s): 303.2550895214081\n",
      "Development F1: 0.8230088495575221; Development Loss: 0.3626023216380013\n",
      "Iteration 0 of epoch 39 complete. \n",
      " Loss: 0.0047602104023098946; Accuracy: 1.0; Time taken (s): 246.28305864334106\n",
      "Iteration 10 of epoch 39 complete. \n",
      " Loss: 0.007053263019770384; Accuracy: 1.0; Time taken (s): 304.2280476093292\n",
      "Iteration 20 of epoch 39 complete. \n",
      " Loss: 0.004444420803338289; Accuracy: 1.0; Time taken (s): 303.7257008552551\n",
      "Development F1: 0.8266666666666667; Development Loss: 0.3643243693643146\n"
     ]
    }
   ],
   "source": [
    "for ep in range(20,40):\n",
    "    eps.append(ep)\n",
    "    net.train()\n",
    "    for it, (seq, mask, seg, labels,idx) in enumerate(train_loader):\n",
    "        \n",
    "        #Clear gradients\n",
    "        opti.zero_grad()\n",
    "        #Converting these to cuda tensors\n",
    "        # seq, mask, seg, labels = seq.to(device), mask.to(device), seg.to(device), labels.to(device)\n",
    "        \n",
    "        stats = np.array(train_stat)\n",
    "        stats = torch.tensor(stats[idx]).float()\n",
    "        #Obtaining the logits from the model\n",
    "        logits = net(seq, mask, seg, stats)\n",
    "        #Computing loss\n",
    "        loss = criterion(logits.squeeze(), labels.float())\n",
    "\n",
    "        #Backpropagating the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #Optimization step\n",
    "        opti.step()\n",
    "\n",
    "        if it % 10 == 0:\n",
    "\n",
    "            acc = get_accuracy_from_logits(logits, labels)\n",
    "            print(\"Iteration {} of epoch {} complete. \\n Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "        \n",
    "    dev_acc, dev_loss = evaluate(net, criterion, dev_loader, 'cpu')\n",
    "    t_loss.append(loss.item())\n",
    "    d_loss.append(dev_loss)\n",
    "    print(\"Development F1: {}; Development Loss: {}\".format(dev_acc, dev_loss))\n",
    "    torch.save(net.state_dict(), 'D:\\\\bertcls_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f08207-d86c-4d92-8486-2b8609053dbb",
   "metadata": {},
   "source": [
    "### Optimize Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c12ece1-995d-4e76-9daa-5054a25b6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RumorEmbedder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RumorEmbedder, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        self.ffnn = nn.Sequential(nn.Linear(811,512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3),\n",
    "                                 nn.Linear(512,256)\n",
    "                                 )\n",
    "\n",
    "    def forward(self, seq, attn_masks, seg, stats):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "        \n",
    "        x = torch.cat((cls_rep,stats),dim=1)\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        embs = self.ffnn(x)\n",
    "\n",
    "        return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f909d01f-df02-4d24-9153-c73f485d02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a,b):\n",
    "    norm_a = torch.norm(a, dim=1)\n",
    "    norm_b = torch.norm(b, dim=0)\n",
    "    return torch.mm(a,b)/torch.mm(norm_a.unsqueeze(1), norm_b.unsqueeze(0))\n",
    "\n",
    "class IntraInterLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(IntraInterLoss, self).__init__()\n",
    "\n",
    "    def forward(self, emb, target):\n",
    "        nr_emb = emb[target==0]\n",
    "        r_emb = emb[target==1]\n",
    "        count_1_1 = r_emb.shape[0]**2\n",
    "        count_0_0 = nr_emb.shape[0]**2\n",
    "        count_0_1 = r_emb.shape[0]*nr_emb.shape[0]\n",
    "        r_cos = torch.sum(cos_sim(r_emb, r_emb.T))/float(count_1_1)\n",
    "        nr_cos = torch.sum(cos_sim(nr_emb, nr_emb.T))/float(count_0_0)\n",
    "        r_nr_cos = torch.sum(cos_sim(r_emb, nr_emb.T))/float(count_0_1)\n",
    "        \n",
    "        return r_nr_cos-0.4*r_cos-0.2*nr_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e08dca2-b5e1-4332-8eb8-b6065997e8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "dev_loader = Data.DataLoader(dev_set, batch_size=len(dev_set), shuffle=False)\n",
    "# torch.cuda.empty_cache ()\n",
    "net = RumorEmbedder()\n",
    "# net = net.to(device)\n",
    "\n",
    "criterion = IntraInterLoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 0.001)\n",
    "scheduler = lr_scheduler.ExponentialLR(opti, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ef172db-267c-474b-bc3b-f066e4989065",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 9 complete. \n",
      " Loss: 0.20478184521198273; Time taken (s): 39.78719925880432\n",
      "Iteration 10 of epoch 9 complete. \n",
      " Loss: -0.024878300726413727; Time taken (s): 371.62356066703796\n",
      "Iteration 20 of epoch 9 complete. \n",
      " Loss: -0.38400164246559143; Time taken (s): 380.68605637550354\n",
      "Development Loss: -0.34426164627075195\n",
      "Iteration 0 of epoch 10 complete. \n",
      " Loss: -0.6006417274475098; Time taken (s): 304.7210125923157\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#Backpropagating the gradients\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#Optimization step\u001b[39;00m\n\u001b[0;32m     23\u001b[0m opti\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "st = time.time()\n",
    "for ep in range(9,20):\n",
    "    net.train()\n",
    "    for it, (seq, mask, seg, labels,idx) in enumerate(train_loader):\n",
    "        \n",
    "        #Clear gradients\n",
    "        opti.zero_grad()\n",
    "        #Converting these to cuda tensors\n",
    "        # seq, mask, seg, labels = seq.to(device), mask.to(device), seg.to(device), labels.to(device)\n",
    "\n",
    "        stats = np.array(train_stat)\n",
    "        stats = torch.tensor(stats[idx]).float()\n",
    "        #Obtaining the logits from the model\n",
    "        logits = net(seq, mask, seg, stats)\n",
    "        #Computing loss\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        #Backpropagating the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #Optimization step\n",
    "        opti.step()\n",
    "\n",
    "        if it % 10 == 0:\n",
    "\n",
    "            print(\"Iteration {} of epoch {} complete. \\n Loss: {}; Time taken (s): {}\".format(it, ep, loss.item(), (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for seq, mask, seg, labels, idx in dev_loader:\n",
    "            dev_embs = net(seq, mask, seg, torch.FloatTensor(np.array(dev_stat)))\n",
    "        dev_loss = criterion(dev_embs, labels)\n",
    "    \n",
    "    print(\"Development Loss: {}\".format(dev_loss.item()))\n",
    "    torch.save(net.state_dict(), 'D:\\\\bertemb_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b984573-0d7f-4fee-b258-64d4dcb55dca",
   "metadata": {},
   "source": [
    "### BertEmbedding Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab7e35b7-3596-4851-8141-19a83c5b1f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = RumorEmbedder()\n",
    "net.load_state_dict(torch.load('D:\\\\bertemb_9.dat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5a43a41b-aa06-4f15-afd1-8b059a936afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = Data.DataLoader(dev_set, batch_size=len(dev_set), shuffle=False,sampler=range(0,len(dev_set)))\n",
    "train_loader = Data.DataLoader(train_set, batch_size=len(train_set), shuffle=False, sampler=range(0,len(train_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8a9f3bda-44cf-4e44-82be-34436683af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for seq, mask, seg, labels,idx in train_loader:\n",
    "        train_embs = net(seq, mask, seg, torch.FloatTensor(np.array(train_stat)))\n",
    "    for seq, mask, seg, labels, idx in dev_loader:\n",
    "        dev_embs = net(seq, mask, seg, torch.FloatTensor(np.array(dev_stat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "44a033b3-bdfc-4a1d-876c-ce16ce89d82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.35964912280701755  Recall:0.7130434782608696  F1:0.47813411078717205\n",
      "Accuracy: 0.6660447761194029\n"
     ]
    }
   ],
   "source": [
    "r_mean = train_embs[np.array(y_train) == 1].mean(dim=0).numpy()\n",
    "nr_mean = train_embs[np.array(y_train) == 0].mean(dim=0).numpy()\n",
    "dev_embs_n = dev_embs.numpy()\n",
    "dev_preds = []\n",
    "\n",
    "for emb in dev_embs_n:\n",
    "    r_sim = np.dot(emb, r_mean)/(np.linalg.norm(emb)*np.linalg.norm(r_mean))\n",
    "    nr_sim = np.dot(emb, nr_mean)/(np.linalg.norm(emb)*np.linalg.norm(nr_mean))\n",
    "    if r_sim >= nr_sim:\n",
    "        dev_preds.append(1)\n",
    "    else:\n",
    "        dev_preds.append(0)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_dev, dev_preds, pos_label=1, average=\"binary\")\n",
    "print('Precision:{}  Recall:{}  F1:{}'.format(p,r,f))\n",
    "print('Accuracy: {}'.format(accuracy_score(y_dev, dev_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d3bd204c-391d-49bb-a8c0-7834e6d67907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.3890675241157556  Recall:0.7610062893081762  F1:0.5148936170212767\n",
      "Accuracy: 0.7112096263457884\n"
     ]
    }
   ],
   "source": [
    "r_mean = train_embs[np.array(y_train) == 1].mean(dim=0).numpy()\n",
    "nr_mean = train_embs[np.array(y_train) == 0].mean(dim=0).numpy()\n",
    "dev_embs_n = dev_embs.numpy()\n",
    "dev_preds = []\n",
    "\n",
    "for emb in train_embs.numpy():\n",
    "    r_sim = np.dot(emb, r_mean)/(np.linalg.norm(emb)*np.linalg.norm(r_mean))\n",
    "    nr_sim = np.dot(emb, nr_mean)/(np.linalg.norm(emb)*np.linalg.norm(nr_mean))\n",
    "    if r_sim >= nr_sim:\n",
    "        dev_preds.append(1)\n",
    "    else:\n",
    "        dev_preds.append(0)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_train, dev_preds, pos_label=1, average=\"binary\")\n",
    "print('Precision:{}  Recall:{}  F1:{}'.format(p,r,f))\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, dev_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f023fad1-ef6b-49fd-9fe4-0a2a3403e624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.5463917525773195  Recall:0.4608695652173913  F1:0.5\n",
      "Accuracy: 0.8022388059701493\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(train_embs.numpy(), y_train)\n",
    "\n",
    "predictions = knn.predict(dev_embs_n)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_dev, predictions, pos_label=1, average=\"binary\")\n",
    "\n",
    "print('Precision:{}  Recall:{}  F1:{}'.format(p,r,f))\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(y_dev, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "68f94aeb-ef17-4fd6-9c5c-1e0f91004959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.6732673267326733  Recall:0.6181818181818182  F1:0.6445497630331753\n",
      "Accuracy: 0.8563218390804598\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('X_train.pkl','rb') as file:\n",
    "    X_train = pickle.load(file)\n",
    "with open('X_dev.pkl','rb') as file:\n",
    "    X_dev = pickle.load(file)\n",
    "with open('y_dev.pkl','rb') as file:\n",
    "    y_dev = pickle.load(file)\n",
    "with open('y_train.pkl','rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "predictions = knn.predict(X_dev)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_dev, predictions, pos_label=1, average=\"binary\")\n",
    "\n",
    "print('Precision:{}  Recall:{}  F1:{}'.format(p,r,f))\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(y_dev, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a0f4decd-3d35-49e4-a15f-7530f5858702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.6666666666666666  Recall:0.06956521739130435  F1:0.12598425196850394\n",
      "Accuracy: 0.792910447761194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trist\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter=100)\n",
    "lr.fit(train_embs.numpy(), y_train)\n",
    "\n",
    "predictions = lr.predict(dev_embs_n)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_dev, predictions, pos_label=1, average=\"binary\")\n",
    "\n",
    "print('Precision:{}  Recall:{}  F1:{}'.format(p,r,f))\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(y_dev, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82228213-cdd8-4f6e-8b83-6a7e611e175b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.559523  , -0.6694411 ,  0.6386693 , -0.3082815 , -1.4688318 ,\n",
       "        1.0080024 ,  0.76538223,  1.5122892 , -0.85284615,  1.7891976 ,\n",
       "       -1.4008505 , -0.55482763,  1.8608657 , -0.6717017 ,  0.97310126,\n",
       "        0.5818697 , -0.5390343 , -0.7755241 ,  1.9045625 , -0.26476654,\n",
       "       -1.3056165 , -0.9074661 ,  0.7634458 , -1.5461605 ,  0.08976556,\n",
       "       -0.8494065 , -0.8777184 , -1.0230728 ,  0.3525765 , -1.5808235 ,\n",
       "        0.01667575, -0.1495756 , -0.3053692 ,  0.07764101, -1.1683062 ,\n",
       "        0.7895137 , -0.09378773, -0.17215891, -0.15668851,  0.42977983,\n",
       "        0.4158164 , -1.5737258 ,  1.8172727 ,  1.4243525 ,  1.8285083 ,\n",
       "        0.71199876, -0.9187279 ,  1.1941608 ,  0.565277  ,  0.28214428,\n",
       "        1.4069462 ,  1.2998426 ,  1.0016237 , -0.82209605, -0.50618243,\n",
       "        0.05480868,  1.6530193 , -0.52838504,  0.8456729 , -1.1414135 ,\n",
       "       -0.13572106,  0.576234  ,  0.7270736 , -1.4084289 ], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1d36e595-b493-4a6f-9b5d-4f583ee7d35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5737024 , -0.6759854 ,  0.6507191 , -0.32405192, -1.4975203 ,\n",
       "        1.0036894 ,  0.78646857,  1.5012347 , -0.87575895,  1.8210517 ,\n",
       "       -1.4066744 , -0.5509191 ,  1.885839  , -0.68972784,  0.9726847 ,\n",
       "        0.5760501 , -0.54722756, -0.80475795,  1.9094418 , -0.26871154,\n",
       "       -1.3065656 , -0.9108496 ,  0.78440595, -1.5575368 ,  0.07096983,\n",
       "       -0.8725201 , -0.853683  , -1.0237498 ,  0.34915003, -1.5788484 ,\n",
       "        0.02422947, -0.13966322, -0.32549387,  0.08283487, -1.1725831 ,\n",
       "        0.7983685 , -0.07583883, -0.16052043, -0.15137553,  0.4250438 ,\n",
       "        0.41498375, -1.5853176 ,  1.8458185 ,  1.4217794 ,  1.8454164 ,\n",
       "        0.7082451 , -0.94747126,  1.2044528 ,  0.55465084,  0.29615617,\n",
       "        1.405537  ,  1.2859378 ,  1.0131048 , -0.8556225 , -0.49518442,\n",
       "        0.05648269,  1.654872  , -0.5226128 ,  0.84434843, -1.1529379 ,\n",
       "       -0.14024371,  0.5695921 ,  0.74909246, -1.4219875 ], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a29684-320a-4f85-aa16-51ce4e227314",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Text Only Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d35c4291-4528-4ff0-853b-36d828760796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRumorClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TextRumorClassifier, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        self.ffnn = nn.Sequential(nn.Linear(768,128),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3),\n",
    "                                 nn.Linear(128,64),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3),\n",
    "                                  nn.Linear(64,1),\n",
    "                                  nn.Sigmoid()\n",
    "                                 )\n",
    "\n",
    "    def forward(self, seq, attn_masks, seg):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "        \n",
    "        # x = torch.cat((cls_rep,stats),dim=1)\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.ffnn(cls_rep)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fe7e7a2-650c-4e24-aeb3-ee256bdcae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "net = TextRumorClassifier()\n",
    "# net = net.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f46bc5bb-8c33-4806-85ca-2b0e1380b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_evaluate(net, criterion, dataloader, device):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, mask, seg, labels, idx in dataloader:\n",
    "            # seq, labels = seq.to(device), labels.to(device)\n",
    "            # stats = np.array(train_stat.iloc[:,1:])\n",
    "            # stats = torch.tensor(stats[idx]).float()\n",
    "            #Obtaining the logits from the model\n",
    "            logits = net(seq, mask, seg)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09e88416-c6ea-457f-94c7-be1087fc15f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "best_acc = 0\n",
    "st = time.time()\n",
    "eps = []\n",
    "t_loss = []\n",
    "d_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a32a1b2f-72a6-4f05-a43a-9d9b94694e3f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. \n",
      " Loss: 0.6802356839179993; Accuracy: 0.671875; Time taken (s): 81.13227605819702\n",
      "Iteration 10 of epoch 0 complete. \n",
      " Loss: 0.5898295044898987; Accuracy: 0.78125; Time taken (s): 299.43943309783936\n",
      "Iteration 20 of epoch 0 complete. \n",
      " Loss: 0.5761324763298035; Accuracy: 0.75; Time taken (s): 298.0222783088684\n",
      "Development Accuracy: 0.7715277671813965; Development Loss: 0.5319444206025865\n",
      "Iteration 0 of epoch 1 complete. \n",
      " Loss: 0.5774604678153992; Accuracy: 0.734375; Time taken (s): 220.4582164287567\n",
      "Iteration 10 of epoch 1 complete. \n",
      " Loss: 0.4819682836532593; Accuracy: 0.8125; Time taken (s): 296.9613416194916\n",
      "Iteration 20 of epoch 1 complete. \n",
      " Loss: 0.45131972432136536; Accuracy: 0.796875; Time taken (s): 297.24166321754456\n",
      "Development Accuracy: 0.7996528148651123; Development Loss: 0.43302328056759304\n",
      "Iteration 0 of epoch 2 complete. \n",
      " Loss: 0.4024360477924347; Accuracy: 0.8125; Time taken (s): 225.989727973938\n",
      "Iteration 10 of epoch 2 complete. \n",
      " Loss: 0.4486580789089203; Accuracy: 0.765625; Time taken (s): 297.42676305770874\n",
      "Iteration 20 of epoch 2 complete. \n",
      " Loss: 0.4015752673149109; Accuracy: 0.765625; Time taken (s): 302.4298474788666\n",
      "Development Accuracy: 0.7715277671813965; Development Loss: 0.40253406431939864\n",
      "Iteration 0 of epoch 3 complete. \n",
      " Loss: 0.3700542151927948; Accuracy: 0.859375; Time taken (s): 221.20544385910034\n",
      "Iteration 10 of epoch 3 complete. \n",
      " Loss: 0.3114929795265198; Accuracy: 0.875; Time taken (s): 296.1478228569031\n",
      "Iteration 20 of epoch 3 complete. \n",
      " Loss: 0.4102683663368225; Accuracy: 0.703125; Time taken (s): 297.2571277618408\n",
      "Development Accuracy: 0.7809027433395386; Development Loss: 0.3777681522899204\n",
      "Iteration 0 of epoch 4 complete. \n",
      " Loss: 0.3427141010761261; Accuracy: 0.890625; Time taken (s): 218.97034573554993\n",
      "Iteration 10 of epoch 4 complete. \n",
      " Loss: 0.3261837065219879; Accuracy: 0.859375; Time taken (s): 297.1521053314209\n",
      "Iteration 20 of epoch 4 complete. \n",
      " Loss: 0.1853872537612915; Accuracy: 1.0; Time taken (s): 297.5361819267273\n",
      "Development Accuracy: 0.8743055462837219; Development Loss: 0.3650273084640503\n",
      "Iteration 0 of epoch 5 complete. \n",
      " Loss: 0.23465262353420258; Accuracy: 0.953125; Time taken (s): 220.8443422317505\n",
      "Iteration 10 of epoch 5 complete. \n",
      " Loss: 0.2828870117664337; Accuracy: 0.90625; Time taken (s): 296.63211154937744\n",
      "Iteration 20 of epoch 5 complete. \n",
      " Loss: 0.23903009295463562; Accuracy: 0.9375; Time taken (s): 296.602587223053\n",
      "Development Accuracy: 0.8656250238418579; Development Loss: 0.31499414808220333\n",
      "Iteration 0 of epoch 6 complete. \n",
      " Loss: 0.2568912208080292; Accuracy: 0.90625; Time taken (s): 220.6560664176941\n",
      "Iteration 10 of epoch 6 complete. \n",
      " Loss: 0.247569739818573; Accuracy: 0.953125; Time taken (s): 297.4129750728607\n",
      "Iteration 20 of epoch 6 complete. \n",
      " Loss: 0.21808868646621704; Accuracy: 0.953125; Time taken (s): 303.97619438171387\n",
      "Development Accuracy: 0.8732638955116272; Development Loss: 0.384066647125615\n",
      "Iteration 0 of epoch 7 complete. \n",
      " Loss: 0.19976870715618134; Accuracy: 0.9375; Time taken (s): 220.87582063674927\n",
      "Iteration 10 of epoch 7 complete. \n",
      " Loss: 0.27573078870773315; Accuracy: 0.921875; Time taken (s): 296.1779816150665\n",
      "Iteration 20 of epoch 7 complete. \n",
      " Loss: 0.18678443133831024; Accuracy: 0.953125; Time taken (s): 297.39706158638\n",
      "Development Accuracy: 0.9340277910232544; Development Loss: 0.22147591825988558\n",
      "Iteration 0 of epoch 8 complete. \n",
      " Loss: 0.10352814942598343; Accuracy: 0.984375; Time taken (s): 220.7351589202881\n",
      "Iteration 10 of epoch 8 complete. \n",
      " Loss: 0.22030408680438995; Accuracy: 0.953125; Time taken (s): 296.3964297771454\n",
      "Iteration 20 of epoch 8 complete. \n",
      " Loss: 0.19165925681591034; Accuracy: 0.953125; Time taken (s): 296.3505527973175\n",
      "Development Accuracy: 0.9211804866790771; Development Loss: 0.24064936406082577\n",
      "Iteration 0 of epoch 9 complete. \n",
      " Loss: 0.150380939245224; Accuracy: 0.96875; Time taken (s): 218.93811440467834\n",
      "Iteration 10 of epoch 9 complete. \n",
      " Loss: 0.18152907490730286; Accuracy: 0.96875; Time taken (s): 331.6500794887543\n",
      "Iteration 20 of epoch 9 complete. \n",
      " Loss: 0.1733761727809906; Accuracy: 0.96875; Time taken (s): 296.8183481693268\n",
      "Development Accuracy: 0.9083333611488342; Development Loss: 0.22732179363568625\n",
      "Iteration 0 of epoch 10 complete. \n",
      " Loss: 0.09950870275497437; Accuracy: 0.984375; Time taken (s): 221.98526763916016\n",
      "Iteration 10 of epoch 10 complete. \n",
      " Loss: 0.09946000576019287; Accuracy: 1.0; Time taken (s): 296.6942472457886\n",
      "Iteration 20 of epoch 10 complete. \n",
      " Loss: 0.08078835904598236; Accuracy: 0.984375; Time taken (s): 297.30266857147217\n",
      "Development Accuracy: 0.9281249642372131; Development Loss: 0.22359735684262383\n",
      "Iteration 0 of epoch 11 complete. \n",
      " Loss: 0.16118431091308594; Accuracy: 0.96875; Time taken (s): 221.00080060958862\n",
      "Iteration 10 of epoch 11 complete. \n",
      " Loss: 0.18316838145256042; Accuracy: 0.96875; Time taken (s): 303.366548538208\n",
      "Iteration 20 of epoch 11 complete. \n",
      " Loss: 0.11347579956054688; Accuracy: 0.96875; Time taken (s): 297.30295848846436\n",
      "Development Accuracy: 0.9322916865348816; Development Loss: 0.20730443360904852\n",
      "Iteration 0 of epoch 12 complete. \n",
      " Loss: 0.1285964399576187; Accuracy: 0.984375; Time taken (s): 218.70448565483093\n",
      "Iteration 10 of epoch 12 complete. \n",
      " Loss: 0.11941289901733398; Accuracy: 0.984375; Time taken (s): 296.63081550598145\n",
      "Iteration 20 of epoch 12 complete. \n",
      " Loss: 0.058265190571546555; Accuracy: 1.0; Time taken (s): 297.2467796802521\n",
      "Development Accuracy: 0.9281249642372131; Development Loss: 0.25157110227478874\n",
      "Iteration 0 of epoch 13 complete. \n",
      " Loss: 0.060456059873104095; Accuracy: 0.984375; Time taken (s): 220.57226514816284\n",
      "Iteration 10 of epoch 13 complete. \n",
      " Loss: 0.1359124779701233; Accuracy: 0.96875; Time taken (s): 296.16311144828796\n",
      "Iteration 20 of epoch 13 complete. \n",
      " Loss: 0.24394303560256958; Accuracy: 0.9375; Time taken (s): 296.1930947303772\n",
      "Development Accuracy: 0.9315971732139587; Development Loss: 0.2178215417597029\n",
      "Iteration 0 of epoch 14 complete. \n",
      " Loss: 0.043951794505119324; Accuracy: 1.0; Time taken (s): 219.92360663414001\n",
      "Iteration 10 of epoch 14 complete. \n",
      " Loss: 0.03783406689763069; Accuracy: 1.0; Time taken (s): 295.7859377861023\n",
      "Iteration 20 of epoch 14 complete. \n",
      " Loss: 0.10898635536432266; Accuracy: 0.984375; Time taken (s): 295.7577893733978\n",
      "Development Accuracy: 0.9409722089767456; Development Loss: 0.2148100563014547\n",
      "Iteration 0 of epoch 15 complete. \n",
      " Loss: 0.03337579220533371; Accuracy: 1.0; Time taken (s): 220.61015558242798\n",
      "Iteration 10 of epoch 15 complete. \n",
      " Loss: 0.031077606603503227; Accuracy: 1.0; Time taken (s): 301.6768755912781\n",
      "Iteration 20 of epoch 15 complete. \n",
      " Loss: 0.030278261750936508; Accuracy: 1.0; Time taken (s): 294.6953103542328\n",
      "Development Accuracy: 0.9281249642372131; Development Loss: 0.29197897513707477\n",
      "Iteration 0 of epoch 16 complete. \n",
      " Loss: 0.18090198934078217; Accuracy: 0.96875; Time taken (s): 220.5163128376007\n",
      "Iteration 10 of epoch 16 complete. \n",
      " Loss: 0.037820782512426376; Accuracy: 1.0; Time taken (s): 294.73997259140015\n",
      "Iteration 20 of epoch 16 complete. \n",
      " Loss: 0.031452666968107224; Accuracy: 1.0; Time taken (s): 295.52116107940674\n",
      "Development Accuracy: 0.9246527552604675; Development Loss: 0.2850782523552577\n",
      "Iteration 0 of epoch 17 complete. \n",
      " Loss: 0.08480245620012283; Accuracy: 0.984375; Time taken (s): 220.00190448760986\n",
      "Iteration 10 of epoch 17 complete. \n",
      " Loss: 0.038943544030189514; Accuracy: 1.0; Time taken (s): 294.96084427833557\n",
      "Iteration 20 of epoch 17 complete. \n",
      " Loss: 0.08793707191944122; Accuracy: 0.984375; Time taken (s): 296.37982153892517\n",
      "Development Accuracy: 0.9409722089767456; Development Loss: 0.23112470884290007\n",
      "Iteration 0 of epoch 18 complete. \n",
      " Loss: 0.09463363140821457; Accuracy: 0.984375; Time taken (s): 218.76718187332153\n",
      "Iteration 10 of epoch 18 complete. \n",
      " Loss: 0.07292123138904572; Accuracy: 0.984375; Time taken (s): 297.42747044563293\n",
      "Iteration 20 of epoch 18 complete. \n",
      " Loss: 0.09867572039365768; Accuracy: 0.984375; Time taken (s): 297.27133989334106\n",
      "Development Accuracy: 0.9392361044883728; Development Loss: 0.21728208433422777\n",
      "Iteration 0 of epoch 19 complete. \n",
      " Loss: 0.085542231798172; Accuracy: 0.984375; Time taken (s): 221.0791130065918\n",
      "Iteration 10 of epoch 19 complete. \n",
      " Loss: 0.110785573720932; Accuracy: 0.984375; Time taken (s): 296.2733783721924\n",
      "Iteration 20 of epoch 19 complete. \n",
      " Loss: 0.031884439289569855; Accuracy: 1.0; Time taken (s): 303.8342869281769\n",
      "Development Accuracy: 0.9315971732139587; Development Loss: 0.25821830415063435\n"
     ]
    }
   ],
   "source": [
    "for ep in range(20):\n",
    "    eps.append(ep)\n",
    "    net.train()\n",
    "    for it, (seq, mask, seg, labels,idx) in enumerate(train_loader):\n",
    "\n",
    "        #Clear gradients\n",
    "        opti.zero_grad()\n",
    "        #Converting these to cuda tensors\n",
    "        # seq, mask, seg, labels = seq.to(device), mask.to(device), seg.to(device), labels.to(device)\n",
    "\n",
    "        # stats = np.array(train_stat.iloc[:,1:])\n",
    "        # stats = torch.tensor(stats[idx]).float()\n",
    "        #Obtaining the logits from the model\n",
    "        logits = net(seq, mask, seg)\n",
    "\n",
    "        #Computing loss\n",
    "        loss = criterion(logits.squeeze(), labels.float())\n",
    "\n",
    "        #Backpropagating the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #Optimization step\n",
    "        opti.step()\n",
    "\n",
    "        if it % 10 == 0:\n",
    "\n",
    "            acc = get_accuracy_from_logits(logits, labels)\n",
    "            print(\"Iteration {} of epoch {} complete. \\n Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "\n",
    "    dev_acc, dev_loss = text_evaluate(net, criterion, dev_loader, 'cpu')\n",
    "    t_loss.append(loss.item())\n",
    "    d_loss.append(dev_loss)\n",
    "    print(\"Development Accuracy: {}; Development Loss: {}\".format(dev_acc, dev_loss))\n",
    "    torch.save(net.state_dict(), 'D:\\\\bertcls_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadfc51b-742c-4c70-8752-4fb30faf7ba8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Text Only Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47479f40-9744-4907-87fa-6a1d151aa79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = TextRumorClassifier()\n",
    "text_model.load_state_dict(torch.load('D:\\\\bertcls_14.dat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a20adf9e-5a46-40e8-b365-bc0e52ad1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet = pd.read_csv('./sep_data/test_tweet_df.csv')\n",
    "test_stat = pd.read_csv('./sep_data/test_stat_feat_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20176d53-9f25-43b5-bf3d-1bb0279eb141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/558 [00:00<?, ?it/s]C:\\Users\\trist\\AppData\\Local\\Temp\\ipykernel_6104\\3174842241.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_tweet.text.iloc[i] = '[CLS] ' + str(test_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(test_tweet.reply_text.iloc[i]).strip()\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 558/558 [00:08<00:00, 68.94it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tweet.text.fillna('', inplace=True)\n",
    "test_tweet.reply_text.fillna('', inplace=True)\n",
    "for i in tqdm(range(len(test_tweet))):\n",
    "    test_tweet.text.iloc[i] = '[CLS] ' + str(test_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(test_tweet.reply_text.iloc[i]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4afdab8d-8f34-4e24-999d-517f8407b5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 558/558 [00:02<00:00, 270.06it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 256\n",
    "test_seq = []\n",
    "test_mask = []\n",
    "test_seg = []\n",
    "\n",
    "for i in tqdm(range(len(test_tweet))):\n",
    "    try:\n",
    "        txt = test_tweet.text.iloc[i]\n",
    "        tokens = tokenizer.tokenize(txt)\n",
    "        if len(tokens) < max_len:\n",
    "             padded_tokens = tokens + ['[PAD]' for _ in range(max_len - len(tokens))]\n",
    "        else:\n",
    "            padded_tokens = tokens[:max_len-1] + ['[SEP]']\n",
    "        attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "        seg_ids = []\n",
    "        seg_idx = 0\n",
    "        for token in padded_tokens:\n",
    "            seg_ids.append(seg_idx)\n",
    "            if token == '[SEP]':\n",
    "                seg_idx += 1\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "\n",
    "        test_seq.append(token_ids)\n",
    "        test_mask.append(attn_mask)\n",
    "        test_seg.append(seg_ids)\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5521f55-9298-4586-ba4f-cb65883f31b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 558/558 [02:19<00:00,  4.01it/s]\n"
     ]
    }
   ],
   "source": [
    "text_model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_seq))):\n",
    "        seq = torch.tensor(test_seq[i]).unsqueeze(0)\n",
    "        mask = torch.tensor(test_mask[i]).unsqueeze(0)\n",
    "        seg = torch.tensor(test_seg[i]).unsqueeze(0)\n",
    "        \n",
    "        preds.append(text_model(seq,mask,seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d823e06d-a924-40a0-b4c4-610f79be8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(preds)):\n",
    "    preds[i] = preds[i].squeeze().squeeze()\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    preds[i] = preds[i].numpy()\n",
    "\n",
    "predictions = preds[0]\n",
    "for i in range(1,len(preds)):\n",
    "    predictions = np.hstack((predictions,preds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5abe73fe-7769-40ed-858e-3ad1d862f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {'Id':[i for i in range(len(predictions))], 'Predicted':predictions}\n",
    "pred_df = DataFrame(pred_dict)\n",
    "pred_df.Predicted = pred_df.Predicted.apply(lambda x: 1 if x > 0.5 else 0)\n",
    "pred_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f622652f-5545-4a44-8e17-4581e9852eff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### BERT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d1a22695-b546-4db3-b35f-dc59b70ef88c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RumorClassifier()\n",
    "model.load_state_dict(torch.load('D:\\\\bertcls_14.dat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "260e407b-3dfe-4c14-bfc0-9e2f4c3ae50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development F1: 0.7892376681614348; Development Loss: 0.27994822296831345\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "dev_acc, dev_loss = evaluate(model, criterion, dev_loader, 'cpu')\n",
    "print(\"Development F1: {}; Development Loss: {}\".format(dev_acc, dev_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "00683eb0-e793-4e20-8d29-d8a9530783db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_tweet = pd.read_csv('./sep_data/test_tweet_df.csv')\n",
    "test_stat = pd.read_csv('./sep_data/test_scaled_stat_feat_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a50c9a8c-f5e2-45a8-9f64-94331ce0dc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>reply_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1246482832316301319</td>\n",
       "      <td>covid spread txhdeupetg iyyirwcksp</td>\n",
       "      <td>thank wcco station trust media provid true new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1252279738099433473</td>\n",
       "      <td>hate keep say capit implod without zlzlcroxzl</td>\n",
       "      <td>believ look chang week [SEP] tell peopl protes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1236050255394877440</td>\n",
       "      <td>covid influenza virus differ coronaviru cpraumngq</td>\n",
       "      <td>covid influenza virus similar coronaviru [SEP]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1235582115900796928</td>\n",
       "      <td>una de le q coronavirus de la pàgina web de q ...</td>\n",
       "      <td>aquesta informació es basa sobr tot en un arti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1258787515592572928</td>\n",
       "      <td>absolut blame politician whoever els involv ho...</td>\n",
       "      <td>forget racism institut peopl xuwlvqspdi [SEP] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>518827403452637184</td>\n",
       "      <td>hewlett packard plan split two compani wsj rep...</td>\n",
       "      <td>heward packlett [SEP] hewlett packard plan spl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>489829414704648192</td>\n",
       "      <td>http http qygkkjftiw</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>580348081100734464</td>\n",
       "      <td>germanw sorri confirm passeng crew board fligh...</td>\n",
       "      <td>sad news germanw sorri confirm passeng crew bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1248121143808098305</td>\n",
       "      <td>sure differ path mean differ</td>\n",
       "      <td>sometim still stop track believ insulin free w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1258487399014858752</td>\n",
       "      <td>depend get drive vehicl alreadi gass iigtbtqtlq</td>\n",
       "      <td>mayb miss someth would unsaf famili go away da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1246482832316301319                 covid spread txhdeupetg iyyirwcksp   \n",
       "1  1252279738099433473      hate keep say capit implod without zlzlcroxzl   \n",
       "2  1236050255394877440  covid influenza virus differ coronaviru cpraumngq   \n",
       "3  1235582115900796928  una de le q coronavirus de la pàgina web de q ...   \n",
       "4  1258787515592572928  absolut blame politician whoever els involv ho...   \n",
       "5   518827403452637184  hewlett packard plan split two compani wsj rep...   \n",
       "6   489829414704648192                               http http qygkkjftiw   \n",
       "7   580348081100734464  germanw sorri confirm passeng crew board fligh...   \n",
       "8  1248121143808098305                       sure differ path mean differ   \n",
       "9  1258487399014858752    depend get drive vehicl alreadi gass iigtbtqtlq   \n",
       "\n",
       "                                          reply_text  \n",
       "0  thank wcco station trust media provid true new...  \n",
       "1  believ look chang week [SEP] tell peopl protes...  \n",
       "2  covid influenza virus similar coronaviru [SEP]...  \n",
       "3  aquesta informació es basa sobr tot en un arti...  \n",
       "4  forget racism institut peopl xuwlvqspdi [SEP] ...  \n",
       "5  heward packlett [SEP] hewlett packard plan spl...  \n",
       "6                                                NaN  \n",
       "7  sad news germanw sorri confirm passeng crew bo...  \n",
       "8  sometim still stop track believ insulin free w...  \n",
       "9  mayb miss someth would unsaf famili go away da...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e8d702b-131a-4018-8c63-dd9ad312693a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reply_possibly_sensitive</th>\n",
       "      <th>reply_retweet_count</th>\n",
       "      <th>reply_favorite_count</th>\n",
       "      <th>reply_mentioned_url_num</th>\n",
       "      <th>reply_id_num</th>\n",
       "      <th>reply_followers_count</th>\n",
       "      <th>reply_friends_count</th>\n",
       "      <th>reply_listed_count</th>\n",
       "      <th>reply_favourites_count</th>\n",
       "      <th>reply_statuses_count</th>\n",
       "      <th>...</th>\n",
       "      <th>truncated</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>geo_enabled</th>\n",
       "      <th>verified</th>\n",
       "      <th>isweekday</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>is_translation_enabled</th>\n",
       "      <th>has_extended_profile</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.146906</td>\n",
       "      <td>-0.048668</td>\n",
       "      <td>-0.039520</td>\n",
       "      <td>-0.359423</td>\n",
       "      <td>-0.352452</td>\n",
       "      <td>-0.125729</td>\n",
       "      <td>-0.430135</td>\n",
       "      <td>-0.142861</td>\n",
       "      <td>-0.468949</td>\n",
       "      <td>-0.439718</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.954789</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>0.929638</td>\n",
       "      <td>1.251990</td>\n",
       "      <td>-1.722596</td>\n",
       "      <td>-0.504152</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>-0.722953</td>\n",
       "      <td>-0.717882</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.146906</td>\n",
       "      <td>-0.048668</td>\n",
       "      <td>-0.039520</td>\n",
       "      <td>-0.040946</td>\n",
       "      <td>-0.217940</td>\n",
       "      <td>-0.125330</td>\n",
       "      <td>-0.260658</td>\n",
       "      <td>-0.140310</td>\n",
       "      <td>0.020302</td>\n",
       "      <td>-0.205033</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047352</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>-1.075688</td>\n",
       "      <td>-0.798728</td>\n",
       "      <td>0.580519</td>\n",
       "      <td>-0.193920</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>-0.722953</td>\n",
       "      <td>1.392986</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.146906</td>\n",
       "      <td>-0.048668</td>\n",
       "      <td>-0.039468</td>\n",
       "      <td>-0.279803</td>\n",
       "      <td>-0.374870</td>\n",
       "      <td>-0.123518</td>\n",
       "      <td>-0.428935</td>\n",
       "      <td>-0.139435</td>\n",
       "      <td>-0.466908</td>\n",
       "      <td>-0.436924</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047352</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>0.929638</td>\n",
       "      <td>-0.798728</td>\n",
       "      <td>0.580519</td>\n",
       "      <td>-0.452447</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>-0.722953</td>\n",
       "      <td>1.392986</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.146906</td>\n",
       "      <td>-0.048530</td>\n",
       "      <td>-0.039417</td>\n",
       "      <td>-0.200184</td>\n",
       "      <td>-0.374870</td>\n",
       "      <td>-0.125684</td>\n",
       "      <td>-0.424884</td>\n",
       "      <td>-0.142752</td>\n",
       "      <td>-0.468983</td>\n",
       "      <td>-0.439474</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047352</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>-1.075688</td>\n",
       "      <td>-0.798728</td>\n",
       "      <td>0.580519</td>\n",
       "      <td>-0.400741</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>1.383215</td>\n",
       "      <td>1.392986</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.146906</td>\n",
       "      <td>-0.047841</td>\n",
       "      <td>-0.039365</td>\n",
       "      <td>0.038673</td>\n",
       "      <td>-0.195521</td>\n",
       "      <td>-0.125523</td>\n",
       "      <td>-0.313216</td>\n",
       "      <td>-0.142570</td>\n",
       "      <td>-0.307555</td>\n",
       "      <td>-0.358209</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047352</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>0.929638</td>\n",
       "      <td>-0.798728</td>\n",
       "      <td>0.580519</td>\n",
       "      <td>-0.245625</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>1.383215</td>\n",
       "      <td>-0.717882</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>0.823597</td>\n",
       "      <td>-0.048530</td>\n",
       "      <td>-0.039442</td>\n",
       "      <td>0.755244</td>\n",
       "      <td>0.252850</td>\n",
       "      <td>-0.123699</td>\n",
       "      <td>0.153559</td>\n",
       "      <td>-0.098318</td>\n",
       "      <td>0.214535</td>\n",
       "      <td>0.045030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.954789</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>0.929638</td>\n",
       "      <td>1.251990</td>\n",
       "      <td>0.580519</td>\n",
       "      <td>0.788482</td>\n",
       "      <td>2.736649</td>\n",
       "      <td>-0.722953</td>\n",
       "      <td>-0.717882</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>-0.146906</td>\n",
       "      <td>-0.048530</td>\n",
       "      <td>-0.039184</td>\n",
       "      <td>-0.200184</td>\n",
       "      <td>1.194431</td>\n",
       "      <td>-0.117722</td>\n",
       "      <td>0.571502</td>\n",
       "      <td>-0.100140</td>\n",
       "      <td>0.816771</td>\n",
       "      <td>0.688410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.954789</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>0.929638</td>\n",
       "      <td>1.251990</td>\n",
       "      <td>-1.722596</td>\n",
       "      <td>2.649876</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>1.383215</td>\n",
       "      <td>-0.717882</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>-0.146906</td>\n",
       "      <td>-0.048530</td>\n",
       "      <td>-0.039494</td>\n",
       "      <td>-0.200184</td>\n",
       "      <td>-0.061010</td>\n",
       "      <td>-0.123979</td>\n",
       "      <td>-0.051749</td>\n",
       "      <td>-0.121355</td>\n",
       "      <td>-0.254072</td>\n",
       "      <td>-0.131456</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.954789</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>-1.075688</td>\n",
       "      <td>1.251990</td>\n",
       "      <td>0.580519</td>\n",
       "      <td>-0.038804</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>-0.722953</td>\n",
       "      <td>-0.717882</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>-0.146906</td>\n",
       "      <td>-0.041774</td>\n",
       "      <td>-0.036267</td>\n",
       "      <td>-0.200184</td>\n",
       "      <td>0.320106</td>\n",
       "      <td>1.855980</td>\n",
       "      <td>-0.329794</td>\n",
       "      <td>2.296395</td>\n",
       "      <td>-0.404845</td>\n",
       "      <td>-0.319952</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047352</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>0.929638</td>\n",
       "      <td>1.251990</td>\n",
       "      <td>0.580519</td>\n",
       "      <td>-0.349036</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>1.383215</td>\n",
       "      <td>-0.717882</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>-0.146906</td>\n",
       "      <td>-0.048392</td>\n",
       "      <td>-0.039468</td>\n",
       "      <td>-0.279803</td>\n",
       "      <td>-0.352452</td>\n",
       "      <td>-0.125410</td>\n",
       "      <td>-0.380977</td>\n",
       "      <td>-0.142715</td>\n",
       "      <td>-0.349507</td>\n",
       "      <td>-0.406812</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047352</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>-1.075688</td>\n",
       "      <td>-0.798728</td>\n",
       "      <td>-1.722596</td>\n",
       "      <td>-0.452447</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>-0.722953</td>\n",
       "      <td>-0.717882</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>558 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     reply_possibly_sensitive  reply_retweet_count  reply_favorite_count  \\\n",
       "0                   -0.146906            -0.048668             -0.039520   \n",
       "1                   -0.146906            -0.048668             -0.039520   \n",
       "2                   -0.146906            -0.048668             -0.039468   \n",
       "3                   -0.146906            -0.048530             -0.039417   \n",
       "4                   -0.146906            -0.047841             -0.039365   \n",
       "..                        ...                  ...                   ...   \n",
       "553                  0.823597            -0.048530             -0.039442   \n",
       "554                 -0.146906            -0.048530             -0.039184   \n",
       "555                 -0.146906            -0.048530             -0.039494   \n",
       "556                 -0.146906            -0.041774             -0.036267   \n",
       "557                 -0.146906            -0.048392             -0.039468   \n",
       "\n",
       "     reply_mentioned_url_num  reply_id_num  reply_followers_count  \\\n",
       "0                  -0.359423     -0.352452              -0.125729   \n",
       "1                  -0.040946     -0.217940              -0.125330   \n",
       "2                  -0.279803     -0.374870              -0.123518   \n",
       "3                  -0.200184     -0.374870              -0.125684   \n",
       "4                   0.038673     -0.195521              -0.125523   \n",
       "..                       ...           ...                    ...   \n",
       "553                 0.755244      0.252850              -0.123699   \n",
       "554                -0.200184      1.194431              -0.117722   \n",
       "555                -0.200184     -0.061010              -0.123979   \n",
       "556                -0.200184      0.320106               1.855980   \n",
       "557                -0.279803     -0.352452              -0.125410   \n",
       "\n",
       "     reply_friends_count  reply_listed_count  reply_favourites_count  \\\n",
       "0              -0.430135           -0.142861               -0.468949   \n",
       "1              -0.260658           -0.140310                0.020302   \n",
       "2              -0.428935           -0.139435               -0.466908   \n",
       "3              -0.424884           -0.142752               -0.468983   \n",
       "4              -0.313216           -0.142570               -0.307555   \n",
       "..                   ...                 ...                     ...   \n",
       "553             0.153559           -0.098318                0.214535   \n",
       "554             0.571502           -0.100140                0.816771   \n",
       "555            -0.051749           -0.121355               -0.254072   \n",
       "556            -0.329794            2.296395               -0.404845   \n",
       "557            -0.380977           -0.142715               -0.349507   \n",
       "\n",
       "     reply_statuses_count  ...  truncated  is_quote_status  geo_enabled  \\\n",
       "0               -0.439718  ...  -0.954789        -0.152745     0.929638   \n",
       "1               -0.205033  ...   1.047352        -0.152745    -1.075688   \n",
       "2               -0.436924  ...   1.047352        -0.152745     0.929638   \n",
       "3               -0.439474  ...   1.047352        -0.152745    -1.075688   \n",
       "4               -0.358209  ...   1.047352        -0.152745     0.929638   \n",
       "..                    ...  ...        ...              ...          ...   \n",
       "553              0.045030  ...  -0.954789        -0.152745     0.929638   \n",
       "554              0.688410  ...  -0.954789        -0.152745     0.929638   \n",
       "555             -0.131456  ...  -0.954789        -0.152745    -1.075688   \n",
       "556             -0.319952  ...   1.047352        -0.152745     0.929638   \n",
       "557             -0.406812  ...   1.047352        -0.152745    -1.075688   \n",
       "\n",
       "     verified  isweekday  reply_count  is_translation_enabled  \\\n",
       "0    1.251990  -1.722596    -0.504152               -0.365410   \n",
       "1   -0.798728   0.580519    -0.193920               -0.365410   \n",
       "2   -0.798728   0.580519    -0.452447               -0.365410   \n",
       "3   -0.798728   0.580519    -0.400741               -0.365410   \n",
       "4   -0.798728   0.580519    -0.245625               -0.365410   \n",
       "..        ...        ...          ...                     ...   \n",
       "553  1.251990   0.580519     0.788482                2.736649   \n",
       "554  1.251990  -1.722596     2.649876               -0.365410   \n",
       "555  1.251990   0.580519    -0.038804               -0.365410   \n",
       "556  1.251990   0.580519    -0.349036               -0.365410   \n",
       "557 -0.798728  -1.722596    -0.452447               -0.365410   \n",
       "\n",
       "     has_extended_profile  default_profile  default_profile_image  \n",
       "0               -0.722953        -0.717882              -0.050395  \n",
       "1               -0.722953         1.392986              -0.050395  \n",
       "2               -0.722953         1.392986              -0.050395  \n",
       "3                1.383215         1.392986              -0.050395  \n",
       "4                1.383215        -0.717882              -0.050395  \n",
       "..                    ...              ...                    ...  \n",
       "553             -0.722953        -0.717882              -0.050395  \n",
       "554              1.383215        -0.717882              -0.050395  \n",
       "555             -0.722953        -0.717882              -0.050395  \n",
       "556              1.383215        -0.717882              -0.050395  \n",
       "557             -0.722953        -0.717882              -0.050395  \n",
       "\n",
       "[558 rows x 41 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stat.drop(columns=train_zero, inplace=True)\n",
    "test_stat.drop(columns=['Unnamed: 0', 'contributors'], inplace=True)\n",
    "test_stat.fillna(train_stat.mean(), inplace=True)\n",
    "test_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f29bdc2-5848-4d0f-8339-b2f69fcc0b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 558 entries, 0 to 557\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweet_id    558 non-null    int64 \n",
      " 1   text        556 non-null    object\n",
      " 2   reply_text  545 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 13.2+ KB\n"
     ]
    }
   ],
   "source": [
    "test_tweet.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7e3d74d-9603-4dec-b820-cbc49488afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet.text.fillna('', inplace=True)\n",
    "test_tweet.reply_text.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1069919-5425-4456-9ac5-159877e11660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/558 [00:00<?, ?it/s]C:\\Users\\trist\\AppData\\Local\\Temp\\ipykernel_3132\\3420928069.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_tweet.text.iloc[i] = '[CLS] ' + str(test_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(test_tweet.reply_text.iloc[i]).strip() + ' [SEP]'\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 558/558 [00:06<00:00, 80.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_tweet))):\n",
    "    test_tweet.text.iloc[i] = '[CLS] ' + str(test_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(test_tweet.reply_text.iloc[i]).strip() + ' [SEP]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca68fd5e-aee4-4c2c-917a-43293615b737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 558/558 [00:01<00:00, 280.14it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 256\n",
    "test_seq = []\n",
    "test_mask = []\n",
    "test_seg = []\n",
    "\n",
    "for i in tqdm(range(len(test_tweet))):\n",
    "    try:\n",
    "        txt = test_tweet.text.iloc[i]\n",
    "        tokens = tokenizer.tokenize(txt)\n",
    "        if len(tokens) < max_len:\n",
    "             padded_tokens = tokens + ['[PAD]' for _ in range(max_len - len(tokens))]\n",
    "        else:\n",
    "            padded_tokens = tokens[:max_len-1] + ['[SEP]']\n",
    "        attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "        seg_ids = []\n",
    "        seg_idx = 0\n",
    "        for token in padded_tokens:\n",
    "            seg_ids.append(seg_idx)\n",
    "            if token == '[SEP]':\n",
    "                seg_idx += 1\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "\n",
    "        test_seq.append(token_ids)\n",
    "        test_mask.append(attn_mask)\n",
    "        test_seg.append(seg_ids)\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d9e8b069-72a0-483b-80ef-5b9b0144f168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 558 entries, 0 to 557\n",
      "Data columns (total 41 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   reply_possibly_sensitive      558 non-null    float64\n",
      " 1   reply_retweet_count           558 non-null    float64\n",
      " 2   reply_favorite_count          558 non-null    float64\n",
      " 3   reply_mentioned_url_num       558 non-null    float64\n",
      " 4   reply_id_num                  558 non-null    float64\n",
      " 5   reply_followers_count         558 non-null    float64\n",
      " 6   reply_friends_count           558 non-null    float64\n",
      " 7   reply_listed_count            558 non-null    float64\n",
      " 8   reply_favourites_count        558 non-null    float64\n",
      " 9   reply_statuses_count          558 non-null    float64\n",
      " 10  reply_has_url                 558 non-null    float64\n",
      " 11  reply_senti_score             558 non-null    float64\n",
      " 12  reply_truncated               558 non-null    float64\n",
      " 13  reply_is_quote_status         558 non-null    float64\n",
      " 14  reply_geo_enabled             558 non-null    float64\n",
      " 15  reply_verified                558 non-null    float64\n",
      " 16  reply_isweekday               558 non-null    float64\n",
      " 17  reply_is_translation_enabled  558 non-null    float64\n",
      " 18  reply_has_extended_profile    558 non-null    float64\n",
      " 19  reply_default_profile         558 non-null    float64\n",
      " 20  reply_default_profile_image   558 non-null    float64\n",
      " 21  possibly_sensitive            558 non-null    float64\n",
      " 22  favorite_count                558 non-null    float64\n",
      " 23  mentioned_url_num             558 non-null    float64\n",
      " 24  id_num                        558 non-null    float64\n",
      " 25  followers_count               558 non-null    float64\n",
      " 26  friends_count                 558 non-null    float64\n",
      " 27  listed_count                  558 non-null    float64\n",
      " 28  favourites_count              558 non-null    float64\n",
      " 29  has_url                       558 non-null    float64\n",
      " 30  senti_score                   558 non-null    float64\n",
      " 31  truncated                     558 non-null    float64\n",
      " 32  is_quote_status               558 non-null    float64\n",
      " 33  geo_enabled                   558 non-null    float64\n",
      " 34  verified                      558 non-null    float64\n",
      " 35  isweekday                     558 non-null    float64\n",
      " 36  reply_count                   558 non-null    float64\n",
      " 37  is_translation_enabled        558 non-null    float64\n",
      " 38  has_extended_profile          558 non-null    float64\n",
      " 39  default_profile               558 non-null    float64\n",
      " 40  default_profile_image         558 non-null    float64\n",
      "dtypes: float64(41)\n",
      "memory usage: 178.9 KB\n"
     ]
    }
   ],
   "source": [
    "test_stat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4c8cbb8e-eee5-4b5c-86a5-2e0c90d1e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTweetDataset(Data.Dataset):\n",
    "    def __init__(self, seq, mask, seg, stat):\n",
    "        self.seq = torch.tensor(seq).long()\n",
    "        self.mask = torch.tensor(mask).long()\n",
    "        self.seg = torch.tensor(seg).long()\n",
    "        self.stat = torch.tensor(stat).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx],self.mask[idx],self.seg[idx], self.stat[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dac9a35a-5c60-4c32-971f-45a71480dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = TestTweetDataset(test_seq, test_mask, test_seg, np.array(test_stat))\n",
    "test_loader = Data.DataLoader(test_set, batch_size=64, shuffle=False, sampler=range(0,len(test_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "60b06a00-bccf-4376-8965-fd9839365f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [02:06, 14.10s/it]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "test_stat = np.array(test_stat)\n",
    "with torch.no_grad():\n",
    "    for i, (seq, mask, seg, stat) in tqdm(enumerate(test_loader)):\n",
    "        preds.append(model(seq, mask, seg, stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "312437ab-3532-449c-b63a-f77446597e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 558/558 [03:31<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# import gc\n",
    "# test_stat = np.array(test_stat)\n",
    "# model.eval()\n",
    "# preds = []\n",
    "# with torch.no_grad():\n",
    "#     for i in tqdm(range(len(test_seq))):\n",
    "#         seq = torch.tensor(test_seq[i]).unsqueeze(0)\n",
    "#         mask = torch.tensor(test_mask[i]).unsqueeze(0)\n",
    "#         seg = torch.tensor(test_seg[i]).unsqueeze(0)\n",
    "#         stat = torch.tensor(test_stat[i]).unsqueeze(0).float()\n",
    "        \n",
    "#         preds.append(model(seq,mask,seg,stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "241ffe91-d5aa-4a3d-b66c-c739dd44f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(preds)):\n",
    "    preds[i] = preds[i].squeeze().squeeze()\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    preds[i] = preds[i].numpy()\n",
    "\n",
    "predictions = preds[0]\n",
    "for i in range(1,len(preds)):\n",
    "    predictions = np.hstack((predictions,preds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5e2c52ee-7db7-478b-8f67-bd7bf181764a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01103744, 0.02429889, 0.01073438, 0.01114028, 0.0557049 ,\n",
       "       0.942422  , 0.7602823 , 0.7536534 , 0.01202546, 0.01052115,\n",
       "       0.9715797 , 0.95407295, 0.01061155, 0.06238277, 0.01087098,\n",
       "       0.01098265, 0.01138624, 0.01084566, 0.01085408, 0.0106442 ,\n",
       "       0.01080074, 0.01058637, 0.01068432, 0.973152  , 0.01926528,\n",
       "       0.01126047, 0.01063889, 0.12392541, 0.01136388, 0.97529286,\n",
       "       0.01065597, 0.96591413, 0.966156  , 0.01067697, 0.01072857,\n",
       "       0.01052672, 0.01065933, 0.01061057, 0.01045015, 0.4757545 ,\n",
       "       0.01139514, 0.8899736 , 0.01126349, 0.26755974, 0.01094513,\n",
       "       0.01114806, 0.01083188, 0.83372307, 0.01070315, 0.01063965,\n",
       "       0.01066258, 0.01124992, 0.97513026, 0.01183491, 0.01236483,\n",
       "       0.9713407 , 0.18475726, 0.01033938, 0.01080643, 0.01089348,\n",
       "       0.01062205, 0.9742261 , 0.01204596, 0.01097226, 0.01143503,\n",
       "       0.01071359, 0.97228706, 0.01786041, 0.01077582, 0.01083991,\n",
       "       0.01044134, 0.01073373, 0.01066977, 0.97359246, 0.0126698 ,\n",
       "       0.9733776 , 0.01044562, 0.9755637 , 0.9753303 , 0.01114514,\n",
       "       0.01606308, 0.01085645, 0.9750885 , 0.0108918 , 0.97476304,\n",
       "       0.01043905, 0.01045169, 0.01054497, 0.01078772, 0.01058268,\n",
       "       0.01162118, 0.01069158, 0.01091181, 0.01106595, 0.01199126,\n",
       "       0.01224113, 0.01064286, 0.01062784, 0.9735004 , 0.01075927,\n",
       "       0.01068591, 0.01255963, 0.01087541, 0.9751864 , 0.01059991,\n",
       "       0.01050992, 0.9706863 , 0.01281074, 0.01084293, 0.9747482 ,\n",
       "       0.15711263, 0.01097668, 0.01056952, 0.01004823, 0.01226112,\n",
       "       0.01221944, 0.01089089, 0.01032131, 0.03393933, 0.01103583,\n",
       "       0.97632796, 0.01034183, 0.01047673, 0.01084495, 0.01047406,\n",
       "       0.9753718 , 0.9740603 , 0.9546753 , 0.01294067, 0.9752303 ,\n",
       "       0.01092973, 0.010967  , 0.9188541 , 0.01051176, 0.01117651,\n",
       "       0.97547543, 0.01070152, 0.95608485, 0.01092163, 0.01043754,\n",
       "       0.01167482, 0.01121236, 0.9751061 , 0.0103145 , 0.965195  ,\n",
       "       0.01085164, 0.01084672, 0.9742255 , 0.01068382, 0.0110436 ,\n",
       "       0.01071981, 0.01101386, 0.0111683 , 0.01067451, 0.5807091 ,\n",
       "       0.97555643, 0.01150362, 0.01030581, 0.9734875 , 0.01264323,\n",
       "       0.9716562 , 0.9649949 , 0.01133554, 0.02056327, 0.95224315,\n",
       "       0.01044387, 0.01331524, 0.01081784, 0.01177841, 0.01085054,\n",
       "       0.01037919, 0.9747791 , 0.01095183, 0.97613007, 0.97024566,\n",
       "       0.01104818, 0.01159883, 0.01068163, 0.01058573, 0.01052896,\n",
       "       0.01058651, 0.01951507, 0.01048437, 0.01166512, 0.00952245,\n",
       "       0.01041662, 0.07411677, 0.01065077, 0.9748862 , 0.01269097,\n",
       "       0.01581364, 0.01010385, 0.0112604 , 0.01051844, 0.01078258,\n",
       "       0.01089957, 0.01255695, 0.01054547, 0.01070195, 0.01263918,\n",
       "       0.01245002, 0.9746903 , 0.01059675, 0.01057495, 0.9690295 ,\n",
       "       0.01084203, 0.7770431 , 0.01036143, 0.01109154, 0.01260509,\n",
       "       0.11433417, 0.9753666 , 0.9612644 , 0.9647061 , 0.9745477 ,\n",
       "       0.03516741, 0.0126648 , 0.01065293, 0.9552764 , 0.96799254,\n",
       "       0.01058559, 0.2623781 , 0.01055039, 0.01051831, 0.01100845,\n",
       "       0.01088696, 0.01102789, 0.0105099 , 0.01104667, 0.01050273,\n",
       "       0.01095246, 0.01058758, 0.01144694, 0.01077527, 0.01048597,\n",
       "       0.0109021 , 0.01035762, 0.97349226, 0.01007261, 0.95575935,\n",
       "       0.01066424, 0.909326  , 0.01111981, 0.01060317, 0.0109562 ,\n",
       "       0.07121085, 0.01215005, 0.01038079, 0.01084228, 0.01035411,\n",
       "       0.010791  , 0.01076628, 0.02067578, 0.70697814, 0.01037293,\n",
       "       0.01067675, 0.0118441 , 0.97520465, 0.01072725, 0.01042643,\n",
       "       0.01156695, 0.01198263, 0.01098666, 0.0102431 , 0.01021303,\n",
       "       0.01099773, 0.0114558 , 0.0108203 , 0.9749826 , 0.01055908,\n",
       "       0.01090625, 0.97576106, 0.01107905, 0.02262759, 0.01075925,\n",
       "       0.11187533, 0.0103515 , 0.9737257 , 0.0107459 , 0.02289728,\n",
       "       0.02386005, 0.01070835, 0.01066782, 0.9244952 , 0.01256249,\n",
       "       0.01133101, 0.01051027, 0.01094854, 0.97494274, 0.01035866,\n",
       "       0.07851741, 0.01057288, 0.6330576 , 0.01019477, 0.9745189 ,\n",
       "       0.01687449, 0.01069494, 0.01071375, 0.01053038, 0.01083208,\n",
       "       0.97315514, 0.01082015, 0.972409  , 0.97454506, 0.01057656,\n",
       "       0.01216354, 0.00985797, 0.01162442, 0.01104206, 0.01124696,\n",
       "       0.01031411, 0.01043419, 0.01085192, 0.01089137, 0.01060256,\n",
       "       0.01065197, 0.01063391, 0.01110441, 0.01236638, 0.01066889,\n",
       "       0.97312397, 0.01106353, 0.9750504 , 0.66169417, 0.97499347,\n",
       "       0.01047742, 0.00994931, 0.02381784, 0.01038928, 0.01105355,\n",
       "       0.0104094 , 0.01113129, 0.01114144, 0.01090274, 0.01122065,\n",
       "       0.9750922 , 0.01058178, 0.97481596, 0.0110085 , 0.9618772 ,\n",
       "       0.01167595, 0.01148813, 0.01089834, 0.01278564, 0.01076445,\n",
       "       0.9736024 , 0.97442573, 0.01051001, 0.0124924 , 0.9499237 ,\n",
       "       0.9752853 , 0.96688366, 0.9753955 , 0.01034691, 0.01047204,\n",
       "       0.01362634, 0.01164529, 0.01088872, 0.97471046, 0.0108126 ,\n",
       "       0.01052053, 0.01068255, 0.97540265, 0.01236623, 0.01072564,\n",
       "       0.6496355 , 0.01052468, 0.01028863, 0.9747842 , 0.01063211,\n",
       "       0.01232062, 0.01061242, 0.9745908 , 0.97538054, 0.01032441,\n",
       "       0.97501945, 0.01033033, 0.01054072, 0.9754068 , 0.01093171,\n",
       "       0.01242887, 0.01062778, 0.97338146, 0.01082174, 0.9741615 ,\n",
       "       0.01038865, 0.01059227, 0.0105813 , 0.01098821, 0.97345483,\n",
       "       0.01046365, 0.01030533, 0.01076418, 0.9736519 , 0.01164494,\n",
       "       0.0106173 , 0.01042666, 0.54709494, 0.01064398, 0.0112305 ,\n",
       "       0.53115505, 0.97508913, 0.01069083, 0.973179  , 0.01071878,\n",
       "       0.01077425, 0.97616315, 0.7256943 , 0.01028402, 0.01131819,\n",
       "       0.01115092, 0.97524583, 0.9733802 , 0.01039995, 0.9748195 ,\n",
       "       0.9477593 , 0.01137968, 0.01535158, 0.0104668 , 0.01096989,\n",
       "       0.01159814, 0.02733469, 0.9745267 , 0.0284801 , 0.9757185 ,\n",
       "       0.01377491, 0.01093528, 0.01318423, 0.01068265, 0.01067681,\n",
       "       0.17975104, 0.88681215, 0.01088902, 0.01052107, 0.01072967,\n",
       "       0.01091837, 0.01097396, 0.01110778, 0.01092965, 0.9716898 ,\n",
       "       0.01099672, 0.9736788 , 0.01069166, 0.01051919, 0.04546372,\n",
       "       0.01062708, 0.974393  , 0.01095083, 0.01049415, 0.01127009,\n",
       "       0.0120831 , 0.01052189, 0.9701586 , 0.01048518, 0.01099187,\n",
       "       0.97411036, 0.01058283, 0.9385982 , 0.01063873, 0.01074498,\n",
       "       0.01118472, 0.01033749, 0.04155801, 0.01070414, 0.01075775,\n",
       "       0.04996433, 0.0110639 , 0.01401615, 0.01055599, 0.01068796,\n",
       "       0.0108986 , 0.01085161, 0.01045629, 0.01071346, 0.97591984,\n",
       "       0.97514105, 0.97470427, 0.01744665, 0.01112242, 0.01112567,\n",
       "       0.01175442, 0.01159871, 0.01100134, 0.01598841, 0.01061965,\n",
       "       0.0110113 , 0.9756412 , 0.0105112 , 0.01107926, 0.01099279,\n",
       "       0.01042836, 0.01081362, 0.9749706 , 0.01079397, 0.01049435,\n",
       "       0.01063104, 0.0110708 , 0.01056434, 0.01087032, 0.012003  ,\n",
       "       0.0104685 , 0.97498363, 0.01062013, 0.9627753 , 0.01065367,\n",
       "       0.01204617, 0.97494626, 0.011132  , 0.01043662, 0.01071613,\n",
       "       0.01037033, 0.01737796, 0.01040371, 0.01772548, 0.9665    ,\n",
       "       0.01151701, 0.97393733, 0.01110622, 0.9754496 , 0.01086978,\n",
       "       0.01051469, 0.0111032 , 0.01069539, 0.97166675, 0.01076014,\n",
       "       0.01092927, 0.01075761, 0.01269041, 0.01042538, 0.1649244 ,\n",
       "       0.01103608, 0.01072529, 0.01064454, 0.97201693, 0.9747423 ,\n",
       "       0.01127164, 0.01037947, 0.0106103 , 0.01069522, 0.01070186,\n",
       "       0.01055035, 0.01067193, 0.01069406, 0.01045643, 0.01101022,\n",
       "       0.01193095, 0.01451081, 0.01316748, 0.01011921, 0.97618514,\n",
       "       0.9712243 , 0.01087002, 0.9730525 , 0.01121326, 0.01257713,\n",
       "       0.9750056 , 0.01025386, 0.01076476], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "58b84233-9e78-44c6-b99d-932f91109f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {'Id':[i for i in range(len(predictions))], 'Predicted':predictions}\n",
    "pred_df = DataFrame(pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3574adbd-b4af-4ce4-b873-b03161c54ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.011037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.024299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.010734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.011140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.055705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>553</td>\n",
       "      <td>0.011213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>554</td>\n",
       "      <td>0.012577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>555</td>\n",
       "      <td>0.975006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>556</td>\n",
       "      <td>0.010254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>557</td>\n",
       "      <td>0.010765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>558 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  Predicted\n",
       "0      0   0.011037\n",
       "1      1   0.024299\n",
       "2      2   0.010734\n",
       "3      3   0.011140\n",
       "4      4   0.055705\n",
       "..   ...        ...\n",
       "553  553   0.011213\n",
       "554  554   0.012577\n",
       "555  555   0.975006\n",
       "556  556   0.010254\n",
       "557  557   0.010765\n",
       "\n",
       "[558 rows x 2 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8de5c313-831d-4d7f-bbe2-7ee219671ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.Predicted = pred_df.Predicted.apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d67d4be5-0389-48af-af69-3471cbc6bdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.Predicted.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6c4b7f3f-999d-4a6b-9b1a-a0adf646462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3fe9a67-e3d7-4dc3-aaa3-1cd5232428a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('predictions1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9e3e681-7d3e-403e-85d3-e669ae16c956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(a)):\n",
    "    if a.iloc[i].Predicted != pred_df.iloc[i].Predicted:\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70c29698-030b-4be7-918d-b8942f828d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(558, 256)\n",
      "(558, 24)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(test_seq).shape)\n",
    "print(np.array(test_stat).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e4a48f8-54cb-4ec0-a742-c9d19d74f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = torch.tensor(test_seq).long()\n",
    "test_mask = torch.tensor(test_mask).long()\n",
    "test_seg = torch.tensor(test_seg).long()\n",
    "test_stat = torch.tensor(np.array(test_stat)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b8c5b1-37c1-4333-a42b-ece0470ed583",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "test_stat = np.array(test_stat)\n",
    "for i in range(len(test_seq)):\n",
    "    seq = torch.tensor(test_seq[i]).long().unsqueeze(0)\n",
    "    mask = torch.tensor(test_mask[i]).long().unsqueeze(0)\n",
    "    seg = torch.tensor(test_seg[i]).long().unsqueeze(0)\n",
    "    stat = torch.tensor(test_stat[i,1:]).float().unsqueeze(0)\n",
    "    preds.append(model(seq, mask, seg, stat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e460c87-f926-4d28-bef7-93dc439938af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create Train and Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "feecfa1f-4cfe-498b-acdd-7ea02a534380",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_dev = []\n",
    "for i in range(train_stat.shape[0]):\n",
    "    X_train.append(list(train_tokens[i].reshape(-1)))\n",
    "    for j in range(1,train_stat.shape[1]):\n",
    "        X_train[i].append(train_stat.iloc[i,j])\n",
    "\n",
    "for i in range(dev_stat.shape[0]):\n",
    "    X_dev.append(list(dev_tokens[i].reshape(-1)))\n",
    "    for j in range(1,dev_stat.shape[1]):\n",
    "        X_dev[i].append(dev_stat.iloc[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4450b35a-db76-45dc-8139-435a7f25e6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1542"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "442b807a-76ac-4cb9-b6a1-f3c0fed6ad63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3382aaff-cdec-4aa9-9f64-4ae2cafae6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_tweet['label']\n",
    "y_dev = dev_tweet['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e435d-c046-4abf-bc48-8363c3aa35ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Test on simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e804cab2-226c-4c48-b725-5940eba222d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1e0a2bed-a9c8-4176-a6e9-dff543d9272b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.867816091954023"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(predictions, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7a2b01c3-66e6-45f3-bf3e-a0f6259d66fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.943579766536965"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds = lr.predict(X_train)\n",
    "accuracy_score(train_preds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "030b153a-0955-4dbc-9a82-38a0ca5f640b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6609195402298851"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "accuracy_score(gnb.predict(X_dev), y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9bdb3ba2-1573-4a79-94d2-371ccedffd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('X_train.pkl', 'wb') as file:\n",
    "    pickle.dump(X_train, file)\n",
    "with open('X_dev.pkl', 'wb') as file:\n",
    "    pickle.dump(X_dev, file)\n",
    "with open('y_train.pkl', 'wb') as file:\n",
    "    pickle.dump(list(y_train), file)\n",
    "with open('y_dev.pkl', 'wb') as file:\n",
    "    pickle.dump(list(y_dev), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a39838-4e40-4d32-a268-0445606cd164",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Test simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4d72b566-f8d7-4da6-8933-78a1bdd0ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mlp, self).__init__()\n",
    "        self.ffnn = nn.Sequential(nn.Linear(791,128),\n",
    "                                  nn.ReLU(),\n",
    "                                  # nn.Dropout(0.3),\n",
    "                                 nn.Linear(128,64),\n",
    "                                  nn.ReLU(),\n",
    "                                  # nn.Dropout(0.3),\n",
    "                                  nn.Linear(64,1),\n",
    "                                  nn.Sigmoid()\n",
    "                                 )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.ffnn(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ef89ef1c-17d2-416b-974e-71a914a25c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b334580150>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d50aff54-ec06-4038-8187-b2d2dc4e6033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "EPOCH = 5\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8d055d7b-773e-4d50-b92e-0155c0a54037",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_dev = np.array(X_dev)\n",
    "y_train = np.array(y_train)\n",
    "y_dev = np.array(y_dev)\n",
    "X = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "dev_X = torch.from_numpy(X_dev).type(torch.FloatTensor)\n",
    "dev_y = torch.from_numpy(y_dev).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "70a0b495-7d5c-40b5-bffb-76c7aaa151b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1542"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e33cc3f4-b096-4594-974b-ee9995d5c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "296dc716-edd7-4ce6-b663-efb7fe28f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TweetDataset(X,y)\n",
    "dev_set = TweetDataset(dev_X, dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f355687f-8290-42c9-9dda-f8e72d35acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = Data.DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "78c34c8c-9150-4c49-9ba5-e08d0e4d0d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, criterion, dataloader, device):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, labels in dataloader:\n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            logits = net(seq)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "7749fc1b-31f6-4621-977f-1b0815999566",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = mlp()\n",
    "net = net.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "opti = optim.Adam(net.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "14e9825f-acd4-417d-8e84-57894369eed6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. \n",
      " Loss: 0.02590068057179451; Accuracy: 1.0; Time taken (s): 0.004000186920166016\n",
      "Development Accuracy: 0.8802083134651184; Development Loss: 0.434159043762419\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "best_acc = 0\n",
    "st = time.time()\n",
    "eps = []\n",
    "t_loss = []\n",
    "d_loss = []\n",
    "for ep in range(1):\n",
    "    eps.append(ep)\n",
    "    net.train()\n",
    "    for it, (seq, labels) in enumerate(train_loader):\n",
    "        \n",
    "        #Clear gradients\n",
    "        opti.zero_grad()\n",
    "        #Converting these to cuda tensors\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "\n",
    "        #Obtaining the logits from the model\n",
    "        logits = net(seq)\n",
    "        \n",
    "        #Computing loss\n",
    "        loss = criterion(logits.squeeze(), labels.float())\n",
    "\n",
    "        #Backpropagating the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #Optimization step\n",
    "        opti.step()\n",
    "\n",
    "        if it % 100 == 0:\n",
    "\n",
    "            acc = get_accuracy_from_logits(logits, labels)\n",
    "            print(\"Iteration {} of epoch {} complete. \\n Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "        \n",
    "    dev_acc, dev_loss = evaluate(net, criterion, dev_loader, device)\n",
    "    t_loss.append(loss.item())\n",
    "    d_loss.append(dev_loss)\n",
    "    print(\"Development Accuracy: {}; Development Loss: {}\".format(dev_acc, dev_loss))\n",
    "    if dev_acc > best_acc:\n",
    "        # print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "        best_acc = dev_acc\n",
    "        # torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "289e761b-9b28-4e76-a02c-31eabbc776a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(eps,t_loss)\n",
    "plt.plot(eps,d_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "95f7cc04-ce28-417b-a1d4-41cc3bb75eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707ac33-63aa-4e81-b71e-604078a51291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
