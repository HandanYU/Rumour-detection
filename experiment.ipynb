{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a00ed975-afc9-4cbe-89e9-651cd7187fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# import sampler as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108ce5df-3d00-451b-8a38-48e2bf0a8e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reply_contributors', 'reply_possibly_sensitive_appealable', 'reply_favorited', 'reply_retweeted', 'reply_protected', 'reply_contributors_enabled', 'reply_is_translator', 'reply_following', 'reply_follow_request_sent', 'reply_notifications', 'possibly_sensitive_appealable', 'favorited', 'retweeted', 'protected', 'contributors_enabled', 'is_translator', 'following', 'follow_request_sent', 'notifications']\n",
      "['reply_contributors', 'reply_possibly_sensitive_appealable', 'reply_favorited', 'reply_retweeted', 'reply_protected', 'reply_contributors_enabled', 'reply_is_translator', 'reply_following', 'reply_follow_request_sent', 'reply_notifications', 'possibly_sensitive_appealable', 'favorited', 'retweeted', 'protected', 'contributors_enabled', 'is_translator', 'following', 'follow_request_sent', 'notifications']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>reply_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1240727985491193862</td>\n",
       "      <td>covid viru transmit area hot humid world healt...</td>\n",
       "      <td>humid good demonstr virus surviv temperatur hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>634943791934406657</td>\n",
       "      <td>marilyn monro jame dean smoke new york citi http</td>\n",
       "      <td>icon [SEP] http [SEP] yo puedo demostrar que e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1243967693297987584</td>\n",
       "      <td>symptom covid jcinigeria</td>\n",
       "      <td>symptom usual mild gradual common [SEP] infect...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1233175449980874752</td>\n",
       "      <td>coronaviru wear mask protect covid</td>\n",
       "      <td>thank [SEP]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1245592346344841216</td>\n",
       "      <td>symptom covid let watch new episod q covid know</td>\n",
       "      <td>infect peopl around world believ togeth stop [...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id                                               text  \\\n",
       "0  1240727985491193862  covid viru transmit area hot humid world healt...   \n",
       "1   634943791934406657   marilyn monro jame dean smoke new york citi http   \n",
       "2  1243967693297987584                           symptom covid jcinigeria   \n",
       "3  1233175449980874752                 coronaviru wear mask protect covid   \n",
       "4  1245592346344841216    symptom covid let watch new episod q covid know   \n",
       "\n",
       "                                          reply_text  label  \n",
       "0  humid good demonstr virus surviv temperatur hi...      0  \n",
       "1  icon [SEP] http [SEP] yo puedo demostrar que e...      1  \n",
       "2  symptom usual mild gradual common [SEP] infect...      0  \n",
       "3                                       thank [SEP]       0  \n",
       "4  infect peopl around world believ togeth stop [...      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stat = pd.read_csv('./sep_data/train_scaled_stat_feat_df.csv')\n",
    "dev_stat = pd.read_csv('./sep_data/dev_scaled_stat_feat_df.csv')\n",
    "\n",
    "train_tweet = pd.read_csv('./sep_data/train_tweet_df.csv')\n",
    "dev_tweet = pd.read_csv('./sep_data/dev_tweet_df.csv')\n",
    "\n",
    "# train_stat.info()\n",
    "\n",
    "# dev_stat.info()\n",
    "\n",
    "train_stat.drop(columns=['Unnamed: 0','label'], inplace=True)\n",
    "train_stat.head()\n",
    "\n",
    "dev_stat.drop(columns=['Unnamed: 0','label'], inplace=True)\n",
    "dev_stat.head()\n",
    "\n",
    "train_stat.sum()\n",
    "\n",
    "dev_zero = []\n",
    "for column in dev_stat.columns:\n",
    "    if dev_stat[column].sum() == 0:\n",
    "        dev_zero.append(column)\n",
    "print(dev_zero)\n",
    "\n",
    "train_zero = []\n",
    "for column in train_stat.columns:\n",
    "    if train_stat[column].sum() == 0:\n",
    "        train_zero.append(column)\n",
    "print(train_zero)\n",
    "\n",
    "train_stat.drop(columns=train_zero, inplace=True)\n",
    "dev_stat.drop(columns=dev_zero, inplace=True)\n",
    "\n",
    "dev_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d9d7b-b4cc-4144-8a0b-ac2e1f50d586",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fill NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9ddbb0-fe88-4ff9-9fc3-ea6ee1e85a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 536 entries, 0 to 535\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweet_id    536 non-null    int64 \n",
      " 1   text        536 non-null    object\n",
      " 2   reply_text  536 non-null    object\n",
      " 3   label       536 non-null    int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 16.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# dev_tweet.info()\n",
    "\n",
    "# train_tweet.info()\n",
    "\n",
    "dev_tweet.reply_text.fillna('', inplace=True)\n",
    "train_tweet.reply_text.fillna('', inplace=True)\n",
    "dev_tweet.text.fillna('', inplace=True)\n",
    "train_tweet.text.fillna('', inplace=True)\n",
    "\n",
    "dev_tweet.info()\n",
    "\n",
    "# dev_tweet.iloc[0].reply_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d11469-56b1-4ba0-b50f-96208aabf747",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a78af90-9ac2-4e12-9d87-03aae7cdf566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/536 [00:00<?, ?it/s]C:\\Users\\trist\\AppData\\Local\\Temp\\ipykernel_28584\\2101879647.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dev_tweet.text.iloc[i] = '[CLS] ' + str(dev_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(dev_tweet.reply_text.iloc[i]).strip() + ' [SEP]'\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 536/536 [00:08<00:00, 66.90it/s]\n",
      "  0%|                                                                                         | 0/1579 [00:00<?, ?it/s]C:\\Users\\trist\\AppData\\Local\\Temp\\ipykernel_28584\\2101879647.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_tweet.text.iloc[i] = '[CLS] ' + str(train_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(train_tweet.reply_text.iloc[i]).strip() + ' [SEP]'\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1579/1579 [00:19<00:00, 80.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] covid viru transmit area hot humid world health organ [SEP] humid good demonstr virus surviv temperatur high [SEP] mean warm weather good elimin [SEP]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in tqdm(range(len(dev_tweet))):\n",
    "    dev_tweet.text.iloc[i] = '[CLS] ' + str(dev_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(dev_tweet.reply_text.iloc[i]).strip() + ' [SEP]'\n",
    "\n",
    "for i in tqdm(range(len(train_tweet))):\n",
    "    train_tweet.text.iloc[i] = '[CLS] ' + str(train_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(train_tweet.reply_text.iloc[i]).strip() + ' [SEP]'\n",
    "\n",
    "dev_tweet.iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d08ad6-df37-4712-ba21-e1eaf42f1ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "# bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee25759-322a-420b-8a7d-84b2be197b6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9aadc52e-0850-448b-8282-6b35952d9d75",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▋                                                                              | 18/536 [00:07<03:44,  2.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [139]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m attn_mask_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(attn_mask)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m#Shape : [1, 12]\u001b[39;00m\n\u001b[0;32m     25\u001b[0m seg_ids_t   \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(seg_ids)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m#Shape : [1, 12]\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_mask_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseg_ids_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m cont_reps \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m     29\u001b[0m cls_rep \u001b[38;5;241m=\u001b[39m cont_reps[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    987\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    989\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    990\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    991\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    995\u001b[0m )\n\u001b[1;32m--> 996\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1008\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1009\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    576\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    577\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    578\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    583\u001b[0m     )\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 585\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:513\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    510\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    511\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 513\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\modeling_utils.py:2928\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m   2925\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m   2926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m-> 2928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:526\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    525\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 526\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 439\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    441\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[1;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_len = 256\n",
    "dev_tokens = []\n",
    "train_tokens = []\n",
    "\n",
    "for i in tqdm(range(len(dev_tweet))):\n",
    "    txt = dev_tweet.text.iloc[i]\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    if len(tokens) < max_len:\n",
    "         padded_tokens = tokens + ['[PAD]' for _ in range(max_len - len(tokens))]\n",
    "    else:\n",
    "        padded_tokens = tokens[:max_len-1] + ['[SEP]']\n",
    "    attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "    seg_ids = []\n",
    "    seg_idx = 0\n",
    "    for token in padded_tokens:\n",
    "        seg_ids.append(seg_idx)\n",
    "        if token == '[SEP]':\n",
    "            if seg_idx == 1:\n",
    "                seg_idx = 0\n",
    "            else:\n",
    "                seg_idx=1\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "    token_ids_t = torch.tensor(token_ids).unsqueeze(0) #Shape : [1, 12]\n",
    "    attn_mask_t = torch.tensor(attn_mask).unsqueeze(0) #Shape : [1, 12]\n",
    "    seg_ids_t   = torch.tensor(seg_ids).unsqueeze(0) #Shape : [1, 12]\n",
    "    outputs = bert_model(token_ids_t, attention_mask = attn_mask_t,\\\n",
    "                                  token_type_ids = seg_ids_t, return_dict=True)\n",
    "    cont_reps = outputs.last_hidden_state\n",
    "    cls_rep = cont_reps[:, 0]\n",
    "    dev_tokens.append(cls_rep.detach().numpy())\n",
    "\n",
    "for i in tqdm(range(len(train_tweet))):\n",
    "    txt = train_tweet.text.iloc[i]\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    if len(tokens) < max_len:\n",
    "         padded_tokens = tokens + ['[PAD]' for _ in range(max_len - len(tokens))]\n",
    "    else:\n",
    "        padded_tokens = tokens[:max_len-1] + ['[SEP]']\n",
    "    attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "    seg_ids = [1 if token == '[SEP]' else 0 for token in padded_tokens]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "    token_ids_t = torch.tensor(token_ids).unsqueeze(0)\n",
    "    attn_mask_t = torch.tensor(attn_mask).unsqueeze(0)\n",
    "    seg_ids_t   = torch.tensor(seg_ids).unsqueeze(0)\n",
    "    outputs = bert_model(token_ids_t, attention_mask = attn_mask_t,\\\n",
    "                                  token_type_ids = seg_ids_t, return_dict=True)\n",
    "    cont_reps = outputs.last_hidden_state\n",
    "    cls_rep = cont_reps[:, 0]\n",
    "    train_tokens.append(cls_rep.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6fad0a-794c-4b6c-8958-07e9a4da423d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### BERT seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9adfbd5-67e1-4334-8ccf-4cb9af1b7580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 536/536 [00:01<00:00, 270.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1579/1579 [00:04<00:00, 332.47it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 256\n",
    "dev_seq = []\n",
    "train_seq = []\n",
    "dev_mask = []\n",
    "train_mask = []\n",
    "dev_seg = []\n",
    "train_seg = []\n",
    "\n",
    "for i in tqdm(range(len(dev_tweet))):\n",
    "    txt = dev_tweet.text.iloc[i]\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    if len(tokens) < max_len:\n",
    "         padded_tokens = tokens + ['[PAD]' for _ in range(max_len - len(tokens))]\n",
    "    else:\n",
    "        padded_tokens = tokens[:max_len-1] + ['[SEP]']\n",
    "    attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "    seg_ids = []\n",
    "    seg_idx = 0\n",
    "    for token in padded_tokens:\n",
    "        seg_ids.append(seg_idx)\n",
    "        if token == '[SEP]':\n",
    "            if seg_idx == 1:\n",
    "                seg_idx = 0\n",
    "            else:\n",
    "                seg_idx=1\n",
    "    # seg_ids = [1 if token == '[SEP]' else 0 for token in padded_tokens]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "    \n",
    "    dev_seq.append(token_ids)\n",
    "    dev_mask.append(attn_mask)\n",
    "    dev_seg.append(seg_ids)\n",
    "\n",
    "for i in tqdm(range(len(train_tweet))):\n",
    "    txt = train_tweet.text.iloc[i]\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    if len(tokens) < max_len:\n",
    "         padded_tokens = tokens + ['[PAD]' for _ in range(max_len - len(tokens))]\n",
    "    else:\n",
    "        padded_tokens = tokens[:max_len-1] + ['[SEP]']\n",
    "    attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "    seg_ids = []\n",
    "    seg_idx = 0\n",
    "    for token in padded_tokens:\n",
    "        seg_ids.append(seg_idx)\n",
    "        if token == '[SEP]':\n",
    "            if seg_idx == 1:\n",
    "                seg_idx = 0\n",
    "            else:\n",
    "                seg_idx=1\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "    \n",
    "    train_seq.append(token_ids)\n",
    "    train_mask.append(attn_mask)\n",
    "    train_seg.append(seg_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c85e1-9dd7-467c-a3f4-3d49fdbc43a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### test BERT adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d14e6939-428d-4eb8-b857-e85c5999baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdTweetDataset(Data.Dataset):\n",
    "    def __init__(self, seq_, mask_, seg_, y_):\n",
    "        self.seq = torch.tensor(seq_)\n",
    "        self.mask = torch.tensor(mask_)\n",
    "        self.seg = torch.tensor(seg_)\n",
    "        self.y = torch.tensor(y_)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx],self.mask[idx],self.seg[idx], self.y[idx], idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c982771-f7de-4e1b-b6ca-25dc0b33d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_tweet['label']\n",
    "y_dev = dev_tweet['label']\n",
    "w_nonr = len(y_train)/(len(y_train)-y_train.sum())\n",
    "w_r = len(y_train)/(y_train.sum())\n",
    "weights = []\n",
    "for l in y_train:\n",
    "    if l == 0:\n",
    "        weights.append(w_nonr)\n",
    "    else:\n",
    "        weights.append(w_r)\n",
    "weights = torch.FloatTensor(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23186e80-a34a-4888-b065-20a04b64826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_set = AdTweetDataset(train_seq, train_mask, train_seg,y_train)\n",
    "dev_set = AdTweetDataset(dev_seq, dev_mask, dev_seg, y_dev)\n",
    "\n",
    "# sampler_s = sp.StratifiedSampler(class_vector=torch.from_numpy(np.array(y_train)), batch_size=64)\n",
    "train_sampler = Data.WeightedRandomSampler(weights, len(train_set), replacement=True)\n",
    "train_loader = Data.DataLoader(train_set, sampler=train_sampler,batch_size=64)\n",
    "# train_loader = Data.DataLoader(train_set,batch_size=64,shuffle=True)\n",
    "dev_loader = Data.DataLoader(dev_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac4ee5f3-9a21-4305-9296-146e7ae607a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RumorClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RumorClassifier, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        self.ffnn = nn.Sequential(nn.Linear(811,128),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3),\n",
    "                                 nn.Linear(128,64),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3),\n",
    "                                  nn.Linear(64,1),\n",
    "                                  nn.Sigmoid()\n",
    "                                 )\n",
    "\n",
    "    def forward(self, seq_,attn_masks, seg_, stats):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq_, attention_mask = attn_masks,token_type_ids = seg_, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "        \n",
    "        x = torch.cat((cls_rep,stats),dim=1)\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.ffnn(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a15bb73-449a-438b-9037-7e7db3321991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.empty_cache ()\n",
    "net = RumorClassifier()\n",
    "# net = net.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99dd698a-ed70-46b8-a860-bb755cba759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = logits.unsqueeze(-1)\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "def get_f1_from_logits(logits, labels):\n",
    "    preds = (logits > 0.5).astype(int)\n",
    "    p, r, f, _ = precision_recall_fscore_support(labels, preds, pos_label=1, average=\"binary\")\n",
    "    return f\n",
    "\n",
    "def evaluate(net, criterion, dataloader, device):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "    all_log = np.array([])\n",
    "    all_labels = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for seq, mask, seg, labels, idx in dataloader:\n",
    "            # seq, labels = seq.to(device), labels.to(device)\n",
    "            stats = np.array(train_stat)\n",
    "            stats = torch.tensor(stats[idx]).float()\n",
    "            #Obtaining the logits from the model\n",
    "            logits = net(seq, mask, seg, stats)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            # mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            \n",
    "            all_log = np.hstack((all_log, logits.squeeze()))\n",
    "            all_labels = np.hstack((all_labels, labels.numpy()))\n",
    "            count += 1\n",
    "        \n",
    "        f = get_f1_from_logits(all_log, all_labels)\n",
    "    return f, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9666cce-201a-4823-ba27-db02fc97e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "best_acc = 0\n",
    "st = time.time()\n",
    "eps = []\n",
    "t_loss = []\n",
    "d_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebeadff2-f94c-405b-a565-edc35d3313c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. \n",
      " Loss: 0.6907795071601868; Accuracy: 0.546875; Time taken (s): 37.0798876285553\n",
      "Iteration 10 of epoch 0 complete. \n",
      " Loss: 0.6650289297103882; Accuracy: 0.78125; Time taken (s): 366.09376072883606\n",
      "Iteration 20 of epoch 0 complete. \n",
      " Loss: 0.6258490681648254; Accuracy: 0.75; Time taken (s): 364.69323897361755\n",
      "Development F1: 0.5406976744186047; Development Loss: 0.5908371806144714\n",
      "Iteration 0 of epoch 1 complete. \n",
      " Loss: 0.5946479439735413; Accuracy: 0.75; Time taken (s): 414.4184765815735\n",
      "Iteration 10 of epoch 1 complete. \n",
      " Loss: 0.5516828894615173; Accuracy: 0.765625; Time taken (s): 398.88207840919495\n",
      "Iteration 20 of epoch 1 complete. \n",
      " Loss: 0.5077426433563232; Accuracy: 0.8125; Time taken (s): 367.97492384910583\n",
      "Development F1: 0.5733788395904437; Development Loss: 0.5149298906326294\n",
      "Iteration 0 of epoch 2 complete. \n",
      " Loss: 0.43578824400901794; Accuracy: 0.875; Time taken (s): 285.096022605896\n",
      "Iteration 10 of epoch 2 complete. \n",
      " Loss: 0.38304707407951355; Accuracy: 0.90625; Time taken (s): 502.8556990623474\n",
      "Iteration 20 of epoch 2 complete. \n",
      " Loss: 0.3216489553451538; Accuracy: 0.9375; Time taken (s): 385.85989236831665\n",
      "Development F1: 0.6457680250783698; Development Loss: 0.48761268456776935\n",
      "Iteration 0 of epoch 3 complete. \n",
      " Loss: 0.39715614914894104; Accuracy: 0.859375; Time taken (s): 288.64862990379333\n",
      "Iteration 10 of epoch 3 complete. \n",
      " Loss: 0.30276110768318176; Accuracy: 0.9375; Time taken (s): 372.8054747581482\n",
      "Iteration 20 of epoch 3 complete. \n",
      " Loss: 0.43300703167915344; Accuracy: 0.859375; Time taken (s): 491.29602432250977\n",
      "Development F1: 0.7054794520547946; Development Loss: 0.43063854840066695\n",
      "Iteration 0 of epoch 4 complete. \n",
      " Loss: 0.34463346004486084; Accuracy: 0.875; Time taken (s): 300.5821611881256\n",
      "Iteration 10 of epoch 4 complete. \n",
      " Loss: 0.29529067873954773; Accuracy: 0.90625; Time taken (s): 369.31375432014465\n",
      "Iteration 20 of epoch 4 complete. \n",
      " Loss: 0.163397416472435; Accuracy: 0.984375; Time taken (s): 371.28168058395386\n",
      "Development F1: 0.7756653992395436; Development Loss: 0.32526226176155937\n",
      "Iteration 0 of epoch 5 complete. \n",
      " Loss: 0.1883888691663742; Accuracy: 0.96875; Time taken (s): 398.0545988082886\n",
      "Iteration 10 of epoch 5 complete. \n",
      " Loss: 0.29653918743133545; Accuracy: 0.890625; Time taken (s): 397.4311110973358\n",
      "Iteration 20 of epoch 5 complete. \n",
      " Loss: 0.15077939629554749; Accuracy: 0.96875; Time taken (s): 375.45315527915955\n",
      "Development F1: 0.7358490566037736; Development Loss: 0.2868502578801579\n",
      "Iteration 0 of epoch 6 complete. \n",
      " Loss: 0.21200935542583466; Accuracy: 0.9375; Time taken (s): 294.2549533843994\n",
      "Iteration 10 of epoch 6 complete. \n",
      " Loss: 0.2561756372451782; Accuracy: 0.921875; Time taken (s): 493.625848531723\n",
      "Iteration 20 of epoch 6 complete. \n",
      " Loss: 0.13134954869747162; Accuracy: 0.984375; Time taken (s): 382.61061835289\n",
      "Development F1: 0.7860262008733624; Development Loss: 0.26150132053428227\n",
      "Iteration 0 of epoch 7 complete. \n",
      " Loss: 0.1213211789727211; Accuracy: 0.984375; Time taken (s): 291.9037666320801\n",
      "Iteration 10 of epoch 7 complete. \n",
      " Loss: 0.1132272407412529; Accuracy: 0.984375; Time taken (s): 371.67405676841736\n",
      "Iteration 20 of epoch 7 complete. \n",
      " Loss: 0.22950825095176697; Accuracy: 0.9375; Time taken (s): 501.3328332901001\n",
      "Development F1: 0.7906976744186046; Development Loss: 0.24653262396653494\n",
      "Iteration 0 of epoch 8 complete. \n",
      " Loss: 0.22799840569496155; Accuracy: 0.953125; Time taken (s): 294.3361625671387\n",
      "Iteration 10 of epoch 8 complete. \n",
      " Loss: 0.15208233892917633; Accuracy: 0.96875; Time taken (s): 369.1846752166748\n",
      "Iteration 20 of epoch 8 complete. \n",
      " Loss: 0.11835531145334244; Accuracy: 0.984375; Time taken (s): 380.306941986084\n",
      "Development F1: 0.8333333333333334; Development Loss: 0.2302199842201339\n",
      "Iteration 0 of epoch 9 complete. \n",
      " Loss: 0.09894954413175583; Accuracy: 0.984375; Time taken (s): 404.96080446243286\n",
      "Iteration 10 of epoch 9 complete. \n",
      " Loss: 0.18026341497898102; Accuracy: 0.921875; Time taken (s): 402.4094169139862\n",
      "Iteration 20 of epoch 9 complete. \n",
      " Loss: 0.12324472516775131; Accuracy: 0.984375; Time taken (s): 395.22191166877747\n",
      "Development F1: 0.8090909090909091; Development Loss: 0.23989737861686283\n",
      "Iteration 0 of epoch 10 complete. \n",
      " Loss: 0.27518126368522644; Accuracy: 0.9375; Time taken (s): 311.4915919303894\n",
      "Iteration 10 of epoch 10 complete. \n",
      " Loss: 0.21211667358875275; Accuracy: 0.9375; Time taken (s): 452.23881435394287\n",
      "Iteration 20 of epoch 10 complete. \n",
      " Loss: 0.15408214926719666; Accuracy: 0.953125; Time taken (s): 376.04612016677856\n",
      "Development F1: 0.825910931174089; Development Loss: 0.2870526413122813\n",
      "Iteration 0 of epoch 11 complete. \n",
      " Loss: 0.1320035606622696; Accuracy: 0.96875; Time taken (s): 292.14920806884766\n",
      "Iteration 10 of epoch 11 complete. \n",
      " Loss: 0.11442270129919052; Accuracy: 0.96875; Time taken (s): 372.4296295642853\n",
      "Iteration 20 of epoch 11 complete. \n",
      " Loss: 0.16579985618591309; Accuracy: 0.96875; Time taken (s): 370.1772503852844\n",
      "Development F1: 0.8597285067873303; Development Loss: 0.19922231758634248\n",
      "Iteration 0 of epoch 12 complete. \n",
      " Loss: 0.14583297073841095; Accuracy: 0.96875; Time taken (s): 291.9313995838165\n",
      "Iteration 10 of epoch 12 complete. \n",
      " Loss: 0.03930303454399109; Accuracy: 1.0; Time taken (s): 369.295392036438\n",
      "Iteration 20 of epoch 12 complete. \n",
      " Loss: 0.04430251568555832; Accuracy: 1.0; Time taken (s): 372.969797372818\n",
      "Development F1: 0.8490566037735848; Development Loss: 0.2121197630961736\n",
      "Iteration 0 of epoch 13 complete. \n",
      " Loss: 0.03672125190496445; Accuracy: 1.0; Time taken (s): 290.49694204330444\n",
      "Iteration 10 of epoch 13 complete. \n",
      " Loss: 0.04144661873579025; Accuracy: 1.0; Time taken (s): 370.90389251708984\n",
      "Iteration 20 of epoch 13 complete. \n",
      " Loss: 0.03522539138793945; Accuracy: 1.0; Time taken (s): 369.7039647102356\n",
      "Development F1: 0.7755102040816325; Development Loss: 0.2718058607230584\n",
      "Iteration 0 of epoch 14 complete. \n",
      " Loss: 0.08273792266845703; Accuracy: 0.984375; Time taken (s): 290.1716077327728\n",
      "Iteration 10 of epoch 14 complete. \n",
      " Loss: 0.0320793092250824; Accuracy: 1.0; Time taken (s): 368.25967288017273\n",
      "Iteration 20 of epoch 14 complete. \n",
      " Loss: 0.03775941580533981; Accuracy: 1.0; Time taken (s): 371.46489334106445\n",
      "Development F1: 0.803030303030303; Development Loss: 0.3279392123222351\n",
      "Iteration 0 of epoch 15 complete. \n",
      " Loss: 0.06607475131750107; Accuracy: 0.984375; Time taken (s): 291.44633412361145\n",
      "Iteration 10 of epoch 15 complete. \n",
      " Loss: 0.024171486496925354; Accuracy: 1.0; Time taken (s): 366.9814841747284\n",
      "Iteration 20 of epoch 15 complete. \n",
      " Loss: 0.022314298897981644; Accuracy: 1.0; Time taken (s): 368.6245594024658\n",
      "Development F1: 0.8761061946902654; Development Loss: 0.1986982532673412\n",
      "Iteration 0 of epoch 16 complete. \n",
      " Loss: 0.02320295013487339; Accuracy: 1.0; Time taken (s): 290.4058747291565\n",
      "Iteration 10 of epoch 16 complete. \n",
      " Loss: 0.02552439086139202; Accuracy: 1.0; Time taken (s): 371.8927745819092\n",
      "Iteration 20 of epoch 16 complete. \n",
      " Loss: 0.018454652279615402; Accuracy: 1.0; Time taken (s): 369.3135871887207\n",
      "Development F1: 0.8727272727272727; Development Loss: 0.1976412609219551\n",
      "Iteration 0 of epoch 17 complete. \n",
      " Loss: 0.07621150463819504; Accuracy: 0.984375; Time taken (s): 302.304625749588\n",
      "Iteration 10 of epoch 17 complete. \n",
      " Loss: 0.022149140015244484; Accuracy: 1.0; Time taken (s): 380.95265316963196\n",
      "Iteration 20 of epoch 17 complete. \n",
      " Loss: 0.13928095996379852; Accuracy: 0.96875; Time taken (s): 381.2784547805786\n",
      "Development F1: 0.8430493273542601; Development Loss: 0.23024226476748785\n",
      "Iteration 0 of epoch 18 complete. \n",
      " Loss: 0.014455141499638557; Accuracy: 1.0; Time taken (s): 297.03764629364014\n",
      "Iteration 10 of epoch 18 complete. \n",
      " Loss: 0.016884036362171173; Accuracy: 1.0; Time taken (s): 377.78322553634644\n",
      "Iteration 20 of epoch 18 complete. \n",
      " Loss: 0.016858402639627457; Accuracy: 1.0; Time taken (s): 383.6544830799103\n",
      "Development F1: 0.7653061224489797; Development Loss: 0.29310059630208546\n",
      "Iteration 0 of epoch 19 complete. \n",
      " Loss: 0.01634720154106617; Accuracy: 1.0; Time taken (s): 292.3762209415436\n",
      "Iteration 10 of epoch 19 complete. \n",
      " Loss: 0.019909877330064774; Accuracy: 1.0; Time taken (s): 370.9675786495209\n",
      "Iteration 20 of epoch 19 complete. \n",
      " Loss: 0.01328907161951065; Accuracy: 1.0; Time taken (s): 367.7763624191284\n",
      "Development F1: 0.8582995951417004; Development Loss: 0.30423446993033093\n"
     ]
    }
   ],
   "source": [
    "for ep in range(20):\n",
    "    eps.append(ep)\n",
    "    net.train()\n",
    "    for it, (seq, mask, seg, labels,idx) in enumerate(train_loader):\n",
    "        \n",
    "        #Clear gradients\n",
    "        opti.zero_grad()\n",
    "        #Converting these to cuda tensors\n",
    "        # seq, mask, seg, labels = seq.to(device), mask.to(device), seg.to(device), labels.to(device)\n",
    "\n",
    "        stats = np.array(train_stat)\n",
    "        stats = torch.tensor(stats[idx]).float()\n",
    "        #Obtaining the logits from the model\n",
    "        logits = net(seq, mask, seg, stats)\n",
    "        #Computing loss\n",
    "        loss = criterion(logits.squeeze(), labels.float())\n",
    "\n",
    "        #Backpropagating the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #Optimization step\n",
    "        opti.step()\n",
    "\n",
    "        if it % 10 == 0:\n",
    "\n",
    "            acc = get_accuracy_from_logits(logits, labels)\n",
    "            print(\"Iteration {} of epoch {} complete. \\n Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "        \n",
    "    dev_acc, dev_loss = evaluate(net, criterion, dev_loader, 'cpu')\n",
    "    t_loss.append(loss.item())\n",
    "    d_loss.append(dev_loss)\n",
    "    print(\"Development F1: {}; Development Loss: {}\".format(dev_acc, dev_loss))\n",
    "    torch.save(net.state_dict(), 'D:\\\\bertcls_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06fd0ef9-7d68-4f85-9619-5a712cf7468a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 20 complete. \n",
      " Loss: 0.019885603338479996; Accuracy: 1.0; Time taken (s): 246.2210726737976\n",
      "Iteration 10 of epoch 20 complete. \n",
      " Loss: 0.015068178065121174; Accuracy: 1.0; Time taken (s): 364.4286153316498\n",
      "Iteration 20 of epoch 20 complete. \n",
      " Loss: 0.014043761417269707; Accuracy: 1.0; Time taken (s): 456.5346965789795\n",
      "Development F1: 0.8034934497816594; Development Loss: 0.32976017230086857\n",
      "Iteration 0 of epoch 21 complete. \n",
      " Loss: 0.02222348563373089; Accuracy: 1.0; Time taken (s): 248.7010898590088\n",
      "Iteration 10 of epoch 21 complete. \n",
      " Loss: 0.020274026319384575; Accuracy: 1.0; Time taken (s): 303.29566860198975\n",
      "Iteration 20 of epoch 21 complete. \n",
      " Loss: 0.015691112726926804; Accuracy: 1.0; Time taken (s): 309.9355397224426\n",
      "Development F1: 0.8127853881278538; Development Loss: 0.2758317864499986\n",
      "Iteration 0 of epoch 22 complete. \n",
      " Loss: 0.08303241431713104; Accuracy: 0.984375; Time taken (s): 304.21868419647217\n",
      "Iteration 10 of epoch 22 complete. \n",
      " Loss: 0.015413559041917324; Accuracy: 1.0; Time taken (s): 457.6564712524414\n",
      "Iteration 20 of epoch 22 complete. \n",
      " Loss: 0.05906928703188896; Accuracy: 0.984375; Time taken (s): 304.9207911491394\n",
      "Development F1: 0.8270042194092827; Development Loss: 0.28376719179666704\n",
      "Iteration 0 of epoch 23 complete. \n",
      " Loss: 0.08762548118829727; Accuracy: 0.984375; Time taken (s): 247.13635396957397\n",
      "Iteration 10 of epoch 23 complete. \n",
      " Loss: 0.014156580902636051; Accuracy: 1.0; Time taken (s): 303.74406909942627\n",
      "Iteration 20 of epoch 23 complete. \n",
      " Loss: 0.01193959265947342; Accuracy: 1.0; Time taken (s): 390.6635046005249\n",
      "Development F1: 0.8255319148936171; Development Loss: 0.30597007233235574\n",
      "Iteration 0 of epoch 24 complete. \n",
      " Loss: 0.015071961097419262; Accuracy: 1.0; Time taken (s): 373.2922513484955\n",
      "Iteration 10 of epoch 24 complete. \n",
      " Loss: 0.16163405776023865; Accuracy: 0.96875; Time taken (s): 314.5331063270569\n",
      "Iteration 20 of epoch 24 complete. \n",
      " Loss: 0.014783065766096115; Accuracy: 1.0; Time taken (s): 304.26830315589905\n",
      "Development F1: 0.8154506437768241; Development Loss: 0.3153880031572448\n",
      "Iteration 0 of epoch 25 complete. \n",
      " Loss: 0.012907586991786957; Accuracy: 1.0; Time taken (s): 247.12117624282837\n",
      "Iteration 10 of epoch 25 complete. \n",
      " Loss: 0.08658713102340698; Accuracy: 0.984375; Time taken (s): 388.495600938797\n",
      "Iteration 20 of epoch 25 complete. \n",
      " Loss: 0.08797440677881241; Accuracy: 0.984375; Time taken (s): 444.62221026420593\n",
      "Development F1: 0.8220338983050848; Development Loss: 0.3235528651210997\n",
      "Iteration 0 of epoch 26 complete. \n",
      " Loss: 0.009531870484352112; Accuracy: 1.0; Time taken (s): 245.6775255203247\n",
      "Iteration 10 of epoch 26 complete. \n",
      " Loss: 0.10084151476621628; Accuracy: 0.984375; Time taken (s): 303.85618233680725\n",
      "Iteration 20 of epoch 26 complete. \n",
      " Loss: 0.014829369261860847; Accuracy: 1.0; Time taken (s): 303.41104912757874\n",
      "Development F1: 0.8070175438596492; Development Loss: 0.3228878262970183\n",
      "Iteration 0 of epoch 27 complete. \n",
      " Loss: 0.011221525259315968; Accuracy: 1.0; Time taken (s): 252.37632632255554\n",
      "Iteration 10 of epoch 27 complete. \n",
      " Loss: 0.011448283679783344; Accuracy: 1.0; Time taken (s): 304.37904691696167\n",
      "Iteration 20 of epoch 27 complete. \n",
      " Loss: 0.0925203412771225; Accuracy: 0.984375; Time taken (s): 302.5675551891327\n",
      "Development F1: 0.808695652173913; Development Loss: 0.3628538300593694\n",
      "Iteration 0 of epoch 28 complete. \n",
      " Loss: 0.010461912490427494; Accuracy: 1.0; Time taken (s): 245.39943170547485\n",
      "Iteration 10 of epoch 28 complete. \n",
      " Loss: 0.10049627721309662; Accuracy: 0.984375; Time taken (s): 302.95568680763245\n",
      "Iteration 20 of epoch 28 complete. \n",
      " Loss: 0.013972880318760872; Accuracy: 1.0; Time taken (s): 303.3659427165985\n",
      "Development F1: 0.8135593220338982; Development Loss: 0.355592323674096\n",
      "Iteration 0 of epoch 29 complete. \n",
      " Loss: 0.012208727188408375; Accuracy: 1.0; Time taken (s): 248.34082889556885\n",
      "Iteration 10 of epoch 29 complete. \n",
      " Loss: 0.009290837682783604; Accuracy: 1.0; Time taken (s): 304.03312253952026\n",
      "Iteration 20 of epoch 29 complete. \n",
      " Loss: 0.01629195362329483; Accuracy: 1.0; Time taken (s): 309.3891339302063\n",
      "Development F1: 0.8211382113821138; Development Loss: 0.354914559258355\n",
      "Iteration 0 of epoch 30 complete. \n",
      " Loss: 0.009985179640352726; Accuracy: 1.0; Time taken (s): 246.72114539146423\n",
      "Iteration 10 of epoch 30 complete. \n",
      " Loss: 0.08295148611068726; Accuracy: 0.984375; Time taken (s): 302.49599862098694\n",
      "Iteration 20 of epoch 30 complete. \n",
      " Loss: 0.010746770538389683; Accuracy: 1.0; Time taken (s): 304.997713804245\n",
      "Development F1: 0.8; Development Loss: 0.3106331398917569\n",
      "Iteration 0 of epoch 31 complete. \n",
      " Loss: 0.012927914038300514; Accuracy: 1.0; Time taken (s): 245.40931391716003\n",
      "Iteration 10 of epoch 31 complete. \n",
      " Loss: 0.01420043595135212; Accuracy: 1.0; Time taken (s): 303.50465416908264\n",
      "Iteration 20 of epoch 31 complete. \n",
      " Loss: 0.058363381773233414; Accuracy: 0.984375; Time taken (s): 304.5951814651489\n",
      "Development F1: 0.8138528138528138; Development Loss: 0.33783677385913\n",
      "Iteration 0 of epoch 32 complete. \n",
      " Loss: 0.01577906683087349; Accuracy: 1.0; Time taken (s): 246.53348898887634\n",
      "Iteration 10 of epoch 32 complete. \n",
      " Loss: 0.0076201907359063625; Accuracy: 1.0; Time taken (s): 303.21191334724426\n",
      "Iteration 20 of epoch 32 complete. \n",
      " Loss: 0.019136643037199974; Accuracy: 1.0; Time taken (s): 303.1140329837799\n",
      "Development F1: 0.8075117370892018; Development Loss: 0.3337654744585355\n",
      "Iteration 0 of epoch 33 complete. \n",
      " Loss: 0.009034951217472553; Accuracy: 1.0; Time taken (s): 247.39248204231262\n",
      "Iteration 10 of epoch 33 complete. \n",
      " Loss: 0.01830347068607807; Accuracy: 0.984375; Time taken (s): 309.5242328643799\n",
      "Iteration 20 of epoch 33 complete. \n",
      " Loss: 0.007760205306112766; Accuracy: 1.0; Time taken (s): 303.16456937789917\n",
      "Development F1: 0.8105726872246696; Development Loss: 0.36395369635687935\n",
      "Iteration 0 of epoch 34 complete. \n",
      " Loss: 0.006267976947128773; Accuracy: 1.0; Time taken (s): 247.36873483657837\n",
      "Iteration 10 of epoch 34 complete. \n",
      " Loss: 0.006056802812963724; Accuracy: 1.0; Time taken (s): 303.10969710350037\n",
      "Iteration 20 of epoch 34 complete. \n",
      " Loss: 0.007883936166763306; Accuracy: 1.0; Time taken (s): 304.3833019733429\n",
      "Development F1: 0.8209606986899564; Development Loss: 0.3815513253211975\n",
      "Iteration 0 of epoch 35 complete. \n",
      " Loss: 0.006764204241335392; Accuracy: 1.0; Time taken (s): 246.40640783309937\n",
      "Iteration 10 of epoch 35 complete. \n",
      " Loss: 0.008328605443239212; Accuracy: 1.0; Time taken (s): 303.22836923599243\n",
      "Iteration 20 of epoch 35 complete. \n",
      " Loss: 0.009615111164748669; Accuracy: 1.0; Time taken (s): 303.521719455719\n",
      "Development F1: 0.812227074235808; Development Loss: 0.3704833785692851\n",
      "Iteration 0 of epoch 36 complete. \n",
      " Loss: 0.007990188896656036; Accuracy: 1.0; Time taken (s): 245.58361196517944\n",
      "Iteration 10 of epoch 36 complete. \n",
      " Loss: 0.007527108769863844; Accuracy: 1.0; Time taken (s): 304.3862864971161\n",
      "Iteration 20 of epoch 36 complete. \n",
      " Loss: 0.007162157911807299; Accuracy: 1.0; Time taken (s): 302.77595829963684\n",
      "Development F1: 0.8214285714285714; Development Loss: 0.3426625066333347\n",
      "Iteration 0 of epoch 37 complete. \n",
      " Loss: 0.007065384648740292; Accuracy: 1.0; Time taken (s): 247.03540992736816\n",
      "Iteration 10 of epoch 37 complete. \n",
      " Loss: 0.009799385443329811; Accuracy: 1.0; Time taken (s): 304.599214553833\n",
      "Iteration 20 of epoch 37 complete. \n",
      " Loss: 0.0058921510353684425; Accuracy: 1.0; Time taken (s): 309.83567070961\n",
      "Development F1: 0.8230088495575221; Development Loss: 0.392632813917266\n",
      "Iteration 0 of epoch 38 complete. \n",
      " Loss: 0.006193756125867367; Accuracy: 1.0; Time taken (s): 244.7565999031067\n",
      "Iteration 10 of epoch 38 complete. \n",
      " Loss: 0.0037228900473564863; Accuracy: 1.0; Time taken (s): 303.10095715522766\n",
      "Iteration 20 of epoch 38 complete. \n",
      " Loss: 0.004564584232866764; Accuracy: 1.0; Time taken (s): 303.2550895214081\n",
      "Development F1: 0.8230088495575221; Development Loss: 0.3626023216380013\n",
      "Iteration 0 of epoch 39 complete. \n",
      " Loss: 0.0047602104023098946; Accuracy: 1.0; Time taken (s): 246.28305864334106\n",
      "Iteration 10 of epoch 39 complete. \n",
      " Loss: 0.007053263019770384; Accuracy: 1.0; Time taken (s): 304.2280476093292\n",
      "Iteration 20 of epoch 39 complete. \n",
      " Loss: 0.004444420803338289; Accuracy: 1.0; Time taken (s): 303.7257008552551\n",
      "Development F1: 0.8266666666666667; Development Loss: 0.3643243693643146\n"
     ]
    }
   ],
   "source": [
    "for ep in range(20,40):\n",
    "    eps.append(ep)\n",
    "    net.train()\n",
    "    for it, (seq, mask, seg, labels,idx) in enumerate(train_loader):\n",
    "        \n",
    "        #Clear gradients\n",
    "        opti.zero_grad()\n",
    "        #Converting these to cuda tensors\n",
    "        # seq, mask, seg, labels = seq.to(device), mask.to(device), seg.to(device), labels.to(device)\n",
    "        \n",
    "        stats = np.array(train_stat)\n",
    "        stats = torch.tensor(stats[idx]).float()\n",
    "        #Obtaining the logits from the model\n",
    "        logits = net(seq, mask, seg, stats)\n",
    "        #Computing loss\n",
    "        loss = criterion(logits.squeeze(), labels.float())\n",
    "\n",
    "        #Backpropagating the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #Optimization step\n",
    "        opti.step()\n",
    "\n",
    "        if it % 10 == 0:\n",
    "\n",
    "            acc = get_accuracy_from_logits(logits, labels)\n",
    "            print(\"Iteration {} of epoch {} complete. \\n Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "        \n",
    "    dev_acc, dev_loss = evaluate(net, criterion, dev_loader, 'cpu')\n",
    "    t_loss.append(loss.item())\n",
    "    d_loss.append(dev_loss)\n",
    "    print(\"Development F1: {}; Development Loss: {}\".format(dev_acc, dev_loss))\n",
    "    torch.save(net.state_dict(), 'D:\\\\bertcls_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f08207-d86c-4d92-8486-2b8609053dbb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Optimize Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c12ece1-995d-4e76-9daa-5054a25b6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RumorEmbedder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RumorEmbedder, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        self.ffnn = nn.Sequential(nn.Linear(811,512),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3),\n",
    "                                 nn.Linear(512,256)\n",
    "                                 )\n",
    "\n",
    "    def forward(self, seq, attn_masks, seg, stats):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks,token_type_ids=seg, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "        \n",
    "        x = torch.cat((cls_rep,stats),dim=1)\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        embs = self.ffnn(x)\n",
    "\n",
    "        return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f909d01f-df02-4d24-9153-c73f485d02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a,b):\n",
    "    norm_a = torch.norm(a, dim=1)\n",
    "    norm_b = torch.norm(b, dim=0)\n",
    "    return torch.mm(a,b)/torch.mm(norm_a.unsqueeze(1), norm_b.unsqueeze(0))\n",
    "\n",
    "class IntraInterLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(IntraInterLoss, self).__init__()\n",
    "\n",
    "    def forward(self, emb, target):\n",
    "        nr_emb = emb[target==0]\n",
    "        r_emb = emb[target==1]\n",
    "        count_1_1 = r_emb.shape[0]**2\n",
    "        count_0_0 = nr_emb.shape[0]**2\n",
    "        count_0_1 = r_emb.shape[0]*nr_emb.shape[0]\n",
    "        r_cos = torch.sum(cos_sim(r_emb, r_emb.T))/float(count_1_1)\n",
    "        nr_cos = torch.sum(cos_sim(nr_emb, nr_emb.T))/float(count_0_0)\n",
    "        r_nr_cos = torch.sum(cos_sim(r_emb, nr_emb.T))/float(count_0_1)\n",
    "        \n",
    "        return r_nr_cos-0.4*r_cos-0.2*nr_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e08dca2-b5e1-4332-8eb8-b6065997e8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "dev_loader = Data.DataLoader(dev_set, batch_size=len(dev_set), shuffle=False)\n",
    "# torch.cuda.empty_cache ()\n",
    "net = RumorEmbedder()\n",
    "# net = net.to(device)\n",
    "\n",
    "criterion = IntraInterLoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 0.001)\n",
    "scheduler = lr_scheduler.ExponentialLR(opti, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ef172db-267c-474b-bc3b-f066e4989065",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. \n",
      " Loss: 0.21107256412506104; Time taken (s): 36.5722496509552\n",
      "Iteration 10 of epoch 0 complete. \n",
      " Loss: 0.00258546881377697; Time taken (s): 370.3924024105072\n",
      "Iteration 20 of epoch 0 complete. \n",
      " Loss: -0.2281695157289505; Time taken (s): 371.6792678833008\n",
      "Development Loss: -0.33488768339157104\n",
      "Iteration 0 of epoch 1 complete. \n",
      " Loss: -0.2677144408226013; Time taken (s): 286.99082374572754\n",
      "Iteration 10 of epoch 1 complete. \n",
      " Loss: -0.5775624513626099; Time taken (s): 367.949254989624\n",
      "Iteration 20 of epoch 1 complete. \n",
      " Loss: -0.1781993955373764; Time taken (s): 368.10459208488464\n",
      "Development Loss: -0.36340734362602234\n",
      "Iteration 0 of epoch 2 complete. \n",
      " Loss: -0.5245465040206909; Time taken (s): 280.88317370414734\n",
      "Iteration 10 of epoch 2 complete. \n",
      " Loss: -0.2928198575973511; Time taken (s): 366.15686988830566\n",
      "Iteration 20 of epoch 2 complete. \n",
      " Loss: -0.3540284335613251; Time taken (s): 367.9296464920044\n",
      "Development Loss: -0.40007686614990234\n",
      "Iteration 0 of epoch 3 complete. \n",
      " Loss: -0.36974939703941345; Time taken (s): 285.5919859409332\n",
      "Iteration 10 of epoch 3 complete. \n",
      " Loss: -0.41492435336112976; Time taken (s): 366.25868701934814\n",
      "Iteration 20 of epoch 3 complete. \n",
      " Loss: -0.3720623254776001; Time taken (s): 368.4919285774231\n",
      "Development Loss: -0.387966513633728\n",
      "Iteration 0 of epoch 4 complete. \n",
      " Loss: -0.3331230580806732; Time taken (s): 282.93833208084106\n",
      "Iteration 10 of epoch 4 complete. \n",
      " Loss: -0.2042122781276703; Time taken (s): 370.68238615989685\n",
      "Iteration 20 of epoch 4 complete. \n",
      " Loss: -0.34366604685783386; Time taken (s): 368.8322010040283\n",
      "Development Loss: -0.4087446331977844\n",
      "Iteration 0 of epoch 5 complete. \n",
      " Loss: -0.4694983959197998; Time taken (s): 283.4511892795563\n",
      "Iteration 10 of epoch 5 complete. \n",
      " Loss: -0.4526030123233795; Time taken (s): 370.1289360523224\n",
      "Iteration 20 of epoch 5 complete. \n",
      " Loss: -0.23732803761959076; Time taken (s): 370.60378646850586\n",
      "Development Loss: -0.3925846815109253\n",
      "Iteration 0 of epoch 6 complete. \n",
      " Loss: -0.5496852993965149; Time taken (s): 284.41456484794617\n",
      "Iteration 10 of epoch 6 complete. \n",
      " Loss: -0.30460238456726074; Time taken (s): 368.26869463920593\n",
      "Iteration 20 of epoch 6 complete. \n",
      " Loss: -0.35351303219795227; Time taken (s): 371.91869020462036\n",
      "Development Loss: -0.37507086992263794\n",
      "Iteration 0 of epoch 7 complete. \n",
      " Loss: -0.4837105870246887; Time taken (s): 282.3194000720978\n",
      "Iteration 10 of epoch 7 complete. \n",
      " Loss: -0.6502242684364319; Time taken (s): 368.382684469223\n",
      "Iteration 20 of epoch 7 complete. \n",
      " Loss: -0.37155112624168396; Time taken (s): 368.3867349624634\n",
      "Development Loss: -0.41498079895973206\n",
      "Iteration 0 of epoch 8 complete. \n",
      " Loss: -0.4852931797504425; Time taken (s): 281.8972008228302\n",
      "Iteration 10 of epoch 8 complete. \n",
      " Loss: -0.3475462794303894; Time taken (s): 367.5053939819336\n",
      "Iteration 20 of epoch 8 complete. \n",
      " Loss: -0.14247265458106995; Time taken (s): 367.7105667591095\n",
      "Development Loss: -0.4098520278930664\n",
      "Iteration 0 of epoch 9 complete. \n",
      " Loss: -0.6811965703964233; Time taken (s): 282.7804322242737\n",
      "Iteration 10 of epoch 9 complete. \n",
      " Loss: -0.07070793211460114; Time taken (s): 368.43922424316406\n",
      "Iteration 20 of epoch 9 complete. \n",
      " Loss: -0.7152726054191589; Time taken (s): 365.1060755252838\n",
      "Development Loss: -0.42570215463638306\n",
      "Iteration 0 of epoch 10 complete. \n",
      " Loss: -0.8733524084091187; Time taken (s): 284.17907643318176\n",
      "Iteration 10 of epoch 10 complete. \n",
      " Loss: -0.45877718925476074; Time taken (s): 368.29394268989563\n",
      "Iteration 20 of epoch 10 complete. \n",
      " Loss: -0.4650455713272095; Time taken (s): 366.4545135498047\n",
      "Development Loss: -0.41259104013442993\n",
      "Iteration 0 of epoch 11 complete. \n",
      " Loss: -0.6900891065597534; Time taken (s): 282.01485776901245\n",
      "Iteration 10 of epoch 11 complete. \n",
      " Loss: -0.44307756423950195; Time taken (s): 366.02014899253845\n",
      "Iteration 20 of epoch 11 complete. \n",
      " Loss: 0.14027751982212067; Time taken (s): 364.7574291229248\n",
      "Development Loss: -0.23355624079704285\n",
      "Iteration 0 of epoch 12 complete. \n",
      " Loss: -0.13499480485916138; Time taken (s): 285.5478401184082\n",
      "Iteration 10 of epoch 12 complete. \n",
      " Loss: -0.11243467777967453; Time taken (s): 366.11117792129517\n",
      "Iteration 20 of epoch 12 complete. \n",
      " Loss: -0.0669267475605011; Time taken (s): 370.03937125205994\n",
      "Development Loss: -0.3912962079048157\n",
      "Iteration 0 of epoch 13 complete. \n",
      " Loss: -0.4968322515487671; Time taken (s): 280.8361530303955\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m stats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(stats[idx])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#Obtaining the logits from the model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#Computing loss\u001b[39;00m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36mRumorEmbedder.forward\u001b[1;34m(self, seq, attn_masks, seg, stats)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03mInputs:\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    -seq : Tensor of shape [B, T] containing token ids of sequences\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#Feeding the input to BERT model to obtain contextualized representations\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m cont_reps \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#Obtaining the representation of [CLS] head (the first token)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    987\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    989\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    990\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    991\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    995\u001b[0m )\n\u001b[1;32m--> 996\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1008\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1009\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    576\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    577\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    578\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    583\u001b[0m     )\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 585\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:472\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    462\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    471\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:411\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    394\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    401\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    402\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    403\u001b[0m         hidden_states,\n\u001b[0;32m    404\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    409\u001b[0m         output_attentions,\n\u001b[0;32m    410\u001b[0m     )\n\u001b[1;32m--> 411\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:362\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    361\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m--> 362\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1169\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[1;32m-> 1169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "st = time.time()\n",
    "for ep in range(20):\n",
    "    net.train()\n",
    "    for it, (seq, mask, seg, labels,idx) in enumerate(train_loader):\n",
    "        \n",
    "        #Clear gradients\n",
    "        opti.zero_grad()\n",
    "        #Converting these to cuda tensors\n",
    "        # seq, mask, seg, labels = seq.to(device), mask.to(device), seg.to(device), labels.to(device)\n",
    "\n",
    "        stats = np.array(train_stat)\n",
    "        stats = torch.tensor(stats[idx]).float()\n",
    "        #Obtaining the logits from the model\n",
    "        logits = net(seq, mask, seg, stats)\n",
    "        #Computing loss\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        #Backpropagating the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #Optimization step\n",
    "        opti.step()\n",
    "\n",
    "        if it % 10 == 0:\n",
    "\n",
    "            print(\"Iteration {} of epoch {} complete. \\n Loss: {}; Time taken (s): {}\".format(it, ep, loss.item(), (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for seq, mask, seg, labels, idx in dev_loader:\n",
    "            dev_embs = net(seq, mask, seg, torch.FloatTensor(np.array(dev_stat)))\n",
    "        dev_loss = criterion(dev_embs, labels)\n",
    "    \n",
    "    print(\"Development Loss: {}\".format(dev_loss.item()))\n",
    "    torch.save(net.state_dict(), 'D:\\\\bertemb_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b984573-0d7f-4fee-b258-64d4dcb55dca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### BertEmbedding Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab7e35b7-3596-4851-8141-19a83c5b1f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = RumorEmbedder()\n",
    "net.load_state_dict(torch.load('D:\\\\bertemb_9.dat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5a43a41b-aa06-4f15-afd1-8b059a936afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = Data.DataLoader(dev_set, batch_size=len(dev_set), shuffle=False,sampler=range(0,len(dev_set)))\n",
    "train_loader = Data.DataLoader(train_set, batch_size=len(train_set), shuffle=False, sampler=range(0,len(train_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8a9f3bda-44cf-4e44-82be-34436683af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for seq, mask, seg, labels,idx in train_loader:\n",
    "        train_embs = net(seq, mask, seg, torch.FloatTensor(np.array(train_stat)))\n",
    "    for seq, mask, seg, labels, idx in dev_loader:\n",
    "        dev_embs = net(seq, mask, seg, torch.FloatTensor(np.array(dev_stat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "44a033b3-bdfc-4a1d-876c-ce16ce89d82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.35964912280701755  Recall:0.7130434782608696  F1:0.47813411078717205\n",
      "Accuracy: 0.6660447761194029\n"
     ]
    }
   ],
   "source": [
    "r_mean = train_embs[np.array(y_train) == 1].mean(dim=0).numpy()\n",
    "nr_mean = train_embs[np.array(y_train) == 0].mean(dim=0).numpy()\n",
    "dev_embs_n = dev_embs.numpy()\n",
    "dev_preds = []\n",
    "\n",
    "for emb in dev_embs_n:\n",
    "    r_sim = np.dot(emb, r_mean)/(np.linalg.norm(emb)*np.linalg.norm(r_mean))\n",
    "    nr_sim = np.dot(emb, nr_mean)/(np.linalg.norm(emb)*np.linalg.norm(nr_mean))\n",
    "    if r_sim >= nr_sim:\n",
    "        dev_preds.append(1)\n",
    "    else:\n",
    "        dev_preds.append(0)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_dev, dev_preds, pos_label=1, average=\"binary\")\n",
    "print('Precision:{}  Recall:{}  F1:{}'.format(p,r,f))\n",
    "print('Accuracy: {}'.format(accuracy_score(y_dev, dev_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d3bd204c-391d-49bb-a8c0-7834e6d67907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.3890675241157556  Recall:0.7610062893081762  F1:0.5148936170212767\n",
      "Accuracy: 0.7112096263457884\n"
     ]
    }
   ],
   "source": [
    "r_mean = train_embs[np.array(y_train) == 1].mean(dim=0).numpy()\n",
    "nr_mean = train_embs[np.array(y_train) == 0].mean(dim=0).numpy()\n",
    "dev_embs_n = dev_embs.numpy()\n",
    "dev_preds = []\n",
    "\n",
    "for emb in train_embs.numpy():\n",
    "    r_sim = np.dot(emb, r_mean)/(np.linalg.norm(emb)*np.linalg.norm(r_mean))\n",
    "    nr_sim = np.dot(emb, nr_mean)/(np.linalg.norm(emb)*np.linalg.norm(nr_mean))\n",
    "    if r_sim >= nr_sim:\n",
    "        dev_preds.append(1)\n",
    "    else:\n",
    "        dev_preds.append(0)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_train, dev_preds, pos_label=1, average=\"binary\")\n",
    "print('Precision:{}  Recall:{}  F1:{}'.format(p,r,f))\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, dev_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f023fad1-ef6b-49fd-9fe4-0a2a3403e624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.5463917525773195  Recall:0.4608695652173913  F1:0.5\n",
      "Accuracy: 0.8022388059701493\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(train_embs.numpy(), y_train)\n",
    "\n",
    "predictions = knn.predict(dev_embs_n)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_dev, predictions, pos_label=1, average=\"binary\")\n",
    "\n",
    "print('Precision:{}  Recall:{}  F1:{}'.format(p,r,f))\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(y_dev, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "68f94aeb-ef17-4fd6-9c5c-1e0f91004959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.6732673267326733  Recall:0.6181818181818182  F1:0.6445497630331753\n",
      "Accuracy: 0.8563218390804598\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('X_train.pkl','rb') as file:\n",
    "    X_train = pickle.load(file)\n",
    "with open('X_dev.pkl','rb') as file:\n",
    "    X_dev = pickle.load(file)\n",
    "with open('y_dev.pkl','rb') as file:\n",
    "    y_dev = pickle.load(file)\n",
    "with open('y_train.pkl','rb') as file:\n",
    "    y_train = pickle.load(file)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "predictions = knn.predict(X_dev)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_dev, predictions, pos_label=1, average=\"binary\")\n",
    "\n",
    "print('Precision:{}  Recall:{}  F1:{}'.format(p,r,f))\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(y_dev, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a0f4decd-3d35-49e4-a15f-7530f5858702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.6666666666666666  Recall:0.06956521739130435  F1:0.12598425196850394\n",
      "Accuracy: 0.792910447761194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trist\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter=100)\n",
    "lr.fit(train_embs.numpy(), y_train)\n",
    "\n",
    "predictions = lr.predict(dev_embs_n)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(y_dev, predictions, pos_label=1, average=\"binary\")\n",
    "\n",
    "print('Precision:{}  Recall:{}  F1:{}'.format(p,r,f))\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(y_dev, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82228213-cdd8-4f6e-8b83-6a7e611e175b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.559523  , -0.6694411 ,  0.6386693 , -0.3082815 , -1.4688318 ,\n",
       "        1.0080024 ,  0.76538223,  1.5122892 , -0.85284615,  1.7891976 ,\n",
       "       -1.4008505 , -0.55482763,  1.8608657 , -0.6717017 ,  0.97310126,\n",
       "        0.5818697 , -0.5390343 , -0.7755241 ,  1.9045625 , -0.26476654,\n",
       "       -1.3056165 , -0.9074661 ,  0.7634458 , -1.5461605 ,  0.08976556,\n",
       "       -0.8494065 , -0.8777184 , -1.0230728 ,  0.3525765 , -1.5808235 ,\n",
       "        0.01667575, -0.1495756 , -0.3053692 ,  0.07764101, -1.1683062 ,\n",
       "        0.7895137 , -0.09378773, -0.17215891, -0.15668851,  0.42977983,\n",
       "        0.4158164 , -1.5737258 ,  1.8172727 ,  1.4243525 ,  1.8285083 ,\n",
       "        0.71199876, -0.9187279 ,  1.1941608 ,  0.565277  ,  0.28214428,\n",
       "        1.4069462 ,  1.2998426 ,  1.0016237 , -0.82209605, -0.50618243,\n",
       "        0.05480868,  1.6530193 , -0.52838504,  0.8456729 , -1.1414135 ,\n",
       "       -0.13572106,  0.576234  ,  0.7270736 , -1.4084289 ], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1d36e595-b493-4a6f-9b5d-4f583ee7d35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5737024 , -0.6759854 ,  0.6507191 , -0.32405192, -1.4975203 ,\n",
       "        1.0036894 ,  0.78646857,  1.5012347 , -0.87575895,  1.8210517 ,\n",
       "       -1.4066744 , -0.5509191 ,  1.885839  , -0.68972784,  0.9726847 ,\n",
       "        0.5760501 , -0.54722756, -0.80475795,  1.9094418 , -0.26871154,\n",
       "       -1.3065656 , -0.9108496 ,  0.78440595, -1.5575368 ,  0.07096983,\n",
       "       -0.8725201 , -0.853683  , -1.0237498 ,  0.34915003, -1.5788484 ,\n",
       "        0.02422947, -0.13966322, -0.32549387,  0.08283487, -1.1725831 ,\n",
       "        0.7983685 , -0.07583883, -0.16052043, -0.15137553,  0.4250438 ,\n",
       "        0.41498375, -1.5853176 ,  1.8458185 ,  1.4217794 ,  1.8454164 ,\n",
       "        0.7082451 , -0.94747126,  1.2044528 ,  0.55465084,  0.29615617,\n",
       "        1.405537  ,  1.2859378 ,  1.0131048 , -0.8556225 , -0.49518442,\n",
       "        0.05648269,  1.654872  , -0.5226128 ,  0.84434843, -1.1529379 ,\n",
       "       -0.14024371,  0.5695921 ,  0.74909246, -1.4219875 ], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a29684-320a-4f85-aa16-51ce4e227314",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Text Only Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d35c4291-4528-4ff0-853b-36d828760796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRumorClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TextRumorClassifier, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        self.ffnn = nn.Sequential(nn.Linear(768,128),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3),\n",
    "                                 nn.Linear(128,64),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3),\n",
    "                                  nn.Linear(64,1),\n",
    "                                  nn.Sigmoid()\n",
    "                                 )\n",
    "\n",
    "    def forward(self, seq, attn_masks, seg):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "        \n",
    "        # x = torch.cat((cls_rep,stats),dim=1)\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.ffnn(cls_rep)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fe7e7a2-650c-4e24-aeb3-ee256bdcae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "net = TextRumorClassifier()\n",
    "# net = net.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f46bc5bb-8c33-4806-85ca-2b0e1380b511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_evaluate(net, criterion, dataloader, device):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, mask, seg, labels, idx in dataloader:\n",
    "            # seq, labels = seq.to(device), labels.to(device)\n",
    "            # stats = np.array(train_stat.iloc[:,1:])\n",
    "            # stats = torch.tensor(stats[idx]).float()\n",
    "            #Obtaining the logits from the model\n",
    "            logits = net(seq, mask, seg)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09e88416-c6ea-457f-94c7-be1087fc15f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "best_acc = 0\n",
    "st = time.time()\n",
    "eps = []\n",
    "t_loss = []\n",
    "d_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a32a1b2f-72a6-4f05-a43a-9d9b94694e3f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. \n",
      " Loss: 0.6802356839179993; Accuracy: 0.671875; Time taken (s): 81.13227605819702\n",
      "Iteration 10 of epoch 0 complete. \n",
      " Loss: 0.5898295044898987; Accuracy: 0.78125; Time taken (s): 299.43943309783936\n",
      "Iteration 20 of epoch 0 complete. \n",
      " Loss: 0.5761324763298035; Accuracy: 0.75; Time taken (s): 298.0222783088684\n",
      "Development Accuracy: 0.7715277671813965; Development Loss: 0.5319444206025865\n",
      "Iteration 0 of epoch 1 complete. \n",
      " Loss: 0.5774604678153992; Accuracy: 0.734375; Time taken (s): 220.4582164287567\n",
      "Iteration 10 of epoch 1 complete. \n",
      " Loss: 0.4819682836532593; Accuracy: 0.8125; Time taken (s): 296.9613416194916\n",
      "Iteration 20 of epoch 1 complete. \n",
      " Loss: 0.45131972432136536; Accuracy: 0.796875; Time taken (s): 297.24166321754456\n",
      "Development Accuracy: 0.7996528148651123; Development Loss: 0.43302328056759304\n",
      "Iteration 0 of epoch 2 complete. \n",
      " Loss: 0.4024360477924347; Accuracy: 0.8125; Time taken (s): 225.989727973938\n",
      "Iteration 10 of epoch 2 complete. \n",
      " Loss: 0.4486580789089203; Accuracy: 0.765625; Time taken (s): 297.42676305770874\n",
      "Iteration 20 of epoch 2 complete. \n",
      " Loss: 0.4015752673149109; Accuracy: 0.765625; Time taken (s): 302.4298474788666\n",
      "Development Accuracy: 0.7715277671813965; Development Loss: 0.40253406431939864\n",
      "Iteration 0 of epoch 3 complete. \n",
      " Loss: 0.3700542151927948; Accuracy: 0.859375; Time taken (s): 221.20544385910034\n",
      "Iteration 10 of epoch 3 complete. \n",
      " Loss: 0.3114929795265198; Accuracy: 0.875; Time taken (s): 296.1478228569031\n",
      "Iteration 20 of epoch 3 complete. \n",
      " Loss: 0.4102683663368225; Accuracy: 0.703125; Time taken (s): 297.2571277618408\n",
      "Development Accuracy: 0.7809027433395386; Development Loss: 0.3777681522899204\n",
      "Iteration 0 of epoch 4 complete. \n",
      " Loss: 0.3427141010761261; Accuracy: 0.890625; Time taken (s): 218.97034573554993\n",
      "Iteration 10 of epoch 4 complete. \n",
      " Loss: 0.3261837065219879; Accuracy: 0.859375; Time taken (s): 297.1521053314209\n",
      "Iteration 20 of epoch 4 complete. \n",
      " Loss: 0.1853872537612915; Accuracy: 1.0; Time taken (s): 297.5361819267273\n",
      "Development Accuracy: 0.8743055462837219; Development Loss: 0.3650273084640503\n",
      "Iteration 0 of epoch 5 complete. \n",
      " Loss: 0.23465262353420258; Accuracy: 0.953125; Time taken (s): 220.8443422317505\n",
      "Iteration 10 of epoch 5 complete. \n",
      " Loss: 0.2828870117664337; Accuracy: 0.90625; Time taken (s): 296.63211154937744\n",
      "Iteration 20 of epoch 5 complete. \n",
      " Loss: 0.23903009295463562; Accuracy: 0.9375; Time taken (s): 296.602587223053\n",
      "Development Accuracy: 0.8656250238418579; Development Loss: 0.31499414808220333\n",
      "Iteration 0 of epoch 6 complete. \n",
      " Loss: 0.2568912208080292; Accuracy: 0.90625; Time taken (s): 220.6560664176941\n",
      "Iteration 10 of epoch 6 complete. \n",
      " Loss: 0.247569739818573; Accuracy: 0.953125; Time taken (s): 297.4129750728607\n",
      "Iteration 20 of epoch 6 complete. \n",
      " Loss: 0.21808868646621704; Accuracy: 0.953125; Time taken (s): 303.97619438171387\n",
      "Development Accuracy: 0.8732638955116272; Development Loss: 0.384066647125615\n",
      "Iteration 0 of epoch 7 complete. \n",
      " Loss: 0.19976870715618134; Accuracy: 0.9375; Time taken (s): 220.87582063674927\n",
      "Iteration 10 of epoch 7 complete. \n",
      " Loss: 0.27573078870773315; Accuracy: 0.921875; Time taken (s): 296.1779816150665\n",
      "Iteration 20 of epoch 7 complete. \n",
      " Loss: 0.18678443133831024; Accuracy: 0.953125; Time taken (s): 297.39706158638\n",
      "Development Accuracy: 0.9340277910232544; Development Loss: 0.22147591825988558\n",
      "Iteration 0 of epoch 8 complete. \n",
      " Loss: 0.10352814942598343; Accuracy: 0.984375; Time taken (s): 220.7351589202881\n",
      "Iteration 10 of epoch 8 complete. \n",
      " Loss: 0.22030408680438995; Accuracy: 0.953125; Time taken (s): 296.3964297771454\n",
      "Iteration 20 of epoch 8 complete. \n",
      " Loss: 0.19165925681591034; Accuracy: 0.953125; Time taken (s): 296.3505527973175\n",
      "Development Accuracy: 0.9211804866790771; Development Loss: 0.24064936406082577\n",
      "Iteration 0 of epoch 9 complete. \n",
      " Loss: 0.150380939245224; Accuracy: 0.96875; Time taken (s): 218.93811440467834\n",
      "Iteration 10 of epoch 9 complete. \n",
      " Loss: 0.18152907490730286; Accuracy: 0.96875; Time taken (s): 331.6500794887543\n",
      "Iteration 20 of epoch 9 complete. \n",
      " Loss: 0.1733761727809906; Accuracy: 0.96875; Time taken (s): 296.8183481693268\n",
      "Development Accuracy: 0.9083333611488342; Development Loss: 0.22732179363568625\n",
      "Iteration 0 of epoch 10 complete. \n",
      " Loss: 0.09950870275497437; Accuracy: 0.984375; Time taken (s): 221.98526763916016\n",
      "Iteration 10 of epoch 10 complete. \n",
      " Loss: 0.09946000576019287; Accuracy: 1.0; Time taken (s): 296.6942472457886\n",
      "Iteration 20 of epoch 10 complete. \n",
      " Loss: 0.08078835904598236; Accuracy: 0.984375; Time taken (s): 297.30266857147217\n",
      "Development Accuracy: 0.9281249642372131; Development Loss: 0.22359735684262383\n",
      "Iteration 0 of epoch 11 complete. \n",
      " Loss: 0.16118431091308594; Accuracy: 0.96875; Time taken (s): 221.00080060958862\n",
      "Iteration 10 of epoch 11 complete. \n",
      " Loss: 0.18316838145256042; Accuracy: 0.96875; Time taken (s): 303.366548538208\n",
      "Iteration 20 of epoch 11 complete. \n",
      " Loss: 0.11347579956054688; Accuracy: 0.96875; Time taken (s): 297.30295848846436\n",
      "Development Accuracy: 0.9322916865348816; Development Loss: 0.20730443360904852\n",
      "Iteration 0 of epoch 12 complete. \n",
      " Loss: 0.1285964399576187; Accuracy: 0.984375; Time taken (s): 218.70448565483093\n",
      "Iteration 10 of epoch 12 complete. \n",
      " Loss: 0.11941289901733398; Accuracy: 0.984375; Time taken (s): 296.63081550598145\n",
      "Iteration 20 of epoch 12 complete. \n",
      " Loss: 0.058265190571546555; Accuracy: 1.0; Time taken (s): 297.2467796802521\n",
      "Development Accuracy: 0.9281249642372131; Development Loss: 0.25157110227478874\n",
      "Iteration 0 of epoch 13 complete. \n",
      " Loss: 0.060456059873104095; Accuracy: 0.984375; Time taken (s): 220.57226514816284\n",
      "Iteration 10 of epoch 13 complete. \n",
      " Loss: 0.1359124779701233; Accuracy: 0.96875; Time taken (s): 296.16311144828796\n",
      "Iteration 20 of epoch 13 complete. \n",
      " Loss: 0.24394303560256958; Accuracy: 0.9375; Time taken (s): 296.1930947303772\n",
      "Development Accuracy: 0.9315971732139587; Development Loss: 0.2178215417597029\n",
      "Iteration 0 of epoch 14 complete. \n",
      " Loss: 0.043951794505119324; Accuracy: 1.0; Time taken (s): 219.92360663414001\n",
      "Iteration 10 of epoch 14 complete. \n",
      " Loss: 0.03783406689763069; Accuracy: 1.0; Time taken (s): 295.7859377861023\n",
      "Iteration 20 of epoch 14 complete. \n",
      " Loss: 0.10898635536432266; Accuracy: 0.984375; Time taken (s): 295.7577893733978\n",
      "Development Accuracy: 0.9409722089767456; Development Loss: 0.2148100563014547\n",
      "Iteration 0 of epoch 15 complete. \n",
      " Loss: 0.03337579220533371; Accuracy: 1.0; Time taken (s): 220.61015558242798\n",
      "Iteration 10 of epoch 15 complete. \n",
      " Loss: 0.031077606603503227; Accuracy: 1.0; Time taken (s): 301.6768755912781\n",
      "Iteration 20 of epoch 15 complete. \n",
      " Loss: 0.030278261750936508; Accuracy: 1.0; Time taken (s): 294.6953103542328\n",
      "Development Accuracy: 0.9281249642372131; Development Loss: 0.29197897513707477\n",
      "Iteration 0 of epoch 16 complete. \n",
      " Loss: 0.18090198934078217; Accuracy: 0.96875; Time taken (s): 220.5163128376007\n",
      "Iteration 10 of epoch 16 complete. \n",
      " Loss: 0.037820782512426376; Accuracy: 1.0; Time taken (s): 294.73997259140015\n",
      "Iteration 20 of epoch 16 complete. \n",
      " Loss: 0.031452666968107224; Accuracy: 1.0; Time taken (s): 295.52116107940674\n",
      "Development Accuracy: 0.9246527552604675; Development Loss: 0.2850782523552577\n",
      "Iteration 0 of epoch 17 complete. \n",
      " Loss: 0.08480245620012283; Accuracy: 0.984375; Time taken (s): 220.00190448760986\n",
      "Iteration 10 of epoch 17 complete. \n",
      " Loss: 0.038943544030189514; Accuracy: 1.0; Time taken (s): 294.96084427833557\n",
      "Iteration 20 of epoch 17 complete. \n",
      " Loss: 0.08793707191944122; Accuracy: 0.984375; Time taken (s): 296.37982153892517\n",
      "Development Accuracy: 0.9409722089767456; Development Loss: 0.23112470884290007\n",
      "Iteration 0 of epoch 18 complete. \n",
      " Loss: 0.09463363140821457; Accuracy: 0.984375; Time taken (s): 218.76718187332153\n",
      "Iteration 10 of epoch 18 complete. \n",
      " Loss: 0.07292123138904572; Accuracy: 0.984375; Time taken (s): 297.42747044563293\n",
      "Iteration 20 of epoch 18 complete. \n",
      " Loss: 0.09867572039365768; Accuracy: 0.984375; Time taken (s): 297.27133989334106\n",
      "Development Accuracy: 0.9392361044883728; Development Loss: 0.21728208433422777\n",
      "Iteration 0 of epoch 19 complete. \n",
      " Loss: 0.085542231798172; Accuracy: 0.984375; Time taken (s): 221.0791130065918\n",
      "Iteration 10 of epoch 19 complete. \n",
      " Loss: 0.110785573720932; Accuracy: 0.984375; Time taken (s): 296.2733783721924\n",
      "Iteration 20 of epoch 19 complete. \n",
      " Loss: 0.031884439289569855; Accuracy: 1.0; Time taken (s): 303.8342869281769\n",
      "Development Accuracy: 0.9315971732139587; Development Loss: 0.25821830415063435\n"
     ]
    }
   ],
   "source": [
    "for ep in range(20):\n",
    "    eps.append(ep)\n",
    "    net.train()\n",
    "    for it, (seq, mask, seg, labels,idx) in enumerate(train_loader):\n",
    "\n",
    "        #Clear gradients\n",
    "        opti.zero_grad()\n",
    "        #Converting these to cuda tensors\n",
    "        # seq, mask, seg, labels = seq.to(device), mask.to(device), seg.to(device), labels.to(device)\n",
    "\n",
    "        # stats = np.array(train_stat.iloc[:,1:])\n",
    "        # stats = torch.tensor(stats[idx]).float()\n",
    "        #Obtaining the logits from the model\n",
    "        logits = net(seq, mask, seg)\n",
    "\n",
    "        #Computing loss\n",
    "        loss = criterion(logits.squeeze(), labels.float())\n",
    "\n",
    "        #Backpropagating the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #Optimization step\n",
    "        opti.step()\n",
    "\n",
    "        if it % 10 == 0:\n",
    "\n",
    "            acc = get_accuracy_from_logits(logits, labels)\n",
    "            print(\"Iteration {} of epoch {} complete. \\n Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "\n",
    "    dev_acc, dev_loss = text_evaluate(net, criterion, dev_loader, 'cpu')\n",
    "    t_loss.append(loss.item())\n",
    "    d_loss.append(dev_loss)\n",
    "    print(\"Development Accuracy: {}; Development Loss: {}\".format(dev_acc, dev_loss))\n",
    "    torch.save(net.state_dict(), 'D:\\\\bertcls_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadfc51b-742c-4c70-8752-4fb30faf7ba8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Text Only Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47479f40-9744-4907-87fa-6a1d151aa79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = TextRumorClassifier()\n",
    "text_model.load_state_dict(torch.load('D:\\\\bertcls_14.dat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a20adf9e-5a46-40e8-b365-bc0e52ad1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet = pd.read_csv('./sep_data/test_tweet_df.csv')\n",
    "test_stat = pd.read_csv('./sep_data/test_stat_feat_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20176d53-9f25-43b5-bf3d-1bb0279eb141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/558 [00:00<?, ?it/s]C:\\Users\\trist\\AppData\\Local\\Temp\\ipykernel_6104\\3174842241.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_tweet.text.iloc[i] = '[CLS] ' + str(test_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(test_tweet.reply_text.iloc[i]).strip()\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 558/558 [00:08<00:00, 68.94it/s]\n"
     ]
    }
   ],
   "source": [
    "test_tweet.text.fillna('', inplace=True)\n",
    "test_tweet.reply_text.fillna('', inplace=True)\n",
    "for i in tqdm(range(len(test_tweet))):\n",
    "    test_tweet.text.iloc[i] = '[CLS] ' + str(test_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(test_tweet.reply_text.iloc[i]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4afdab8d-8f34-4e24-999d-517f8407b5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 558/558 [00:02<00:00, 270.06it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 256\n",
    "test_seq = []\n",
    "test_mask = []\n",
    "test_seg = []\n",
    "\n",
    "for i in tqdm(range(len(test_tweet))):\n",
    "    try:\n",
    "        txt = test_tweet.text.iloc[i]\n",
    "        tokens = tokenizer.tokenize(txt)\n",
    "        if len(tokens) < max_len:\n",
    "             padded_tokens = tokens + ['[PAD]' for _ in range(max_len - len(tokens))]\n",
    "        else:\n",
    "            padded_tokens = tokens[:max_len-1] + ['[SEP]']\n",
    "        attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "        seg_ids = []\n",
    "        seg_idx = 0\n",
    "        for token in padded_tokens:\n",
    "            seg_ids.append(seg_idx)\n",
    "            if token == '[SEP]':\n",
    "                seg_idx += 1\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "\n",
    "        test_seq.append(token_ids)\n",
    "        test_mask.append(attn_mask)\n",
    "        test_seg.append(seg_ids)\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5521f55-9298-4586-ba4f-cb65883f31b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 558/558 [02:19<00:00,  4.01it/s]\n"
     ]
    }
   ],
   "source": [
    "text_model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_seq))):\n",
    "        seq = torch.tensor(test_seq[i]).unsqueeze(0)\n",
    "        mask = torch.tensor(test_mask[i]).unsqueeze(0)\n",
    "        seg = torch.tensor(test_seg[i]).unsqueeze(0)\n",
    "        \n",
    "        preds.append(text_model(seq,mask,seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d823e06d-a924-40a0-b4c4-610f79be8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(preds)):\n",
    "    preds[i] = preds[i].squeeze().squeeze()\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    preds[i] = preds[i].numpy()\n",
    "\n",
    "predictions = preds[0]\n",
    "for i in range(1,len(preds)):\n",
    "    predictions = np.hstack((predictions,preds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5abe73fe-7769-40ed-858e-3ad1d862f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {'Id':[i for i in range(len(predictions))], 'Predicted':predictions}\n",
    "pred_df = DataFrame(pred_dict)\n",
    "pred_df.Predicted = pred_df.Predicted.apply(lambda x: 1 if x > 0.5 else 0)\n",
    "pred_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f622652f-5545-4a44-8e17-4581e9852eff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BERT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1a22695-b546-4db3-b35f-dc59b70ef88c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RumorClassifier()\n",
    "model.load_state_dict(torch.load('D:\\\\bertcls_11.dat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "260e407b-3dfe-4c14-bfc0-9e2f4c3ae50b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m----> 2\u001b[0m dev_acc, dev_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevelopment F1: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m; Development Loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dev_acc, dev_loss))\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(net, criterion, dataloader, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m stats \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(stats[idx])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#Obtaining the logits from the model\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m mean_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), labels\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# mean_acc += get_accuracy_from_logits(logits, labels)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36mRumorClassifier.forward\u001b[1;34m(self, seq_, attn_masks, seg_, stats)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03mInputs:\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m    -seq : Tensor of shape [B, T] containing token ids of sequences\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m    -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#Feeding the input to BERT model to obtain contextualized representations\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseg_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m cont_reps \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#Obtaining the representation of [CLS] head (the first token)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    987\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    989\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    990\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    991\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    994\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    995\u001b[0m )\n\u001b[1;32m--> 996\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1007\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1008\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1009\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    576\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    577\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    578\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    583\u001b[0m     )\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 585\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:472\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    462\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    471\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:402\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    394\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    401\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 402\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    411\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    412\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:330\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    327\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m    334\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1680\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1678\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1680\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1681\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1682\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "dev_acc, dev_loss = evaluate(model, criterion, dev_loader, 'cpu')\n",
    "print(\"Development F1: {}; Development Loss: {}\".format(dev_acc, dev_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00683eb0-e793-4e20-8d29-d8a9530783db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_tweet = pd.read_csv('./sep_data/test_tweet_df.csv')\n",
    "test_stat = pd.read_csv('./sep_data/test_scaled_stat_feat_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a50c9a8c-f5e2-45a8-9f64-94331ce0dc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 558 entries, 0 to 557\n",
      "Data columns (total 63 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   tweet_id                             558 non-null    int64  \n",
      " 1   reply_contributors                   558 non-null    float64\n",
      " 2   reply_possibly_sensitive             558 non-null    float64\n",
      " 3   reply_possibly_sensitive_appealable  558 non-null    float64\n",
      " 4   reply_retweet_count                  558 non-null    float64\n",
      " 5   reply_favorite_count                 558 non-null    float64\n",
      " 6   reply_mentioned_url_num              558 non-null    float64\n",
      " 7   reply_id_num                         558 non-null    float64\n",
      " 8   reply_followers_count                558 non-null    float64\n",
      " 9   reply_friends_count                  558 non-null    float64\n",
      " 10  reply_listed_count                   558 non-null    float64\n",
      " 11  reply_favourites_count               558 non-null    float64\n",
      " 12  reply_statuses_count                 558 non-null    float64\n",
      " 13  reply_has_url                        558 non-null    float64\n",
      " 14  reply_senti_score                    558 non-null    float64\n",
      " 15  reply_truncated                      558 non-null    float64\n",
      " 16  reply_is_quote_status                558 non-null    float64\n",
      " 17  reply_favorited                      558 non-null    float64\n",
      " 18  reply_retweeted                      558 non-null    float64\n",
      " 19  reply_protected                      558 non-null    float64\n",
      " 20  reply_geo_enabled                    558 non-null    float64\n",
      " 21  reply_verified                       558 non-null    float64\n",
      " 22  reply_isweekday                      558 non-null    float64\n",
      " 23  reply_contributors_enabled           558 non-null    float64\n",
      " 24  reply_is_translator                  558 non-null    float64\n",
      " 25  reply_is_translation_enabled         558 non-null    float64\n",
      " 26  reply_has_extended_profile           558 non-null    float64\n",
      " 27  reply_default_profile                558 non-null    float64\n",
      " 28  reply_default_profile_image          558 non-null    float64\n",
      " 29  reply_following                      558 non-null    float64\n",
      " 30  reply_follow_request_sent            558 non-null    float64\n",
      " 31  reply_notifications                  558 non-null    float64\n",
      " 32  possibly_sensitive                   558 non-null    float64\n",
      " 33  possibly_sensitive_appealable        558 non-null    float64\n",
      " 34  retweet_count                        558 non-null    float64\n",
      " 35  favorite_count                       558 non-null    float64\n",
      " 36  mentioned_url_num                    558 non-null    float64\n",
      " 37  id_num                               558 non-null    float64\n",
      " 38  followers_count                      558 non-null    float64\n",
      " 39  friends_count                        558 non-null    float64\n",
      " 40  listed_count                         558 non-null    float64\n",
      " 41  favourites_count                     558 non-null    float64\n",
      " 42  statuses_count                       558 non-null    float64\n",
      " 43  has_url                              558 non-null    float64\n",
      " 44  senti_score                          558 non-null    float64\n",
      " 45  truncated                            558 non-null    float64\n",
      " 46  is_quote_status                      558 non-null    float64\n",
      " 47  favorited                            558 non-null    float64\n",
      " 48  retweeted                            558 non-null    float64\n",
      " 49  protected                            558 non-null    float64\n",
      " 50  geo_enabled                          558 non-null    float64\n",
      " 51  verified                             558 non-null    float64\n",
      " 52  isweekday                            558 non-null    float64\n",
      " 53  reply_count                          558 non-null    float64\n",
      " 54  contributors_enabled                 558 non-null    float64\n",
      " 55  is_translator                        558 non-null    float64\n",
      " 56  is_translation_enabled               558 non-null    float64\n",
      " 57  has_extended_profile                 558 non-null    float64\n",
      " 58  default_profile                      558 non-null    float64\n",
      " 59  default_profile_image                558 non-null    float64\n",
      " 60  following                            558 non-null    float64\n",
      " 61  follow_request_sent                  558 non-null    float64\n",
      " 62  notifications                        558 non-null    float64\n",
      "dtypes: float64(62), int64(1)\n",
      "memory usage: 274.8 KB\n"
     ]
    }
   ],
   "source": [
    "test_stat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e8d702b-131a-4018-8c63-dd9ad312693a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reply_possibly_sensitive</th>\n",
       "      <th>reply_retweet_count</th>\n",
       "      <th>reply_favorite_count</th>\n",
       "      <th>reply_mentioned_url_num</th>\n",
       "      <th>reply_id_num</th>\n",
       "      <th>reply_followers_count</th>\n",
       "      <th>reply_friends_count</th>\n",
       "      <th>reply_listed_count</th>\n",
       "      <th>reply_favourites_count</th>\n",
       "      <th>reply_statuses_count</th>\n",
       "      <th>...</th>\n",
       "      <th>truncated</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>geo_enabled</th>\n",
       "      <th>verified</th>\n",
       "      <th>isweekday</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>is_translation_enabled</th>\n",
       "      <th>has_extended_profile</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.141158</td>\n",
       "      <td>-0.035464</td>\n",
       "      <td>-0.032914</td>\n",
       "      <td>1.025336</td>\n",
       "      <td>-0.226263</td>\n",
       "      <td>-0.089944</td>\n",
       "      <td>-0.341263</td>\n",
       "      <td>-0.139497</td>\n",
       "      <td>-0.673800</td>\n",
       "      <td>-0.615712</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.954789</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>0.929638</td>\n",
       "      <td>1.251990</td>\n",
       "      <td>-1.722596</td>\n",
       "      <td>-0.502164</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>-0.722953</td>\n",
       "      <td>-0.717882</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.141158</td>\n",
       "      <td>-0.035464</td>\n",
       "      <td>-0.032914</td>\n",
       "      <td>0.331415</td>\n",
       "      <td>-0.226263</td>\n",
       "      <td>-0.089741</td>\n",
       "      <td>-0.154503</td>\n",
       "      <td>-0.137186</td>\n",
       "      <td>0.402520</td>\n",
       "      <td>-0.059858</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047352</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>-1.075688</td>\n",
       "      <td>-0.798728</td>\n",
       "      <td>0.580519</td>\n",
       "      <td>-0.191906</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>-0.722953</td>\n",
       "      <td>1.392986</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.141158</td>\n",
       "      <td>-0.035464</td>\n",
       "      <td>-0.032862</td>\n",
       "      <td>1.025336</td>\n",
       "      <td>-0.947257</td>\n",
       "      <td>-0.085985</td>\n",
       "      <td>-0.341870</td>\n",
       "      <td>-0.128633</td>\n",
       "      <td>-0.660004</td>\n",
       "      <td>-0.592926</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047352</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>0.929638</td>\n",
       "      <td>-0.798728</td>\n",
       "      <td>0.580519</td>\n",
       "      <td>-0.450454</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>-0.722953</td>\n",
       "      <td>1.392986</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.141158</td>\n",
       "      <td>-0.035371</td>\n",
       "      <td>-0.032845</td>\n",
       "      <td>1.025336</td>\n",
       "      <td>-0.947257</td>\n",
       "      <td>-0.089892</td>\n",
       "      <td>-0.334376</td>\n",
       "      <td>-0.139266</td>\n",
       "      <td>-0.676599</td>\n",
       "      <td>-0.614899</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047352</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>-1.075688</td>\n",
       "      <td>-0.798728</td>\n",
       "      <td>0.580519</td>\n",
       "      <td>-0.398744</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>1.383215</td>\n",
       "      <td>1.392986</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.141158</td>\n",
       "      <td>-0.035185</td>\n",
       "      <td>-0.032862</td>\n",
       "      <td>1.025336</td>\n",
       "      <td>0.014069</td>\n",
       "      <td>-0.089822</td>\n",
       "      <td>-0.192516</td>\n",
       "      <td>-0.139189</td>\n",
       "      <td>-0.261550</td>\n",
       "      <td>-0.390870</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047352</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>0.929638</td>\n",
       "      <td>-0.798728</td>\n",
       "      <td>0.580519</td>\n",
       "      <td>-0.243615</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>1.383215</td>\n",
       "      <td>-0.717882</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>0.231649</td>\n",
       "      <td>-0.035453</td>\n",
       "      <td>-0.032908</td>\n",
       "      <td>-0.002200</td>\n",
       "      <td>-0.170801</td>\n",
       "      <td>-0.089666</td>\n",
       "      <td>-0.169913</td>\n",
       "      <td>-0.128633</td>\n",
       "      <td>-0.271496</td>\n",
       "      <td>-0.306987</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.954789</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>0.929638</td>\n",
       "      <td>1.251990</td>\n",
       "      <td>0.580519</td>\n",
       "      <td>0.790579</td>\n",
       "      <td>2.736649</td>\n",
       "      <td>-0.722953</td>\n",
       "      <td>-0.717882</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>-0.141158</td>\n",
       "      <td>-0.035460</td>\n",
       "      <td>-0.032903</td>\n",
       "      <td>-1.285867</td>\n",
       "      <td>-0.133231</td>\n",
       "      <td>-0.089483</td>\n",
       "      <td>-0.221148</td>\n",
       "      <td>-0.135128</td>\n",
       "      <td>-0.357327</td>\n",
       "      <td>-0.314450</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.954789</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>0.929638</td>\n",
       "      <td>1.251990</td>\n",
       "      <td>-1.722596</td>\n",
       "      <td>2.652129</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>1.383215</td>\n",
       "      <td>-0.717882</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>-0.141158</td>\n",
       "      <td>-0.035436</td>\n",
       "      <td>-0.032909</td>\n",
       "      <td>-0.674769</td>\n",
       "      <td>0.062135</td>\n",
       "      <td>-0.089318</td>\n",
       "      <td>-0.044573</td>\n",
       "      <td>-0.125860</td>\n",
       "      <td>-0.345405</td>\n",
       "      <td>-0.104716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.954789</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>-1.075688</td>\n",
       "      <td>1.251990</td>\n",
       "      <td>0.580519</td>\n",
       "      <td>-0.036776</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>-0.722953</td>\n",
       "      <td>-0.717882</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>-0.141158</td>\n",
       "      <td>-0.031976</td>\n",
       "      <td>-0.031284</td>\n",
       "      <td>0.418155</td>\n",
       "      <td>4.640449</td>\n",
       "      <td>1.684324</td>\n",
       "      <td>-0.146248</td>\n",
       "      <td>3.727434</td>\n",
       "      <td>-0.429186</td>\n",
       "      <td>-0.119282</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047352</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>0.929638</td>\n",
       "      <td>1.251990</td>\n",
       "      <td>0.580519</td>\n",
       "      <td>-0.347035</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>1.383215</td>\n",
       "      <td>-0.717882</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>-0.141158</td>\n",
       "      <td>-0.035185</td>\n",
       "      <td>-0.032862</td>\n",
       "      <td>1.025336</td>\n",
       "      <td>-0.586760</td>\n",
       "      <td>-0.089373</td>\n",
       "      <td>-0.147616</td>\n",
       "      <td>-0.139035</td>\n",
       "      <td>0.246796</td>\n",
       "      <td>-0.342999</td>\n",
       "      <td>...</td>\n",
       "      <td>1.047352</td>\n",
       "      <td>-0.152745</td>\n",
       "      <td>-1.075688</td>\n",
       "      <td>-0.798728</td>\n",
       "      <td>-1.722596</td>\n",
       "      <td>-0.450454</td>\n",
       "      <td>-0.365410</td>\n",
       "      <td>-0.722953</td>\n",
       "      <td>-0.717882</td>\n",
       "      <td>-0.050395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>558 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     reply_possibly_sensitive  reply_retweet_count  reply_favorite_count  \\\n",
       "0                   -0.141158            -0.035464             -0.032914   \n",
       "1                   -0.141158            -0.035464             -0.032914   \n",
       "2                   -0.141158            -0.035464             -0.032862   \n",
       "3                   -0.141158            -0.035371             -0.032845   \n",
       "4                   -0.141158            -0.035185             -0.032862   \n",
       "..                        ...                  ...                   ...   \n",
       "553                  0.231649            -0.035453             -0.032908   \n",
       "554                 -0.141158            -0.035460             -0.032903   \n",
       "555                 -0.141158            -0.035436             -0.032909   \n",
       "556                 -0.141158            -0.031976             -0.031284   \n",
       "557                 -0.141158            -0.035185             -0.032862   \n",
       "\n",
       "     reply_mentioned_url_num  reply_id_num  reply_followers_count  \\\n",
       "0                   1.025336     -0.226263              -0.089944   \n",
       "1                   0.331415     -0.226263              -0.089741   \n",
       "2                   1.025336     -0.947257              -0.085985   \n",
       "3                   1.025336     -0.947257              -0.089892   \n",
       "4                   1.025336      0.014069              -0.089822   \n",
       "..                       ...           ...                    ...   \n",
       "553                -0.002200     -0.170801              -0.089666   \n",
       "554                -1.285867     -0.133231              -0.089483   \n",
       "555                -0.674769      0.062135              -0.089318   \n",
       "556                 0.418155      4.640449               1.684324   \n",
       "557                 1.025336     -0.586760              -0.089373   \n",
       "\n",
       "     reply_friends_count  reply_listed_count  reply_favourites_count  \\\n",
       "0              -0.341263           -0.139497               -0.673800   \n",
       "1              -0.154503           -0.137186                0.402520   \n",
       "2              -0.341870           -0.128633               -0.660004   \n",
       "3              -0.334376           -0.139266               -0.676599   \n",
       "4              -0.192516           -0.139189               -0.261550   \n",
       "..                   ...                 ...                     ...   \n",
       "553            -0.169913           -0.128633               -0.271496   \n",
       "554            -0.221148           -0.135128               -0.357327   \n",
       "555            -0.044573           -0.125860               -0.345405   \n",
       "556            -0.146248            3.727434               -0.429186   \n",
       "557            -0.147616           -0.139035                0.246796   \n",
       "\n",
       "     reply_statuses_count  ...  truncated  is_quote_status  geo_enabled  \\\n",
       "0               -0.615712  ...  -0.954789        -0.152745     0.929638   \n",
       "1               -0.059858  ...   1.047352        -0.152745    -1.075688   \n",
       "2               -0.592926  ...   1.047352        -0.152745     0.929638   \n",
       "3               -0.614899  ...   1.047352        -0.152745    -1.075688   \n",
       "4               -0.390870  ...   1.047352        -0.152745     0.929638   \n",
       "..                    ...  ...        ...              ...          ...   \n",
       "553             -0.306987  ...  -0.954789        -0.152745     0.929638   \n",
       "554             -0.314450  ...  -0.954789        -0.152745     0.929638   \n",
       "555             -0.104716  ...  -0.954789        -0.152745    -1.075688   \n",
       "556             -0.119282  ...   1.047352        -0.152745     0.929638   \n",
       "557             -0.342999  ...   1.047352        -0.152745    -1.075688   \n",
       "\n",
       "     verified  isweekday  reply_count  is_translation_enabled  \\\n",
       "0    1.251990  -1.722596    -0.502164               -0.365410   \n",
       "1   -0.798728   0.580519    -0.191906               -0.365410   \n",
       "2   -0.798728   0.580519    -0.450454               -0.365410   \n",
       "3   -0.798728   0.580519    -0.398744               -0.365410   \n",
       "4   -0.798728   0.580519    -0.243615               -0.365410   \n",
       "..        ...        ...          ...                     ...   \n",
       "553  1.251990   0.580519     0.790579                2.736649   \n",
       "554  1.251990  -1.722596     2.652129               -0.365410   \n",
       "555  1.251990   0.580519    -0.036776               -0.365410   \n",
       "556  1.251990   0.580519    -0.347035               -0.365410   \n",
       "557 -0.798728  -1.722596    -0.450454               -0.365410   \n",
       "\n",
       "     has_extended_profile  default_profile  default_profile_image  \n",
       "0               -0.722953        -0.717882              -0.050395  \n",
       "1               -0.722953         1.392986              -0.050395  \n",
       "2               -0.722953         1.392986              -0.050395  \n",
       "3                1.383215         1.392986              -0.050395  \n",
       "4                1.383215        -0.717882              -0.050395  \n",
       "..                    ...              ...                    ...  \n",
       "553             -0.722953        -0.717882              -0.050395  \n",
       "554              1.383215        -0.717882              -0.050395  \n",
       "555             -0.722953        -0.717882              -0.050395  \n",
       "556              1.383215        -0.717882              -0.050395  \n",
       "557             -0.722953        -0.717882              -0.050395  \n",
       "\n",
       "[558 rows x 43 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stat.drop(columns=train_zero, inplace=True)\n",
    "test_stat.drop(columns=['tweet_id'], inplace=True)\n",
    "test_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9f29bdc2-5848-4d0f-8339-b2f69fcc0b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 558 entries, 0 to 557\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweet_id    558 non-null    int64 \n",
      " 1   text        556 non-null    object\n",
      " 2   reply_text  545 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 13.2+ KB\n"
     ]
    }
   ],
   "source": [
    "test_tweet.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c7e3d74d-9603-4dec-b820-cbc49488afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet.text.fillna('', inplace=True)\n",
    "test_tweet.reply_text.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a1069919-5425-4456-9ac5-159877e11660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/558 [00:00<?, ?it/s]C:\\Users\\trist\\AppData\\Local\\Temp\\ipykernel_28584\\3420928069.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_tweet.text.iloc[i] = '[CLS] ' + str(test_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(test_tweet.reply_text.iloc[i]).strip() + ' [SEP]'\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 558/558 [00:07<00:00, 76.96it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_tweet))):\n",
    "    test_tweet.text.iloc[i] = '[CLS] ' + str(test_tweet.text.iloc[i]).strip() + ' [SEP] ' + str(test_tweet.reply_text.iloc[i]).strip() + ' [SEP]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ca68fd5e-aee4-4c2c-917a-43293615b737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 558/558 [00:01<00:00, 287.49it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 256\n",
    "test_seq = []\n",
    "test_mask = []\n",
    "test_seg = []\n",
    "\n",
    "for i in tqdm(range(len(test_tweet))):\n",
    "    try:\n",
    "        txt = test_tweet.text.iloc[i]\n",
    "        tokens = tokenizer.tokenize(txt)\n",
    "        if len(tokens) < max_len:\n",
    "             padded_tokens = tokens + ['[PAD]' for _ in range(max_len - len(tokens))]\n",
    "        else:\n",
    "            padded_tokens = tokens[:max_len-1] + ['[SEP]']\n",
    "        attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "        seg_ids = []\n",
    "        seg_idx = 0\n",
    "        for token in padded_tokens:\n",
    "            seg_ids.append(seg_idx)\n",
    "        if token == '[SEP]':\n",
    "            if seg_idx == 1:\n",
    "                seg_idx = 0\n",
    "            else:\n",
    "                seg_idx=1\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "\n",
    "        test_seq.append(token_ids)\n",
    "        test_mask.append(attn_mask)\n",
    "        test_seg.append(seg_ids)\n",
    "    except:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4c8cbb8e-eee5-4b5c-86a5-2e0c90d1e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestTweetDataset(Data.Dataset):\n",
    "    def __init__(self, seq, mask, seg, stat):\n",
    "        self.seq = torch.tensor(seq).long()\n",
    "        self.mask = torch.tensor(mask).long()\n",
    "        self.seg = torch.tensor(seg).long()\n",
    "        self.stat = torch.tensor(stat).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.seq.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.seq[idx],self.mask[idx],self.seg[idx], self.stat[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dac9a35a-5c60-4c32-971f-45a71480dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = TestTweetDataset(test_seq, test_mask, test_seg, np.array(test_stat))\n",
    "test_loader = Data.DataLoader(test_set, batch_size=64, shuffle=False, sampler=range(0,len(test_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "60b06a00-bccf-4376-8965-fd9839365f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [02:15, 15.09s/it]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "test_stat = np.array(test_stat)\n",
    "with torch.no_grad():\n",
    "    for i, (seq, mask, seg, stat) in tqdm(enumerate(test_loader)):\n",
    "        preds.append(model(seq, mask, seg, stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "312437ab-3532-449c-b63a-f77446597e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# test_stat = np.array(test_stat)\n",
    "# model.eval()\n",
    "# preds = []\n",
    "# with torch.no_grad():\n",
    "#     for i in tqdm(range(len(test_seq))):\n",
    "#         seq = torch.tensor(test_seq[i]).unsqueeze(0)\n",
    "#         mask = torch.tensor(test_mask[i]).unsqueeze(0)\n",
    "#         seg = torch.tensor(test_seg[i]).unsqueeze(0)\n",
    "#         stat = torch.tensor(test_stat[i]).unsqueeze(0).float()\n",
    "        \n",
    "#         preds.append(model(seq,mask,seg,stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "241ffe91-d5aa-4a3d-b66c-c739dd44f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(preds)):\n",
    "    preds[i] = preds[i].squeeze().squeeze()\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    preds[i] = preds[i].numpy()\n",
    "\n",
    "predictions = preds[0]\n",
    "for i in range(1,len(preds)):\n",
    "    predictions = np.hstack((predictions,preds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5e2c52ee-7db7-478b-8f67-bd7bf181764a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03624956, 0.65624285, 0.03985729, 0.03352253, 0.03352937,\n",
       "       0.05078371, 0.9740293 , 0.9703277 , 0.03520356, 0.03600441,\n",
       "       0.97286236, 0.9763545 , 0.03539949, 0.96500146, 0.033732  ,\n",
       "       0.0359007 , 0.04377111, 0.03490631, 0.03606201, 0.03468729,\n",
       "       0.05439142, 0.03644739, 0.0363027 , 0.9785549 , 0.0355593 ,\n",
       "       0.03551107, 0.03479334, 0.04062033, 0.03485068, 0.9798773 ,\n",
       "       0.03416773, 0.94977665, 0.69044083, 0.03423496, 0.03928319,\n",
       "       0.03408812, 0.03484216, 0.03390949, 0.03498546, 0.90170133,\n",
       "       0.03572008, 0.9758766 , 0.03946194, 0.22858931, 0.03410567,\n",
       "       0.03988909, 0.05014799, 0.9796139 , 0.05072801, 0.06103275,\n",
       "       0.03513773, 0.03527077, 0.9776986 , 0.04389564, 0.03494955,\n",
       "       0.9788784 , 0.97674745, 0.03262617, 0.03612356, 0.0376877 ,\n",
       "       0.03632192, 0.9805767 , 0.03974947, 0.03409415, 0.27382717,\n",
       "       0.03608264, 0.9670574 , 0.06094358, 0.04124007, 0.03418873,\n",
       "       0.03411736, 0.03492579, 0.03580131, 0.98042446, 0.0357026 ,\n",
       "       0.9773268 , 0.03301283, 0.9811267 , 0.98098516, 0.03286244,\n",
       "       0.74860406, 0.0339813 , 0.9777665 , 0.03478972, 0.97915626,\n",
       "       0.03569607, 0.0331469 , 0.03468105, 0.03507631, 0.04040399,\n",
       "       0.03675504, 0.04144506, 0.03718083, 0.03566949, 0.03542947,\n",
       "       0.05539167, 0.621674  , 0.03396541, 0.9801673 , 0.03565206,\n",
       "       0.03634229, 0.1094923 , 0.0364605 , 0.97475284, 0.04393336,\n",
       "       0.034561  , 0.9766287 , 0.04382114, 0.03672748, 0.9778332 ,\n",
       "       0.07687882, 0.03849558, 0.03339977, 0.0360531 , 0.08044291,\n",
       "       0.2332104 , 0.03620473, 0.03430237, 0.03473817, 0.03485252,\n",
       "       0.9800261 , 0.03285941, 0.03463617, 0.04353601, 0.03295401,\n",
       "       0.96455497, 0.97976756, 0.04398238, 0.0359968 , 0.97885656,\n",
       "       0.03404232, 0.03522107, 0.14597009, 0.03548724, 0.03299152,\n",
       "       0.922448  , 0.04003886, 0.979006  , 0.03844101, 0.03479662,\n",
       "       0.04894027, 0.0368099 , 0.97323674, 0.03772474, 0.97340786,\n",
       "       0.03466807, 0.03438509, 0.9789642 , 0.03553471, 0.03647379,\n",
       "       0.03449643, 0.05675225, 0.03598045, 0.03517573, 0.96240765,\n",
       "       0.97275746, 0.0336564 , 0.03781731, 0.97488415, 0.0348895 ,\n",
       "       0.9669631 , 0.9798528 , 0.03885352, 0.03771934, 0.9779222 ,\n",
       "       0.03414806, 0.057774  , 0.03812405, 0.03184865, 0.03467657,\n",
       "       0.03519401, 0.97971344, 0.06763921, 0.980298  , 0.97669035,\n",
       "       0.03530844, 0.04204283, 0.0348822 , 0.03455976, 0.03555641,\n",
       "       0.04105579, 0.0661688 , 0.66916305, 0.04548087, 0.03467869,\n",
       "       0.03376213, 0.20441845, 0.03399552, 0.98068213, 0.03789552,\n",
       "       0.27178153, 0.03289348, 0.03474917, 0.036256  , 0.03551793,\n",
       "       0.03482695, 0.03505938, 0.03446341, 0.03490223, 0.0378403 ,\n",
       "       0.03472897, 0.4693038 , 0.03270913, 0.03308868, 0.9776113 ,\n",
       "       0.03607214, 0.968751  , 0.0339037 , 0.03652067, 0.03458085,\n",
       "       0.03991904, 0.98011404, 0.97975266, 0.70772785, 0.9789128 ,\n",
       "       0.06585012, 0.0791543 , 0.03470662, 0.9744805 , 0.97733295,\n",
       "       0.04122815, 0.9745521 , 0.03489403, 0.03403019, 0.03546024,\n",
       "       0.03610495, 0.03435581, 0.03484835, 0.0773863 , 0.03558012,\n",
       "       0.03822242, 0.03862268, 0.03809637, 0.03578282, 0.03326524,\n",
       "       0.04095708, 0.0341719 , 0.97911763, 0.03348744, 0.9798846 ,\n",
       "       0.03709873, 0.07731772, 0.03738308, 0.03341115, 0.04631115,\n",
       "       0.9721977 , 0.06431436, 0.03537958, 0.03747458, 0.03219845,\n",
       "       0.03361405, 0.03854116, 0.91342556, 0.58760774, 0.03290324,\n",
       "       0.03525163, 0.04205138, 0.9782048 , 0.03330069, 0.03364734,\n",
       "       0.03720565, 0.973307  , 0.03855991, 0.03495378, 0.03432075,\n",
       "       0.0365702 , 0.06206581, 0.21616717, 0.9785689 , 0.03230873,\n",
       "       0.03569531, 0.9787377 , 0.03610358, 0.16131341, 0.03520977,\n",
       "       0.26093206, 0.0331779 , 0.9803879 , 0.03560798, 0.97278553,\n",
       "       0.04226788, 0.05303276, 0.03326032, 0.9758676 , 0.69763225,\n",
       "       0.03424   , 0.03338048, 0.03506937, 0.9797448 , 0.03839745,\n",
       "       0.03551429, 0.03340325, 0.05377262, 0.03674098, 0.98058254,\n",
       "       0.06852719, 0.03467387, 0.03554758, 0.03343626, 0.03360424,\n",
       "       0.04275579, 0.03616584, 0.9697509 , 0.9799687 , 0.06725343,\n",
       "       0.03861896, 0.03311808, 0.03543181, 0.03942731, 0.03712864,\n",
       "       0.03815758, 0.03437938, 0.03443335, 0.09952706, 0.03518018,\n",
       "       0.0344119 , 0.03689801, 0.03472982, 0.05259524, 0.03487614,\n",
       "       0.9803182 , 0.03524417, 0.9789596 , 0.9725582 , 0.98037314,\n",
       "       0.03317836, 0.0333199 , 0.03365513, 0.03440976, 0.04366837,\n",
       "       0.0339518 , 0.15723114, 0.03972667, 0.04000484, 0.03633583,\n",
       "       0.9788522 , 0.04633562, 0.98042506, 0.0365567 , 0.96797734,\n",
       "       0.06066669, 0.04126389, 0.03625878, 0.03515816, 0.03854086,\n",
       "       0.97789294, 0.9796958 , 0.04314733, 0.03457048, 0.6998998 ,\n",
       "       0.9790262 , 0.9690016 , 0.9789108 , 0.03425323, 0.03554622,\n",
       "       0.03583011, 0.03467447, 0.03874657, 0.9816598 , 0.03434834,\n",
       "       0.03398438, 0.03697753, 0.98040706, 0.03255649, 0.03516599,\n",
       "       0.07467882, 0.0360612 , 0.03288746, 0.9752524 , 0.03493204,\n",
       "       0.03424019, 0.03444577, 0.97624344, 0.9806043 , 0.04586633,\n",
       "       0.8119898 , 0.03745364, 0.04256393, 0.9802044 , 0.03559423,\n",
       "       0.04609591, 0.03364006, 0.9797305 , 0.03335918, 0.97272205,\n",
       "       0.03343017, 0.03350699, 0.03626813, 0.03688252, 0.9768464 ,\n",
       "       0.03374809, 0.03273793, 0.03673286, 0.9772505 , 0.03815231,\n",
       "       0.03522138, 0.03398898, 0.03295675, 0.03711971, 0.03614292,\n",
       "       0.8829083 , 0.9800401 , 0.04659572, 0.97400856, 0.03308652,\n",
       "       0.03622399, 0.33882758, 0.9720233 , 0.03426794, 0.03803564,\n",
       "       0.03614132, 0.9788977 , 0.9632918 , 0.03361514, 0.97888297,\n",
       "       0.9691619 , 0.0382442 , 0.03390186, 0.03438025, 0.03672689,\n",
       "       0.03638491, 0.05362248, 0.97450656, 0.03950613, 0.9790981 ,\n",
       "       0.97181726, 0.03639959, 0.03855886, 0.03498931, 0.03675603,\n",
       "       0.90758973, 0.96655154, 0.03610472, 0.03349277, 0.04561852,\n",
       "       0.0358824 , 0.03551628, 0.03698836, 0.03743002, 0.71385956,\n",
       "       0.03454827, 0.97386485, 0.03492843, 0.03429109, 0.130161  ,\n",
       "       0.0353299 , 0.97948986, 0.03707332, 0.66148996, 0.07804795,\n",
       "       0.06183098, 0.03351519, 0.06850105, 0.03551163, 0.9795744 ,\n",
       "       0.9765629 , 0.03364043, 0.9653037 , 0.0375176 , 0.03357619,\n",
       "       0.0359311 , 0.03416755, 0.04838939, 0.03545092, 0.03502628,\n",
       "       0.17036985, 0.03576591, 0.03368951, 0.03706859, 0.03480219,\n",
       "       0.039368  , 0.03761455, 0.03478353, 0.0345798 , 0.9799467 ,\n",
       "       0.98002106, 0.9756708 , 0.97899354, 0.03472905, 0.03426874,\n",
       "       0.03432868, 0.07664379, 0.0352142 , 0.03482182, 0.03994459,\n",
       "       0.03488823, 0.97717834, 0.03725308, 0.03991449, 0.03448552,\n",
       "       0.03388062, 0.03382556, 0.9473526 , 0.06464744, 0.03431522,\n",
       "       0.03420815, 0.0351158 , 0.03356889, 0.0337282 , 0.03980258,\n",
       "       0.03441852, 0.9807438 , 0.03586385, 0.03693181, 0.03469029,\n",
       "       0.04017802, 0.97563076, 0.0338874 , 0.03409957, 0.03452308,\n",
       "       0.03424807, 0.04591627, 0.03209414, 0.0496512 , 0.96376646,\n",
       "       0.03887684, 0.97305524, 0.034049  , 0.97868323, 0.03399699,\n",
       "       0.05020914, 0.03523295, 0.04112864, 0.973769  , 0.03512938,\n",
       "       0.03735521, 0.03518995, 0.9194943 , 0.03471184, 0.04926782,\n",
       "       0.03960348, 0.0353182 , 0.03703282, 0.9779814 , 0.9789397 ,\n",
       "       0.05248694, 0.03605346, 0.0327753 , 0.03512475, 0.03782206,\n",
       "       0.03604085, 0.03741778, 0.03226921, 0.03292313, 0.03841229,\n",
       "       0.04525803, 0.04865985, 0.8977587 , 0.03329338, 0.97478557,\n",
       "       0.04599257, 0.03694528, 0.9784897 , 0.03747212, 0.03922342,\n",
       "       0.97989035, 0.03600019, 0.03445781], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "58b84233-9e78-44c6-b99d-932f91109f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {'Id':[i for i in range(len(predictions))], 'Predicted':predictions}\n",
    "pred_df = DataFrame(pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3574adbd-b4af-4ce4-b873-b03161c54ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.036250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.656243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.039857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.033523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.033529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>553</td>\n",
       "      <td>0.037472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>554</td>\n",
       "      <td>0.039223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>555</td>\n",
       "      <td>0.979890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>556</td>\n",
       "      <td>0.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>557</td>\n",
       "      <td>0.034458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>558 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  Predicted\n",
       "0      0   0.036250\n",
       "1      1   0.656243\n",
       "2      2   0.039857\n",
       "3      3   0.033523\n",
       "4      4   0.033529\n",
       "..   ...        ...\n",
       "553  553   0.037472\n",
       "554  554   0.039223\n",
       "555  555   0.979890\n",
       "556  556   0.036000\n",
       "557  557   0.034458\n",
       "\n",
       "[558 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8de5c313-831d-4d7f-bbe2-7ee219671ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.Predicted = pred_df.Predicted.apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d67d4be5-0389-48af-af69-3471cbc6bdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.Predicted.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6c4b7f3f-999d-4a6b-9b1a-a0adf646462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3fe9a67-e3d7-4dc3-aaa3-1cd5232428a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('predictions1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9e3e681-7d3e-403e-85d3-e669ae16c956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(a)):\n",
    "    if a.iloc[i].Predicted != pred_df.iloc[i].Predicted:\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70c29698-030b-4be7-918d-b8942f828d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(558, 256)\n",
      "(558, 24)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(test_seq).shape)\n",
    "print(np.array(test_stat).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e4a48f8-54cb-4ec0-a742-c9d19d74f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = torch.tensor(test_seq).long()\n",
    "test_mask = torch.tensor(test_mask).long()\n",
    "test_seg = torch.tensor(test_seg).long()\n",
    "test_stat = torch.tensor(np.array(test_stat)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b8c5b1-37c1-4333-a42b-ece0470ed583",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "test_stat = np.array(test_stat)\n",
    "for i in range(len(test_seq)):\n",
    "    seq = torch.tensor(test_seq[i]).long().unsqueeze(0)\n",
    "    mask = torch.tensor(test_mask[i]).long().unsqueeze(0)\n",
    "    seg = torch.tensor(test_seg[i]).long().unsqueeze(0)\n",
    "    stat = torch.tensor(test_stat[i,1:]).float().unsqueeze(0)\n",
    "    preds.append(model(seq, mask, seg, stat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e460c87-f926-4d28-bef7-93dc439938af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create Train and Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "feecfa1f-4cfe-498b-acdd-7ea02a534380",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_dev = []\n",
    "for i in range(train_stat.shape[0]):\n",
    "    X_train.append(list(train_tokens[i].reshape(-1)))\n",
    "    for j in range(1,train_stat.shape[1]):\n",
    "        X_train[i].append(train_stat.iloc[i,j])\n",
    "\n",
    "for i in range(dev_stat.shape[0]):\n",
    "    X_dev.append(list(dev_tokens[i].reshape(-1)))\n",
    "    for j in range(1,dev_stat.shape[1]):\n",
    "        X_dev[i].append(dev_stat.iloc[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4450b35a-db76-45dc-8139-435a7f25e6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1542"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "442b807a-76ac-4cb9-b6a1-f3c0fed6ad63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3382aaff-cdec-4aa9-9f64-4ae2cafae6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_tweet['label']\n",
    "y_dev = dev_tweet['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e435d-c046-4abf-bc48-8363c3aa35ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Test on simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e804cab2-226c-4c48-b725-5940eba222d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1e0a2bed-a9c8-4176-a6e9-dff543d9272b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.867816091954023"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(predictions, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7a2b01c3-66e6-45f3-bf3e-a0f6259d66fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.943579766536965"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds = lr.predict(X_train)\n",
    "accuracy_score(train_preds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "030b153a-0955-4dbc-9a82-38a0ca5f640b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6609195402298851"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "accuracy_score(gnb.predict(X_dev), y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9bdb3ba2-1573-4a79-94d2-371ccedffd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('X_train.pkl', 'wb') as file:\n",
    "    pickle.dump(X_train, file)\n",
    "with open('X_dev.pkl', 'wb') as file:\n",
    "    pickle.dump(X_dev, file)\n",
    "with open('y_train.pkl', 'wb') as file:\n",
    "    pickle.dump(list(y_train), file)\n",
    "with open('y_dev.pkl', 'wb') as file:\n",
    "    pickle.dump(list(y_dev), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a39838-4e40-4d32-a268-0445606cd164",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Test simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4d72b566-f8d7-4da6-8933-78a1bdd0ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mlp, self).__init__()\n",
    "        self.ffnn = nn.Sequential(nn.Linear(791,128),\n",
    "                                  nn.ReLU(),\n",
    "                                  # nn.Dropout(0.3),\n",
    "                                 nn.Linear(128,64),\n",
    "                                  nn.ReLU(),\n",
    "                                  # nn.Dropout(0.3),\n",
    "                                  nn.Linear(64,1),\n",
    "                                  nn.Sigmoid()\n",
    "                                 )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.ffnn(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ef89ef1c-17d2-416b-974e-71a914a25c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b334580150>"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d50aff54-ec06-4038-8187-b2d2dc4e6033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "EPOCH = 5\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "8d055d7b-773e-4d50-b92e-0155c0a54037",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_dev = np.array(X_dev)\n",
    "y_train = np.array(y_train)\n",
    "y_dev = np.array(y_dev)\n",
    "X = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "dev_X = torch.from_numpy(X_dev).type(torch.FloatTensor)\n",
    "dev_y = torch.from_numpy(y_dev).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "70a0b495-7d5c-40b5-bffb-76c7aaa151b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1542"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e33cc3f4-b096-4594-974b-ee9995d5c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "296dc716-edd7-4ce6-b663-efb7fe28f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TweetDataset(X,y)\n",
    "dev_set = TweetDataset(dev_X, dev_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f355687f-8290-42c9-9dda-f8e72d35acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = Data.DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "78c34c8c-9150-4c49-9ba5-e08d0e4d0d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, criterion, dataloader, device):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, labels in dataloader:\n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            logits = net(seq)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "7749fc1b-31f6-4621-977f-1b0815999566",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = mlp()\n",
    "net = net.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "opti = optim.Adam(net.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "14e9825f-acd4-417d-8e84-57894369eed6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of epoch 0 complete. \n",
      " Loss: 0.02590068057179451; Accuracy: 1.0; Time taken (s): 0.004000186920166016\n",
      "Development Accuracy: 0.8802083134651184; Development Loss: 0.434159043762419\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "best_acc = 0\n",
    "st = time.time()\n",
    "eps = []\n",
    "t_loss = []\n",
    "d_loss = []\n",
    "for ep in range(1):\n",
    "    eps.append(ep)\n",
    "    net.train()\n",
    "    for it, (seq, labels) in enumerate(train_loader):\n",
    "        \n",
    "        #Clear gradients\n",
    "        opti.zero_grad()\n",
    "        #Converting these to cuda tensors\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "\n",
    "        #Obtaining the logits from the model\n",
    "        logits = net(seq)\n",
    "        \n",
    "        #Computing loss\n",
    "        loss = criterion(logits.squeeze(), labels.float())\n",
    "\n",
    "        #Backpropagating the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #Optimization step\n",
    "        opti.step()\n",
    "\n",
    "        if it % 100 == 0:\n",
    "\n",
    "            acc = get_accuracy_from_logits(logits, labels)\n",
    "            print(\"Iteration {} of epoch {} complete. \\n Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "        \n",
    "    dev_acc, dev_loss = evaluate(net, criterion, dev_loader, device)\n",
    "    t_loss.append(loss.item())\n",
    "    d_loss.append(dev_loss)\n",
    "    print(\"Development Accuracy: {}; Development Loss: {}\".format(dev_acc, dev_loss))\n",
    "    if dev_acc > best_acc:\n",
    "        # print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "        best_acc = dev_acc\n",
    "        # torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "289e761b-9b28-4e76-a02c-31eabbc776a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(eps,t_loss)\n",
    "plt.plot(eps,d_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "95f7cc04-ce28-417b-a1d4-41cc3bb75eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707ac33-63aa-4e81-b71e-604078a51291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
