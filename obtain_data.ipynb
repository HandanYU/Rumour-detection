{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HandanYU/Rumour-detection/blob/main/obtain_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6ES8bVt7_mL"
   },
   "source": [
    "# 1. split source reply ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aCDBWMxE633r"
   },
   "outputs": [],
   "source": [
    "@timer('ms')\n",
    "def split_source_reply(txt_file):\n",
    "  \"\"\"\n",
    "  txt_file: 'train.data.txt'\n",
    "  \"\"\"\n",
    "  with open(txt_file) as f:\n",
    "      ids = f.readlines()\n",
    "  source_ids = []\n",
    "  reply_ids = []\n",
    "  source_txt_file = txt_file.split('.')[0] + '_source_data.txt'\n",
    "  reply_txt_file = txt_file.split('.')[0] + '_reply_data.txt'\n",
    "  for i in range(len(ids)):\n",
    "      source_ids.append(ids[i].split(',')[0].strip())\n",
    "      reply_ids.extend([r.strip() for r in ids[i].split(',')[1:]])\n",
    "# save source_ids\n",
    "  with open(source_txt_file,'w') as f:\n",
    "      for i in source_ids:\n",
    "          f.write(i)\n",
    "          f.write('\\n')\n",
    "# save reply_ids\n",
    "  with open(reply_txt_file,'w') as f:\n",
    "      for i in reply_ids:\n",
    "        f.write(i)\n",
    "        f.write('\\n')\n",
    "# split_source_reply('dev.data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6THaa5Px6v6b"
   },
   "source": [
    "# 2. Crawl tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CvCIHQI6mwz"
   },
   "outputs": [],
   "source": [
    "# !twarc2 hydrate  dev.txt > dev_reply_data.jsonl\n",
    "# resource: https://scholarslab.github.io/learn-twarc/06-twarc-command-basics.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhcR1aXv62R_"
   },
   "source": [
    "# 3. Get tweet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Qg_VrPBz6gxy"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "@timer('ms')\n",
    "def filter_feature(jsonl_file_name):\n",
    "    \"\"\"\n",
    "    jsonl_file_name: 'dev_source_data.jsonl'\n",
    "    \"\"\"\n",
    "    json_file_name = '_'.join(jsonl_file_name.split('_')[:-1]) + '.json'\n",
    "    json_data = pd.read_json(path_or_buf=jsonl_file_name, lines=True)\n",
    "    data_dict = defaultdict(dict)\n",
    "    for i in range(json_data.shape[0]):\n",
    "        for j in range(len(json_data.data.iloc[i])):\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['text'] = json_data.data.iloc[i][j]['text']\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['reply_count'] = json_data.data.iloc[i][j]['public_metrics']['reply_count']\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['like_count'] = json_data.data.iloc[i][j]['public_metrics']['like_count']\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['retweet_count'] = json_data.data.iloc[i][j]['public_metrics']['retweet_count']\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['quote_count'] = json_data.data.iloc[i][j]['public_metrics']['quote_count']\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['possibly_sensitive'] = json_data.data.iloc[i][j]['possibly_sensitive']\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['created_at'] = json_data.data.iloc[i][j]['created_at'] #add create time\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['user_id'] = json_data.data.iloc[i][j]['author_id'] #add user id\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['has_url'] = 1 if 'entities' in json_data.data.iloc[i][j] else 0 #add shared url number\n",
    "            \n",
    "\n",
    "    #  convert into json format\n",
    "    dict_json=json.dumps(data_dict)\n",
    "    # save json file\n",
    "    with open(json_file_name, 'w+') as file:\n",
    "        file.write(dict_json)\n",
    "# filter_feature('dev_reply_data.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dump data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call filter_feature in 203.33385467529297 ms\n",
      "Call filter_feature in 1865.3488159179688 ms\n",
      "Call filter_feature in 418.25294494628906 ms\n",
      "Call filter_feature in 5743.713140487671 ms\n"
     ]
    }
   ],
   "source": [
    "jsonls = ['./data/full data/dev_source_data.jsonl',\n",
    "          './data/full data/dev_reply_data.jsonl',\n",
    "          './data/full data/train_source_data.jsonl',\n",
    "          './data/full data/train_reply_data.jsonl']\n",
    "\n",
    "for jsonl in jsonls:\n",
    "    filter_feature(jsonl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get user info\n",
    "#### currently only implement on Source tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call get_user_info in 132.65705108642578 ms\n",
      "Call get_user_info in 304.43310737609863 ms\n"
     ]
    }
   ],
   "source": [
    "@timer('ms')\n",
    "def get_user_info(jsonl_file_name):\n",
    "    \"\"\"\n",
    "    jsonl_file_name: 'dev_source_data.jsonl'\n",
    "    \"\"\"\n",
    "    json_file_name = '_'.join(jsonl_file_name.split('_')[:-1]) + '_userinfo.json'\n",
    "    json_data = pd.read_json(path_or_buf=jsonl_file_name, lines=True)\n",
    "    # collect the user info\n",
    "    info_dict = defaultdict(dict)\n",
    "    for i in range(json_data.shape[0]):\n",
    "        for j in range(len(json_data.includes.iloc[i]['users'])):\n",
    "            info_dict[json_data.includes.iloc[i]['users'][j]['id']]['followers_count'] = json_data.includes.iloc[i]['users'][j]['public_metrics']['followers_count']\n",
    "            info_dict[json_data.includes.iloc[i]['users'][j]['id']]['tweet_count'] = json_data.includes.iloc[i]['users'][j]['public_metrics']['tweet_count']\n",
    "            info_dict[json_data.includes.iloc[i]['users'][j]['id']]['verified'] = json_data.includes.iloc[i]['users'][j]['verified']\n",
    "    #  convert into json format\n",
    "    dict_json=json.dumps(info_dict)\n",
    "    # save json file\n",
    "    with open(json_file_name, 'w+') as file:\n",
    "        file.write(dict_json)\n",
    "    \n",
    "jsonls = ['./data/full data/dev_source_data.jsonl',\n",
    "          './data/full data/train_source_data.jsonl']\n",
    "\n",
    "for jsonl in jsonls:\n",
    "    get_user_info(jsonl)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMHgShQ0IwgSBKh9Qpb89Fv",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "obtain data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
