{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HandanYU/Rumour-detection/blob/main/obtain_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6ES8bVt7_mL"
   },
   "source": [
    "# 1. split source reply ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from process import clean_test_data, concat_reply, check_weekday_test, concat_reply_info, extract_stat_tweet_feat\n",
    "def merge_json(data_type, source_or_reply, ids_list):\n",
    "    # merged_json: \"test_source.json\"\n",
    "    merges_file = os.path.join(f'./tweepy_data/objects/', f'{data_type}_{source_or_reply}.json')\n",
    "    path_results = f'./tweepy_data/objects/{data_type}_objects'\n",
    "    with open(merges_file, \"w\", encoding=\"utf-8\") as f0:\n",
    "        for file in os.listdir(path_results):\n",
    "            if file.split('.')[0] in ids_list:\n",
    "                print('write')\n",
    "                with open(os.path.join(path_results, file), \"r\", encoding=\"utf-8\") as f1:\n",
    "                    for line in tqdm.tqdm(f1):\n",
    "                        line_dict = json.loads(line)\n",
    "                        js = json.dumps(line_dict, ensure_ascii=False)\n",
    "                        f0.write(js + '\\n')\n",
    "                    f1.close()\n",
    "        f0.close()\n",
    "\n",
    "def sort_by_time(raw_file, json_file):\n",
    "    with open(raw_file) as file:\n",
    "        ids = file.readlines()\n",
    "    df = pd.read_json(path_or_buf=json_file, lines=True)\n",
    "    df.index = [str(i) for i in df['id']]\n",
    "    save_name = raw_file[:-4] + '_sorted.txt'\n",
    "    with open(save_name, 'w') as file:\n",
    "        date = pd.Series(pd.DatetimeIndex(df['created_at']), index=df.index)\n",
    "        df.drop(['created_at'], axis=1, inplace=True)\n",
    "        df['time'] = date\n",
    "        for id_ in ids:\n",
    "            ids_ = id_.strip().split(',')\n",
    "            source_id = ids_[0]\n",
    "            file.write(source_id)\n",
    "            if len(ids_) > 1:\n",
    "                reply_ids = ids_[1:]\n",
    "                reply_ids[-1] = reply_ids[-1].strip()\n",
    "                valid_ids = [index for index in reply_ids if index in df.index]\n",
    "                sorted_replies = df.loc[valid_ids].sort_values(by='time')\n",
    "                if len(valid_ids) > 0:\n",
    "                    file.write(',')\n",
    "\n",
    "                for i, index in enumerate(sorted_replies.index):\n",
    "                    file.write(index)\n",
    "                    if i != len(sorted_replies.index) - 1:\n",
    "                        file.write(',')\n",
    "\n",
    "            file.write('\\n')\n",
    "def concat_reply(data_type, source_df):\n",
    "    \"\"\"concat replies on source tweets\n",
    "    data_type: 'dev', 'train', 'test'\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=['tweet_id', 'reply'])\n",
    "    with open(f'./tweepy_data/original_data/{data_type}.data_sorted.txt', 'r') as f:\n",
    "    # with open(f'./data/original_data/{data_type}.data_sorted.txt', 'r') as f:\n",
    "        content = f.readlines()\n",
    "    df['tweet_id'] = [c.split(',')[0].strip() for c in content]\n",
    "    df['reply'] = [','.join([i.strip() for i in c.split(',')[1:]]) for c in content]\n",
    "    source_df = pd.merge(source_df, df, on='tweet_id', how='left')\n",
    "    return source_df\n",
    "def concat_label(data_type, source_feature_df):\n",
    "    \"\"\"Concat labels on source tweets\n",
    "    data_type: 'dev', 'train'\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=['tweet_id', 'label'])\n",
    "    with open(f'./tweepy_data/original_data/{data_type}_source.txt', 'r') as f:\n",
    "        ids = f.readlines()\n",
    "    with open(f'./tweepy_data/original_data/{data_type}.label.txt', 'r') as f:\n",
    "        labels = f.readlines()\n",
    "    df['tweet_id'] = [id.strip() for id in ids]\n",
    "    df['label'] = [label.strip() for label in labels]\n",
    "    df_labels = pd.merge(source_feature_df, df, on='tweet_id', how='left')\n",
    "    df_labels['label'] = df_labels['label'].apply(lambda x: 0 if x == 'nonrumour' else 1)\n",
    "    return df_labels\n",
    "\n",
    "\n",
    "def processing(data_type):\n",
    "\n",
    "    # './data/tweet-objects/test_source.json'\n",
    "    used_cols = ['created_at', 'id', 'id_str', 'text', 'truncated', 'entities', 'source',\n",
    "       'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
    "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
    "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
    "       'contributors', 'is_quote_status', 'retweet_count', 'favorite_count',\n",
    "       'favorited', 'retweeted', 'lang', 'extended_entities',\n",
    "       'possibly_sensitive', 'possibly_sensitive_appealable',\n",
    "       'quoted_status_id', 'quoted_status_id_str', 'quoted_status']\n",
    "    source_df = pd.read_json(path_or_buf=f'./tweepy_data/objects/{data_type}_source.json', lines=True)\n",
    "    source_df = source_df[used_cols]\n",
    "    reply_df= pd.read_json(path_or_buf=f'./tweepy_data/objects/{data_type}_reply.json', lines=True)\n",
    "    reply_df = reply_df[used_cols]\n",
    "    source_df = clean_test_data(source_df)\n",
    "    reply_df = clean_test_data(reply_df)\n",
    "    \n",
    "    \n",
    "    # get 'verified', 'followers_count', 'listed_count'\n",
    "    for i in [ 'protected', 'followers_count', 'friends_count', \n",
    "                'listed_count', 'favourites_count', 'geo_enabled', 'verified', \n",
    "                'statuses_count', 'contributors_enabled']:\n",
    "        source_df[i] = source_df['user'].apply(lambda x: x[i])\n",
    "        reply_df[i] = reply_df['user'].apply(lambda x: x[i])\n",
    "    source_df['has_url'] = source_df['entities'].apply(lambda x: 0 if len(x['urls']) == 0 else 1)\n",
    "    # get reply statistic info\n",
    "    reply_df['has_url'] = reply_df['entities'].apply(lambda x: 0 if len(x['urls']) == 0 else 1)\n",
    "    source_df = concat_reply(data_type, source_df)\n",
    "    # get reply count\n",
    "    source_df['reply_count'] = source_df['reply'].apply(lambda x: len(x.split(',')))\n",
    "    source_df.index = [str(i) for i in source_df['tweet_id']]\n",
    "\n",
    "    # sorted ids\n",
    "    if data_type == 'test':\n",
    "        with open('tweep_data/original_data/test_source.txt', 'r') as f:\n",
    "            c = f.readlines()\n",
    "        source_df = source_df.loc[[i.strip() for i in c]]\n",
    "    source_df = check_weekday_test(source_df)\n",
    "    reply_df = check_weekday_test(reply_df)\n",
    "    # add sentiment score\n",
    "    source_df['senti_score'] = source_df['text'].apply(lambda x: 1 if TextBlob(x).sentiment.polarity > 0 else 0)\n",
    "    reply_df['senti_score'] = reply_df['text'].apply(lambda x: 1 if TextBlob(x).sentiment.polarity > 0 else 0)\n",
    "    reply_df.index = [str(i) for i in reply_df['tweet_id']]\n",
    "    # reply_df = reply_df.rename(columns={'retweet_count': 'retweet_count', 'favorite_count': 'like_count',\n",
    "    #                                     'mentioned_url_num': 'mentioned_url_num', 'id_num': 'id_num', 'isweekday': 'isweekday'})\n",
    "    # source_df.index = source_df['tweet_id']\n",
    "    # source_df = source_df.rename(columns={'retweet_count': 'retweet_count', 'favorite_count': 'like_count', 'followers_count': 'followers_count',\n",
    "    #                                     'mentioned_url_num': 'mentioned_url_num', 'id_num': 'id_num', 'isweekday': 'isweekday', \n",
    "    #                                     'verified': 'verified', 'listed_count': 'tweet_count'})\n",
    "    # concat replies info to source_df\n",
    "    # reply_count, quote_count\n",
    "    count_feat = ['in_reply_to_status_id',\n",
    "       'in_reply_to_user_id','quoted_status_id']\n",
    "    statis_feature = [ 'contributors',\n",
    "       'possibly_sensitive', 'possibly_sensitive_appealable', 'retweet_count', 'favorite_count', 'mentioned_url_num', 'id_num',\n",
    "       'followers_count', 'friends_count', 'listed_count', 'favourites_count',\n",
    "       'statuses_count', 'has_url', 'senti_score','truncated', 'is_quote_status', 'favorited', 'retweeted', 'protected',\n",
    "       'geo_enabled', 'verified', 'contributors_enabled', 'isweekday']\n",
    "    source_df[['reply_text'] + ['reply_' + s for s in statis_feature]] = source_df.apply(lambda x: concat_reply_info(x['reply'], reply_df, statis_feature), axis=1, result_type='expand')          \n",
    "    if data_type == 'train':\n",
    "        source_df = concat_label(data_type, source_df)\n",
    "    # source_df[['reply_reply_count', 'reply_quote_count', 'quote_count']] = 0      \n",
    "    return source_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_source_reply(txt_file):\n",
    "  \"\"\"\n",
    "  txt_file: 'train.data.txt'\n",
    "  \"\"\"\n",
    "  with open(txt_file) as f:\n",
    "      ids = f.readlines()\n",
    "  source_ids = []\n",
    "  reply_ids = []\n",
    "  source_txt_file = txt_file.split('.')[0] + '_source.txt'\n",
    "  reply_txt_file = txt_file.split('.')[0] + '_reply.txt'\n",
    "  for i in range(len(ids)):\n",
    "      source_ids.append(ids[i].split(',')[0].strip())\n",
    "      reply_ids.extend([r.strip() for r in ids[i].split(',')[1:]])\n",
    "# save source_ids\n",
    "  with open(source_txt_file,'w') as f:\n",
    "      for i in source_ids:\n",
    "          f.write(i)\n",
    "          f.write('\\n')\n",
    "# save reply_ids\n",
    "  with open(reply_txt_file,'w') as f:\n",
    "      for i in reply_ids:\n",
    "        f.write(i)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stat_tweet_feat(istrain, df):\n",
    "    # extract statistic features\n",
    "    # reply_reply_count， reply_quote_count，quote_count\n",
    "    statistic_features = ['reply_' + i for i in ['in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
    "       'in_reply_to_user_id', 'in_reply_to_user_id_str', 'contributors',\n",
    "       'possibly_sensitive', 'possibly_sensitive_appealable',\n",
    "       'quoted_status_id', 'quoted_status_id_str', 'retweet_count', 'favorite_count', 'mentioned_url_num', 'id_num',\n",
    "       'followers_count', 'friends_count', 'listed_count', 'favourites_count',\n",
    "       'statuses_count', 'has_url', 'senti_score','truncated', 'is_quote_status', 'favorited', 'retweeted', 'protected',\n",
    "       'geo_enabled', 'verified', 'contributors_enabled', 'isweekday']] + ['in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
    "       'in_reply_to_user_id', 'in_reply_to_user_id_str', 'contributors',\n",
    "       'possibly_sensitive', 'possibly_sensitive_appealable',\n",
    "       'quoted_status_id', 'quoted_status_id_str', 'retweet_count', 'favorite_count', 'mentioned_url_num', 'id_num',\n",
    "       'followers_count', 'friends_count', 'listed_count', 'favourites_count',\n",
    "       'statuses_count', 'has_url', 'senti_score','truncated', 'is_quote_status', 'favorited', 'retweeted', 'protected',\n",
    "       'geo_enabled', 'verified', 'contributors_enabled', 'isweekday', 'reply_count']\n",
    "    stat_feat_df = df[statistic_features]\n",
    "    stat_feat_df.index = df['tweet_id']\n",
    "    if istrain:\n",
    "        tweet_df = df[['tweet_id', 'text', 'reply_text', 'label']]\n",
    "    else:\n",
    "        tweet_df = df[['tweet_id', 'text', 'reply_text']]\n",
    "    # tweet_df = df.drop(columns=statistic_features)\n",
    "    tweet_df.index = df['tweet_id']\n",
    "    # convert into float\n",
    "    # for col in ['tweet_count', 'followers_count', 'verified']:\n",
    "    #     stat_feat_df[col] = stat_feat_df[col].apply(lambda x: float(x))\n",
    "    # fill nan using corresponding mean\n",
    "    stat_feat_df = stat_feat_df.fillna(stat_feat_df.mean())\n",
    "    return stat_feat_df, tweet_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_source_reply('tweepy_data/original_data/train.data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweepy_data/original_data/train_source.txt', 'r') as f:\n",
    "    content = f.readlines()\n",
    "source_ids = [c.strip() for c in content]\n",
    "with open('data/original_data/test_reply.txt', 'r') as f:\n",
    "    content = f.readlines()\n",
    "reply_ids = [c.strip() for c in content]\n",
    "merge_json('train', 'source', source_ids)\n",
    "merge_json('train','reply', reply_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files = ['./tweepy_data/original_data/train.data.txt']\n",
    "json_files = ['./tweepy_data/objects/train_reply.json']\n",
    "for raw_file, json_file in zip(raw_files, json_files):\n",
    "    sort_by_time(raw_file, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = pd.read_json(path_or_buf=f'./tweepy_data/objects/train_source.json', lines=True)\n",
    "reply_df= pd.read_json(path_or_buf=f'./tweepy_data/objects/train_reply.json', lines=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = ['created_at', 'id', 'id_str', 'text', 'truncated', 'entities', 'source',\n",
    "       'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
    "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
    "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
    "       'contributors', 'is_quote_status', 'retweet_count', 'favorite_count',\n",
    "       'favorited', 'retweeted', 'lang', 'extended_entities',\n",
    "       'possibly_sensitive', 'possibly_sensitive_appealable',\n",
    "       'quoted_status_id', 'quoted_status_id_str', 'quoted_status']\n",
    "source_df = source_df[used_cols]\n",
    "reply_df = reply_df[used_cols]\n",
    "source_df = clean_test_data(source_df)\n",
    "reply_df = clean_test_data(reply_df)\n",
    "\n",
    "\n",
    "# get 'verified', 'followers_count', 'listed_count'\n",
    "for i in [ 'protected', 'followers_count', 'friends_count', \n",
    "            'listed_count', 'favourites_count', 'geo_enabled', 'verified', \n",
    "            'statuses_count', 'contributors_enabled']:\n",
    "    source_df[i] = source_df['user'].apply(lambda x: x[i])\n",
    "source_df['has_url'] = source_df['entities'].apply(lambda x: 0 if len(x['urls']) == 0 else 1)\n",
    "# get reply statistic info\n",
    "reply_df['has_url'] = reply_df['entities'].apply(lambda x: 0 if len(x['urls']) == 0 else 1)\n",
    "source_df = concat_reply('train', source_df)\n",
    "# get reply count\n",
    "source_df['reply_count'] = source_df['reply'].apply(lambda x: len(x.split(',')))\n",
    "source_df.index = [str(i) for i in source_df['tweet_id']]\n",
    "\n",
    "# sorted ids\n",
    "# if data_type == 'test':\n",
    "#     with open('tweep_data/original_data/test_source.txt', 'r') as f:\n",
    "#         c = f.readlines()\n",
    "#     source_df = source_df.loc[[i.strip() for i in c]]\n",
    "source_df = check_weekday_test(source_df)\n",
    "reply_df = check_weekday_test(reply_df)\n",
    "# add sentiment score\n",
    "source_df['senti_score'] = source_df['text'].apply(lambda x: 1 if TextBlob(x).sentiment.polarity > 0 else 0)\n",
    "reply_df['senti_score'] = reply_df['text'].apply(lambda x: 1 if TextBlob(x).sentiment.polarity > 0 else 0)\n",
    "reply_df.index = [str(i) for i in reply_df['tweet_id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = processing('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stat_feat_df, train_tweet_df = extract_stat_tweet_feat(True, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aCDBWMxE633r"
   },
   "outputs": [],
   "source": [
    "# @timer('ms')\n",
    "def split_source_reply(txt_file):\n",
    "  \"\"\"\n",
    "  txt_file: 'train.data.txt'\n",
    "  \"\"\"\n",
    "  with open(txt_file) as f:\n",
    "      ids = f.readlines()\n",
    "  source_ids = []\n",
    "  reply_ids = []\n",
    "  index = 0\n",
    "  \n",
    "  for i in range(len(ids)):\n",
    "      source_ids.append(ids[i].split(',')[0].strip())\n",
    "      reply_ids.extend([r.strip() for r in ids[i].split(',')[1:]])\n",
    "      if i > 0 and i % 5000 == 0:\n",
    "        source_txt_file = txt_file.split('.')[0] + f'_source_data_{index}.txt'\n",
    "        print(source_txt_file)\n",
    "        reply_txt_file = txt_file.split('.')[0] + f'_reply_data_{index}.txt'\n",
    "      # save source_ids\n",
    "        with open(source_txt_file,'w') as f:\n",
    "          for i in source_ids:\n",
    "              f.write(i)\n",
    "              f.write('\\n')\n",
    "      # save reply_ids\n",
    "        with open(reply_txt_file,'w') as f:\n",
    "          for i in reply_ids:\n",
    "            f.write(i)\n",
    "            f.write('\\n')\n",
    "        index += 1\n",
    "        source_ids = []\n",
    "        reply_ids = []\n",
    "      elif i == len(ids) - 1:\n",
    "        source_txt_file = txt_file.split('.')[0] + f'_source_data_{index}.txt'\n",
    "        print(source_txt_file)\n",
    "        reply_txt_file = txt_file.split('.')[0] + f'_reply_data_{index}.txt'\n",
    "      # save source_ids\n",
    "        with open(source_txt_file,'w') as f:\n",
    "          for i in source_ids:\n",
    "              f.write(i)\n",
    "              f.write('\\n')\n",
    "      # save reply_ids\n",
    "        with open(reply_txt_file,'w') as f:\n",
    "          for i in reply_ids:\n",
    "            f.write(i)\n",
    "            f.write('\\n')\n",
    "# split_source_reply('dev.data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_source_reply(txt_file):\n",
    "  \"\"\"\n",
    "  txt_file: 'train.data.txt'\n",
    "  \"\"\"\n",
    "  with open(txt_file) as f:\n",
    "      ids = f.readlines()\n",
    "  source_ids = []\n",
    "  reply_ids = []\n",
    "  source_txt_file = txt_file.split('.')[0] + '_source.txt'\n",
    "  reply_txt_file = txt_file.split('.')[0] + '_reply.txt'\n",
    "  for i in range(len(ids)):\n",
    "      source_ids.append(ids[i].split(',')[0].strip())\n",
    "      reply_ids.extend([r.strip() for r in ids[i].split(',')[1:]])\n",
    "# save source_ids\n",
    "  with open(source_txt_file,'w') as f:\n",
    "      for i in source_ids:\n",
    "          f.write(i)\n",
    "          f.write('\\n')\n",
    "# save reply_ids\n",
    "  with open(reply_txt_file,'w') as f:\n",
    "      for i in reply_ids:\n",
    "        f.write(i)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = pd.read_json(path_or_buf=f'./tweepy_data/objects/test_objects/test_source.json', lines=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " 'protected', 'followers_count', 'friends_count', 'listed_count', 'favourites_count', 'geo_enabled', 'verified', 'statuses_count', 'contributors_enabled', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_df['user'].iloc[0]['geo_enabled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['created_at', 'id', 'id_str', 'text', 'truncated', 'entities', 'source',\n",
       "       'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
       "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
       "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
       "       'contributors', 'is_quote_status', 'retweet_count', 'favorite_count',\n",
       "       'favorited', 'retweeted', 'lang', 'extended_entities',\n",
       "       'possibly_sensitive', 'possibly_sensitive_appealable',\n",
       "       'quoted_status_id', 'quoted_status_id_str', 'quoted_status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'created_at', 'id', 'text', 'truncated', 'entities', 'source',\n",
    "       'in_reply_to_status_id', 'in_reply_to_status_id_str',\n",
    "       'in_reply_to_user_id', 'in_reply_to_user_id_str',\n",
    "       'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place',\n",
    "       'contributors', 'is_quote_status', 'retweet_count', 'favorite_count',\n",
    "       'favorited', 'retweeted', 'lang', 'extended_entities',\n",
    "       'possibly_sensitive', 'possibly_sensitive_appealable',\n",
    "       'quoted_status_id', 'quoted_status_id_str', 'quoted_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_source_reply(txt_file):\n",
    "  \"\"\"\n",
    "  txt_file: 'train.data.txt'\n",
    "  \"\"\"\n",
    "  with open(txt_file) as f:\n",
    "      ids = f.readlines()\n",
    "  source_ids = []\n",
    "  reply_ids = []\n",
    "  source_txt_file = txt_file.split('.')[0] + '_source.txt'\n",
    "  reply_txt_file = txt_file.split('.')[0] + '_reply.txt'\n",
    "  for i in range(len(ids)):\n",
    "      source_ids.append(ids[i].split(',')[0].strip())\n",
    "      reply_ids.extend([[r.strip(), ids[i].split(',')[0].strip()] for r in ids[i].split(',')[1:]])\n",
    "    #   reply_ids.extend([r.strip() for r in ids[i].split(',')[1:]])\n",
    "# save source_ids\n",
    "  with open(source_txt_file,'w') as f:\n",
    "      for i in source_ids:\n",
    "          f.write(i)\n",
    "          f.write('\\n')\n",
    "# save reply_ids\n",
    "  with open(reply_txt_file,'w') as f:\n",
    "      for i in reply_ids:\n",
    "        f.write(','.join(i))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6THaa5Px6v6b"
   },
   "source": [
    "# 2. Crawl tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CvCIHQI6mwz"
   },
   "outputs": [],
   "source": [
    "# !twarc2 hydrate  dev.txt > dev_reply_data.jsonl\n",
    "# resource: https://scholarslab.github.io/learn-twarc/06-twarc-command-basics.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhcR1aXv62R_"
   },
   "source": [
    "# 3. Get tweet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Qg_VrPBz6gxy"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "@timer('ms')\n",
    "def filter_feature(jsonl_file_name):\n",
    "    \"\"\"\n",
    "    jsonl_file_name: 'dev_source_data.jsonl'\n",
    "    \"\"\"\n",
    "    json_file_name = '_'.join(jsonl_file_name.split('_')[:-1]) + '.json'\n",
    "    json_data = pd.read_json(path_or_buf=jsonl_file_name, lines=True)\n",
    "    data_dict = defaultdict(dict)\n",
    "    for i in range(json_data.shape[0]):\n",
    "        for j in range(len(json_data.data.iloc[i])):\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['text'] = json_data.data.iloc[i][j]['text']\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['reply_count'] = json_data.data.iloc[i][j]['public_metrics']['reply_count']\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['like_count'] = json_data.data.iloc[i][j]['public_metrics']['like_count']\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['retweet_count'] = json_data.data.iloc[i][j]['public_metrics']['retweet_count']\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['quote_count'] = json_data.data.iloc[i][j]['public_metrics']['quote_count']\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['possibly_sensitive'] = json_data.data.iloc[i][j]['possibly_sensitive']\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['created_at'] = json_data.data.iloc[i][j]['created_at'] #add create time\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['user_id'] = json_data.data.iloc[i][j]['author_id'] #add user id\n",
    "            data_dict[json_data.data.iloc[i][j]['id']]['has_url'] = 1 if 'entities' in json_data.data.iloc[i][j] else 0 #add shared url number\n",
    "            \n",
    "\n",
    "    #  convert into json format\n",
    "    dict_json=json.dumps(data_dict)\n",
    "    # save json file\n",
    "    with open(json_file_name, 'w+') as file:\n",
    "        file.write(dict_json)\n",
    "# filter_feature('dev_reply_data.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dump data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call filter_feature in 203.33385467529297 ms\n",
      "Call filter_feature in 1865.3488159179688 ms\n",
      "Call filter_feature in 418.25294494628906 ms\n",
      "Call filter_feature in 5743.713140487671 ms\n"
     ]
    }
   ],
   "source": [
    "jsonls = ['./data/full data/dev_source_data.jsonl',\n",
    "          './data/full data/dev_reply_data.jsonl',\n",
    "          './data/full data/train_source_data.jsonl',\n",
    "          './data/full data/train_reply_data.jsonl']\n",
    "\n",
    "for jsonl in jsonls:\n",
    "    filter_feature(jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get user info\n",
    "#### currently only implement on Source tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call get_user_info in 132.65705108642578 ms\n",
      "Call get_user_info in 304.43310737609863 ms\n"
     ]
    }
   ],
   "source": [
    "@timer('ms')\n",
    "def get_user_info(jsonl_file_name):\n",
    "    \"\"\"\n",
    "    jsonl_file_name: 'dev_source_data.jsonl'\n",
    "    \"\"\"\n",
    "    json_file_name = '_'.join(jsonl_file_name.split('_')[:-1]) + '_userinfo.json'\n",
    "    json_data = pd.read_json(path_or_buf=jsonl_file_name, lines=True)\n",
    "    # collect the user info\n",
    "    info_dict = defaultdict(dict)\n",
    "    for i in range(json_data.shape[0]):\n",
    "        for j in range(len(json_data.includes.iloc[i]['users'])):\n",
    "            info_dict[json_data.includes.iloc[i]['users'][j]['id']]['followers_count'] = json_data.includes.iloc[i]['users'][j]['public_metrics']['followers_count']\n",
    "            info_dict[json_data.includes.iloc[i]['users'][j]['id']]['tweet_count'] = json_data.includes.iloc[i]['users'][j]['public_metrics']['tweet_count']\n",
    "            info_dict[json_data.includes.iloc[i]['users'][j]['id']]['verified'] = json_data.includes.iloc[i]['users'][j]['verified']\n",
    "    #  convert into json format\n",
    "    dict_json=json.dumps(info_dict)\n",
    "    # save json file\n",
    "    with open(json_file_name, 'w+') as file:\n",
    "        file.write(dict_json)\n",
    "    \n",
    "jsonls = ['./data/full data/dev_source_data.jsonl',\n",
    "          './data/full data/train_source_data.jsonl']\n",
    "\n",
    "for jsonl in jsonls:\n",
    "    get_user_info(jsonl)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMHgShQ0IwgSBKh9Qpb89Fv",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "obtain data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
