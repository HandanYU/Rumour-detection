{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"obtain data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMHgShQ0IwgSBKh9Qpb89Fv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. split source reply ids"],"metadata":{"id":"j6ES8bVt7_mL"}},{"cell_type":"code","source":["def split_source_reply(txt_file):\n","  \"\"\"\n","  txt_file: 'train.data.txt'\n","  \"\"\"\n","  with open(txt_file) as f:\n","    ids = f.readlines()\n","  source_ids = []\n","  reply_ids = []\n","  source_txt_file = txt_file.split('.')[0] + '_source_data.txt'\n","  reply_txt_file = txt_file.split('.')[0] + '_reply_data.txt'\n","  for i in range(len(ids)):\n","    source_ids.append(ids[i].split(',')[0].strip())\n","    reply_ids.extend([r.strip() for r in ids[i].split(',')[1:]])\n","  # save source_ids\n","  with open(source_txt_file,'w') as f:\n","    for i in source_ids:\n","      f.write(i)\n","      f.write('\\n')\n","  # save reply_ids\n","  with open(reply_txt_file,'w') as f:\n","    for i in reply_ids:\n","      f.write(i)\n","      f.write('\\n')\n","# split_source_reply('dev.data.txt')"],"metadata":{"id":"aCDBWMxE633r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Crawl tweet"],"metadata":{"id":"6THaa5Px6v6b"}},{"cell_type":"code","source":["# !twarc2 hydrate  dev.txt > dev_reply_data.jsonl\n","# resource: https://scholarslab.github.io/learn-twarc/06-twarc-command-basics.html"],"metadata":{"id":"5CvCIHQI6mwz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Get features"],"metadata":{"id":"XhcR1aXv62R_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qg_VrPBz6gxy"},"outputs":[],"source":["from collections import defaultdict\n","import json\n","import pandas as pd\n","def filter_text_reply_count(jsonl_file_name):\n","    \"\"\"\n","    jsonl_file_name: 'dev_source_data.jsonl'\n","    \"\"\"\n","    json_file_name = '_'.join(jsonl_file_name.split('_')[:-1]) + '.json'\n","    json_data = pd.read_json(path_or_buf=jsonl_file_name, lines=True)\n","    data_dict = defaultdict(dict)\n","    for i in range(json_data.shape[0]):\n","        for j in range(len(json_data.data.iloc[i])):\n","            data_dict[json_data.data.iloc[i][j]['id']]['text'] = json_data.data.iloc[i][j]['text']\n","            data_dict[json_data.data.iloc[i][j]['id']]['reply_count'] = json_data.data.iloc[i][j]['public_metrics']['reply_count']\n","            data_dict[json_data.data.iloc[i][j]['id']]['like_count'] = json_data.data.iloc[i][j]['public_metrics']['like_count']\n","            data_dict[json_data.data.iloc[i][j]['id']]['retweet_count'] = json_data.data.iloc[i][j]['public_metrics']['retweet_count']\n","            data_dict[json_data.data.iloc[i][j]['id']]['quote_count'] = json_data.data.iloc[i][j]['public_metrics']['quote_count']\n","            data_dict[json_data.data.iloc[i][j]['id']]['possibly_sensitive'] = json_data.data.iloc[i][j]['possibly_sensitive']\n","            \n","    #  convert into json format\n","    dict_json=json.dumps(data_dict)\n","    # save json file\n","    with open(json_file_name, 'w+') as file:\n","        file.write(dict_json)\n","# filter_text_reply_count('dev_reply_data.jsonl')"]},{"cell_type":"markdown","source":["# 4. Convert json to dataframe"],"metadata":{"id":"sZWI7VYPDIY0"}},{"cell_type":"code","source":["def json2df(json_file):\n","    \"\"\"\n","    json_file: 'train_reply.json'\n","    \"\"\"\n","    with open(json_file,'r+') as file:\n","        content=file.read()\n","    content=json.loads(content)#将json格式文件转化为python的字典文件\n","    df = pd.DataFrame(content)\n","    df = df.stack()\n","    df = df.unstack(0)\n","    return df\n","# df = json2df('train_reply.json')"],"metadata":{"id":"sbxtCKh2DHmN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JUkRHmHWNs7F"},"execution_count":null,"outputs":[]}]}