{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HandanYU/Rumour-detection/blob/main/Pre_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fY6now2OGTd"
   },
   "source": [
    "- replace_abbreviations\n",
    "- remove url \n",
    "- get the emoji and replace them as words\n",
    "- remove stop words\n",
    "- keep only english letters\n",
    "- stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PYirQl2NPOE6",
    "outputId": "f1cb8b6b-ba64-4306-fb55-7f395c23cb76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=5a92f91dcf134fc24011694b0e97f509edb85899e2f6e4aeb666514621a3bb94\n",
      "  Stored in directory: c:\\users\\trist\\appdata\\local\\pip\\cache\\wheels\\5e\\8c\\80\\c3646df8201ba6f5070297fe3779a4b70265d0bfd961c15302\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-1.7.0\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in c:\\users\\trist\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (8.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\trist\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (4.63.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\trist\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\trist\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\trist\\anaconda3\\envs\\pytorch\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.7\n"
     ]
    }
   ],
   "source": [
    "# !pip install emoji\n",
    "# !pip install nltk\n",
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort raw data by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer('ms')\n",
    "\n",
    "def sort_by_time(raw_file, json_file):\n",
    "    with open(raw_file) as file:\n",
    "        ids = file.readlines()\n",
    "\n",
    "    with open(json_file, 'r+') as file:\n",
    "        content = file.read()\n",
    "        content=json.loads(content)\n",
    "        df = pd.DataFrame(content)\n",
    "        df = df.T\n",
    "\n",
    "    save_name = raw_file[:-4] + '_sorted.txt'\n",
    "    with open(save_name, 'w') as file:\n",
    "        date = pd.Series(pd.DatetimeIndex(df.iloc[:, 6]), index=df.index)\n",
    "        df.drop(['created_at'], axis=1, inplace=True)\n",
    "        df['time'] = date\n",
    "\n",
    "        for id_ in ids:\n",
    "            ids_ = id_.strip().split(',')\n",
    "            source_id = ids_[0]\n",
    "            file.write(source_id)\n",
    "            if len(ids_) > 1:\n",
    "                reply_ids = ids_[1:]\n",
    "                reply_ids[-1] = reply_ids[-1].replace('\\n', '')\n",
    "                valid_ids = [index for index in reply_ids if index in df.index]\n",
    "                sorted_replies = df.loc[valid_ids].sort_values(by='time')\n",
    "                if len(valid_ids) > 0:\n",
    "                    file.write(',')\n",
    "\n",
    "                for i, index in enumerate(sorted_replies.index):\n",
    "                    file.write(index)\n",
    "                    if i != len(sorted_replies.index) - 1:\n",
    "                        file.write(',')\n",
    "\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files = ['train.data.txt', 'dev.data.txt']\n",
    "json_files = ['./filtered data/train_reply.json', './filtered data/dev_reply.json']\n",
    "for raw_file, json_file in zip(raw_files, json_files):\n",
    "    sort_by_time(raw_file, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(raw_files[0]) as file:\n",
    "    train = file.readlines()\n",
    "with open('train.data_sorted.txt') as file:\n",
    "    s = file.readlines()\n",
    "assert len(train) == len(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "d-oy_gqAPf32"
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "from utils import timer\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "# from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 61]\n",
      "[nltk_data]     Connection refused>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "stopword = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NOc33_ivOCT_"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# @timer('ms')\n",
    "# def replace_duplicate_letter(word):\n",
    "#     repeat_reg = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "#     repl = r'\\1\\2\\3'\n",
    "#     if wordnet.synsets(word):  # 判断当前字符串是否是单词\n",
    "#         return word\n",
    "#     repl_word = repeat_reg.sub(repl, word)\n",
    "#     if repl_word != word:\n",
    "#         return replace_duplicate_letter(repl_word)\n",
    "#     else:\n",
    "#         return repl_word\n",
    "  \n",
    "def clean_tweet(content):\n",
    "    from time import strftime\n",
    "    # def compute_num_month(content):\n",
    "    #     month_num = 0\n",
    "    #     month = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"octorber\", \"november\", \"december\", \n",
    "    #             \"jan.\", \"feb.\", \"mar.\", \"apr.\", \"may.\", \"jun.\", \"jul.\", \"aug.\", \"sept.\", \"oct.\", \"nov.\", \"dec.\"]\n",
    "    #     for i in content:\n",
    "    #         if i in month:\n",
    "    #             month_num += 1\n",
    "    #     return month_num\n",
    "    # replace_abbreviations\n",
    "    content = content.lower()\n",
    "    content = re.sub(r\"won't\", \"will not\", content)\n",
    "    content = re.sub(r\"can't\", \"can not\", content)\n",
    "    content = re.sub(r\"cannot\", \"can not\", content)\n",
    "    content = re.sub(r\"n't\", \" not\", content)\n",
    "    content = re.sub(r\"'re\", \" are\", content)\n",
    "    content = re.sub(r\"'s\", \" is\", content)\n",
    "    content = re.sub(r\"'d\", \" would\", content)\n",
    "    content = re.sub(r\"'ll\", \" will\", content)\n",
    "    content = re.sub(r\"'t\", \" not\", content)\n",
    "    content = re.sub(r\"'ve\", \" have\", content)\n",
    "    content = re.sub(r\"'m\", \" am\", content)\n",
    "    content = re.sub(r\".”\", \" \", content)\n",
    "    \n",
    "    # get the number of month be mentioned\n",
    "    # month_num = compute_num_month(content)\n",
    "\n",
    "    # get the number of url\n",
    "    mentioned_url_num = len(re.findall(r'https?://[^ ]+', content))\n",
    "    mentioned_url_num += len(re.findall(r'www.[^ ]+', content))\n",
    "    # get the number of twitter ID be mentioned\n",
    "    id_num = len(re.findall(r'@[A-Za-z0-9_]+', content))\n",
    "    content = re.sub(r'@[A-Za-z0-9_]+', '', content) # remove twitter ID\n",
    "    # remove url \n",
    "    ## http, https\n",
    "    content = re.sub(r'https?://[^ ]+', '', content) \n",
    "    ## www.\n",
    "    content = re.sub(r'www.[^ ]+', '', content)\n",
    "    # get the emoji and replace them as words\n",
    "    emojis = emoji.distinct_emoji_list(content)\n",
    "    for e in emojis:\n",
    "        content = re.sub(e, emoji.demojize(e), content)\n",
    "    content = re.sub('\\w+\\d+\\w+', '', content) # remove the word contains numbers\n",
    "    \n",
    "    content = re.sub(r'[:_!+“\\-=——,$%^\\?\\\\~\\\"\\'@#$%&*<>{}\\[\\]()/]', ' ', content) # remove punctuation, except .\n",
    "    \n",
    "    content = re.sub(r\"\\s+\", \" \", content) # conver multiple spaces as a single space\n",
    "    content = content.strip()\n",
    "    \n",
    "    # remove stop words \n",
    "    # TODO: and keep only english letters\n",
    "    \n",
    "    content = [c for c in content.split(' ') if c not in stopword and c.isalpha()]\n",
    "    # do stemming\n",
    "    content = [stemmer.stem(token.strip()) for token in content]\n",
    "    \n",
    "    return ' '.join(content), mentioned_url_num, id_num \n",
    "    #, month_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "QzSuMwzhPKnd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @timer('ms')\n",
    "def json2df(json_file):\n",
    "    \"\"\"\n",
    "    json_file: 'train_reply.json'\n",
    "    \"\"\"\n",
    "    with open(json_file,'r+') as file:\n",
    "        content = file.read()\n",
    "    content = json.loads(content)\n",
    "    df = pd.DataFrame(content)\n",
    "    df = df.T\n",
    "    return df\n",
    "# df = json2df('train_reply.json')\n",
    "def clean_data(data_type):\n",
    "  \"\"\"\n",
    "  Args: \n",
    "    data_type: 'dev', 'train'\n",
    "  Returns:\n",
    "    source_df, reply_df\n",
    "  \"\"\"\n",
    "  source_df = json2df(f'./data/full data/{data_type}_source.json')\n",
    "  source_df['temp'] = source_df['text'].apply(lambda x: clean_tweet(x))\n",
    "  source_df['text'] = source_df['temp'].apply(lambda x: x[0])\n",
    "  source_df['mentioned_url_num'] = source_df['temp'].apply(lambda x: x[1])\n",
    "  source_df['id_num'] = source_df['temp'].apply(lambda x: x[2])\n",
    "  source_df = source_df.drop(columns='temp')\n",
    "  source_df['tweet_id'] = source_df.index\n",
    "\n",
    "  reply_df = json2df(f'./data/full data/{data_type}_reply.json')\n",
    "  reply_df['temp'] = reply_df['text'].apply(lambda x: clean_tweet(x))\n",
    "  reply_df['text'] = reply_df['temp'].apply(lambda x: x[0])\n",
    "  reply_df['mentioned_url_num'] = reply_df['temp'].apply(lambda x: x[1])\n",
    "  reply_df['id_num'] = reply_df['temp'].apply(lambda x: x[2])\n",
    "  reply_df = reply_df.drop(columns='temp')\n",
    "  reply_df['tweet_id'] = reply_df.index\n",
    "  \n",
    "  return source_df, reply_df\n",
    "# train_source_df, train_reply_df = clean_data('train')\n",
    "\n",
    "# dev_source_df, dev_reply_df = clean_data('dev')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def concat_label(data_type, source_feature_df):\n",
    "    \"\"\"Concat labels on source tweets\n",
    "    data_type: 'dev', 'train'\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=['tweet_id', 'label'])\n",
    "    with open(f'./data/original_data/{data_type}_source.txt', 'r') as f:\n",
    "        ids = f.readlines()\n",
    "    with open(f'./data/original_data/{data_type}.label.txt', 'r') as f:\n",
    "        labels = f.readlines()\n",
    "    df['tweet_id'] = [id.strip() for id in ids]\n",
    "    df['label'] = [label.strip() for label in labels]\n",
    "    df_labels = pd.merge(source_feature_df, df, on='tweet_id', how='left')\n",
    "    df_labels['label'] = df_labels['label'].apply(lambda x: 0 if x == 'nonrumour' else 1)\n",
    "    return df_labels\n",
    "\n",
    "# dev_source_df = concat_label('dev', dev_source_df)\n",
    "# train_source_df = concat_label('train', train_source_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat Sorted reply ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_reply(data_type, source_df):\n",
    "    \"\"\"concat replies on source tweets\n",
    "    data_type: 'dev', 'train'\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=['tweet_id', 'reply'])\n",
    "    with open(f'./data/original_data/{data_type}.data_sorted.txt', 'r') as f:\n",
    "        content = f.readlines()\n",
    "    df['tweet_id'] = [c.split(',')[0].strip() for c in content]\n",
    "    df['reply'] = [','.join([i.strip() for i in c.split(',')[1:]]) for c in content]\n",
    "    source_df = pd.merge(source_df, df, on='tweet_id', how='left')\n",
    "    return source_df\n",
    "\n",
    "# dev_source_df = concat_reply('dev', dev_source_df)\n",
    "# train_source_df = concat_reply('train', train_source_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check whether the created date is on weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_weekday(df):\n",
    "    \"\"\"\n",
    "    df: source_df or reply_df\n",
    "    \"\"\"\n",
    "    df['isoweekday'] = df['created_at'].apply(lambda x: datetime.strptime(x.split('T')[0], '%Y-%m-%d').isoweekday())\n",
    "    df['isweekday'] = df['isoweekday'].apply(lambda x: 1 <= x <= 5)\n",
    "    df = df.drop(columns='isoweekday')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat user info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_user_info(data_type, source_df):\n",
    "    \"\"\"concat user info on source tweets\n",
    "    data_type: 'dev', 'train'\n",
    "    \"\"\"\n",
    "    df = json2df(f'./data/full data/{data_type}_source_userinfo.json')\n",
    "    df['user_id'] = df.index\n",
    "    source_df = pd.merge(source_df, df, on='user_id', how='left')\n",
    "    return source_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat reply's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_reply_info(reply_ids, reply_df, statis_feature):\n",
    "    statistic_dict = dict()   \n",
    "    replies_txt = ''\n",
    "    if reply_ids == '':\n",
    "        return [np.nan] * (len(statis_feature) + 1)\n",
    "    for r in reply_ids.split(','):\n",
    "        # reply_num = len(reply_ids.split(','))\n",
    "        cur_reply_txt = reply_df.loc[r]['text'].strip()\n",
    "        if cur_reply_txt != '':\n",
    "            replies_txt += cur_reply_txt + ' [SEP] ' ## TODO\n",
    "        ## get statistic features\n",
    "        for f in statis_feature:\n",
    "            statistic_dict[f] = statistic_dict.get(f, 0) + int(reply_df.loc[r][f])\n",
    "    res_values = [replies_txt.strip()]\n",
    "    for f in statis_feature:\n",
    "        res_values.append(statistic_dict[f])\n",
    "    return res_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Text features and get statistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(data_type):\n",
    "    source_df, reply_df = clean_data(data_type)\n",
    "    # source_df: \n",
    "    # ['text', 'reply_count', 'like_count', 'retweet_count', 'quote_count',\n",
    "    # 'possibly_sensitive', 'created_at', 'user_id', 'has_url',\n",
    "    # 'mentioned_url_num', 'id_num', 'tweet_id']\n",
    "\n",
    "    # reply_df:\n",
    "    # ['text', 'reply_count', 'like_count', 'retweet_count', 'quote_count',\n",
    "    # 'possibly_sensitive', 'created_at', 'user_id', 'has_url',\n",
    "    # 'mentioned_url_num', 'id_num', 'tweet_id']\n",
    "    \n",
    "    source_df = concat_label(data_type, source_df)\n",
    "    source_df = concat_reply(data_type, source_df)\n",
    "    source_df = check_weekday(source_df)\n",
    "    source_df = concat_user_info(data_type, source_df)\n",
    "    reply_df = check_weekday(reply_df)\n",
    "    # add sentiment score\n",
    "    source_df['senti_score'] = source_df['text'].apply(lambda x: 1 if TextBlob(x).sentiment.polarity > 0 else 0)\n",
    "    reply_df['senti_score'] = reply_df['text'].apply(lambda x: 1 if TextBlob(x).sentiment.polarity > 0 else 0)\n",
    "    reply_df.index = reply_df['tweet_id']\n",
    "    source_df.index = source_df['tweet_id']\n",
    "    # concat replies info to source_df\n",
    "    statis_feature = ['reply_count', 'like_count', 'retweet_count', 'quote_count', \n",
    "                        'possibly_sensitive', 'has_url', 'mentioned_url_num', \n",
    "                        'id_num', 'isweekday', 'senti_score']\n",
    "    source_df[['reply_text'] + ['reply_' + s for s in statis_feature]] = source_df.apply(lambda x: concat_reply_info(x['reply'], reply_df, statis_feature), axis=1, result_type='expand')                \n",
    "    source_df = source_df.loc[~source_df['reply_text'].isnull()]\n",
    "    return source_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = processing('train')\n",
    "dev_df = processing('dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Statistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stat_tweet_feat(df):\n",
    "    # extract statistic features\n",
    "    statistic_features = ['reply_reply_count', 'reply_like_count',\n",
    "       'reply_retweet_count', 'reply_quote_count', 'reply_possibly_sensitive',\n",
    "       'reply_has_url', 'reply_mentioned_url_num', 'reply_id_num',\n",
    "       'reply_isweekday', 'reply_senti_score', 'reply_count', 'like_count', 'retweet_count', 'quote_count',\n",
    "       'possibly_sensitive', 'has_url',\n",
    "       'mentioned_url_num', 'id_num', 'isweekday', 'followers_count', 'tweet_count', 'verified',\n",
    "       'senti_score' ]\n",
    "    stat_feat_df = df[statistic_features]\n",
    "    stat_feat_df.index = df['tweet_id']\n",
    "    tweet_df = df.drop(columns=statistic_features)\n",
    "    tweet_df.index = df['tweet_id']\n",
    "    # convert into float\n",
    "    for col in ['tweet_count', 'followers_count', 'verified']:\n",
    "        stat_feat_df[col] = stat_feat_df[col].apply(lambda x: float(x))\n",
    "    # fill nan using corresponding mean\n",
    "    stat_feat_df = stat_feat_df.fillna(stat_feat_df.mean())\n",
    "    return stat_feat_df, tweet_df\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuhandan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "train_stat_feat_df, train_tweet_df = extract_stat_tweet_feat(train_df)\n",
    "dev_stat_feat_df, dev_tweet_df = extract_stat_tweet_feat(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_feat_df = pd.read_csv('./data/filtered data/test_stat_feat_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_feat_df.index = stat_feat_df['tweet_id']\n",
    "stat_feat_df = stat_feat_df[train_stat_feat_df.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "minmax = preprocessing.MinMaxScaler()\n",
    "train_scaled_stat_feat_df = pd.DataFrame(columns=train_stat_feat_df.columns, index=train_stat_feat_df.index,\n",
    "                                         data=minmax.fit_transform(train_stat_feat_df))\n",
    "test_scaled_stat_feat_df = pd.DataFrame(columns=stat_feat_df.columns, index=stat_feat_df.index,\n",
    "                                       data=minmax.transform(stat_feat_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled_stat_feat_df.to_csv('./data/filtered data/test_stat_feat_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-35c1f039d21e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mminmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train_scaled_stat_feat_df = pd.DataFrame(columns=train_stat_feat_df.columns, index=train_stat_feat_df.index,\n\u001b[0m\u001b[1;32m      4\u001b[0m                                          data=minmax.fit_transform(train_stat_feat_df))\n\u001b[1;32m      5\u001b[0m dev_scaled_stat_feat_df = pd.DataFrame(columns=dev_stat_feat_df.columns, index=dev_stat_feat_df.index,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "minmax = preprocessing.MinMaxScaler()\n",
    "train_scaled_stat_feat_df = pd.DataFrame(columns=train_stat_feat_df.columns, index=train_stat_feat_df.index,\n",
    "                                         data=minmax.fit_transform(train_stat_feat_df))\n",
    "dev_scaled_stat_feat_df = pd.DataFrame(columns=dev_stat_feat_df.columns, index=dev_stat_feat_df.index,\n",
    "                                       data=minmax.transform(dev_stat_feat_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev_tweet_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-13f1fcebeda6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdev_tweet_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/filtered data/dev_tweet_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdev_scaled_stat_feat_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/filtered data/dev_stat_feat_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_tweet_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/filtered data/train_tweet_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_scaled_stat_feat_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/filtered data/train_stat_feat_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_tweet_df' is not defined"
     ]
    }
   ],
   "source": [
    "dev_tweet_df.to_csv('./data/filtered data/dev_tweet_df.csv')\n",
    "dev_scaled_stat_feat_df.to_csv('./data/filtered data/dev_stat_feat_df.csv')\n",
    "train_tweet_df.to_csv('./data/filtered data/train_tweet_df.csv')\n",
    "train_scaled_stat_feat_df.to_csv('./data/filtered data/train_stat_feat_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuhandan/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.8' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dev_scaled_stat_feat_df = pd.read_csv('./data/filtered data/dev_stat_feat_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'reply_reply_count', 'reply_like_count',\n",
       "       'reply_retweet_count', 'reply_quote_count', 'reply_possibly_sensitive',\n",
       "       'reply_has_url', 'reply_mentioned_url_num', 'reply_id_num',\n",
       "       'reply_isweekday', 'reply_senti_score', 'reply_count', 'like_count',\n",
       "       'retweet_count', 'quote_count', 'possibly_sensitive', 'has_url',\n",
       "       'mentioned_url_num', 'id_num', 'isweekday', 'followers_count',\n",
       "       'tweet_count', 'verified', 'senti_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_scaled_stat_feat_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOajrT/dzdArYA2l9SrPGaw",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Pre-processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
