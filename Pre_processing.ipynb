{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HandanYU/Rumour-detection/blob/main/Pre_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fY6now2OGTd"
   },
   "source": [
    "- replace_abbreviations\n",
    "- remove url \n",
    "- get the emoji and replace them as words\n",
    "- remove stop words\n",
    "- keep only english letters\n",
    "- stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PYirQl2NPOE6",
    "outputId": "f1cb8b6b-ba64-4306-fb55-7f395c23cb76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "\u001b[K     |████████████████████████████████| 175 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=d92ce388b522979dfdc5fd450084cc9205015adc90439c093c847eca5a54a52f\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-1.7.0\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-oy_gqAPf32"
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOc33_ivOCT_"
   },
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopword = stopwords.words('english')\n",
    "def clean_tweet(content):\n",
    "    # replace_abbreviations\n",
    "    content = content.lower()\n",
    "    content = re.sub(r\"won\\'t\", \"will not\", content)\n",
    "    content = re.sub(r\"can\\'t\", \"can not\", content)\n",
    "    content = re.sub(r\"cannot\", \"can not\", content)\n",
    "    content = re.sub(r\"n\\'t\", \" not\", content)\n",
    "    content = re.sub(r\"\\'re\", \" are\", content)\n",
    "    content = re.sub(r\"\\'s\", \" is\", content)\n",
    "    content = re.sub(r\"\\'d\", \" would\", content)\n",
    "    content = re.sub(r\"\\'ll\", \" will\", content)\n",
    "    content = re.sub(r\"\\'t\", \" not\", content)\n",
    "    content = re.sub(r\"\\'ve\", \" have\", content)\n",
    "    content = re.sub(r\"\\'m\", \" am\", content)\n",
    "    content = re.sub(r\"\\\"\", \" am\", content)\n",
    "\n",
    "    content = re.sub(r'@[A-Za-z0-9_]+', '', content) # remove twitter ID\n",
    "    # remove url \n",
    "    ## http, https\n",
    "    content = re.sub(r'https?://[^ ]+', '', content) \n",
    "    ## www.\n",
    "    content = re.sub(r'www.[^ ]+', '', content)\n",
    "    # get the emoji and replace them as words\n",
    "    emojis = emoji.distinct_emoji_list(content)\n",
    "    for e in emojis:\n",
    "        content = re.sub(e, emoji.demojize(e), content)\n",
    "    content = re.sub('\\w+\\d+\\w+', '', content) # remove the word contains numbers\n",
    "    content = re.sub(r'[:_.!+-=——,$%^\\.\\?\\\\~\\\"\\'@#$%&*<>{}\\[\\]()/]', ' ', content) # remove punctuation\n",
    "    content = re.sub(r\"\\s+\", \" \", content) # conver multiple spaces as a single space\n",
    "    content = content.strip()\n",
    "    # remove stop words and keep only english letters\n",
    "    content = [c for c in content.split(' ') if c not in stopword and c.isalpha()]\n",
    "    # do stemming\n",
    "    content = [stemmer.stem(token) for token in content]\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzSuMwzhPKnd"
   },
   "outputs": [],
   "source": [
    "def clean_data(json_file):\n",
    "  \"\"\"\n",
    "  json_file: 'train_reply.json'\n",
    "  \"\"\"\n",
    "  with open(json_file, 'r+') as file:\n",
    "      content=file.read()\n",
    "  content=json.loads(content)\n",
    "  df = pd.DataFrame(content)\n",
    "  df = df.stack()\n",
    "  df = df.unstack(0)\n",
    "  df['text'] = df['text'].apply(lambda x: clean_tweet(x))\n",
    "  return df\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOajrT/dzdArYA2l9SrPGaw",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Pre-processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
