{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HandanYU/Rumour-detection/blob/main/Pre_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fY6now2OGTd"
   },
   "source": [
    "- replace_abbreviations\n",
    "- remove url \n",
    "- get the emoji and replace them as words\n",
    "- remove stop words\n",
    "- keep only english letters\n",
    "- stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PYirQl2NPOE6",
    "outputId": "f1cb8b6b-ba64-4306-fb55-7f395c23cb76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=5a92f91dcf134fc24011694b0e97f509edb85899e2f6e4aeb666514621a3bb94\n",
      "  Stored in directory: c:\\users\\trist\\appdata\\local\\pip\\cache\\wheels\\5e\\8c\\80\\c3646df8201ba6f5070297fe3779a4b70265d0bfd961c15302\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-1.7.0\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in c:\\users\\trist\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (8.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\trist\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (4.63.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\trist\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\trist\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\trist\\anaconda3\\envs\\pytorch\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.7\n"
     ]
    }
   ],
   "source": [
    "# !pip install emoji\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "d-oy_gqAPf32"
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "from utils import timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOc33_ivOCT_"
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopword = stopwords.words('english')\n",
    "\n",
    "\n",
    "@timer('ms')\n",
    "def clean_tweet(content):\n",
    "    # replace_abbreviations\n",
    "    content = content.lower()\n",
    "    content = re.sub(r\"won't\", \"will not\", content)\n",
    "    content = re.sub(r\"can't\", \"can not\", content)\n",
    "    content = re.sub(r\"cannot\", \"can not\", content)\n",
    "    content = re.sub(r\"n't\", \" not\", content)\n",
    "    content = re.sub(r\"'re\", \" are\", content)\n",
    "    content = re.sub(r\"'s\", \" is\", content)\n",
    "    content = re.sub(r\"'d\", \" would\", content)\n",
    "    content = re.sub(r\"'ll\", \" will\", content)\n",
    "    content = re.sub(r\"'t\", \" not\", content)\n",
    "    content = re.sub(r\"'ve\", \" have\", content)\n",
    "    content = re.sub(r\"'m\", \" am\", content)\n",
    "\n",
    "    content = re.sub(r'@[A-Za-z0-9_]+', '', content) # remove twitter ID\n",
    "    # remove url \n",
    "    ## http, https\n",
    "    content = re.sub(r'https?://[^ ]+', '', content) \n",
    "    ## www.\n",
    "    content = re.sub(r'www.[^ ]+', '', content)\n",
    "    # get the emoji and replace them as words\n",
    "    emojis = emoji.distinct_emoji_list(content)\n",
    "    for e in emojis:\n",
    "        content = re.sub(e, emoji.demojize(e), content)\n",
    "    content = re.sub('\\w+\\d+\\w+', '', content) # remove the word contains numbers\n",
    "    content = re.sub(r'[:_.!+-=——,$%^\\.\\?\\\\~\\\"\\'@#$%&*<>{}\\[\\]()/]', ' ', content) # remove punctuation\n",
    "    content = re.sub(r\"\\s+\", \" \", content) # conver multiple spaces as a single space\n",
    "    content = content.strip()\n",
    "    # remove stop words and keep only english letters\n",
    "    content = [c for c in content.split(' ') if c not in stopword and c.isalpha()]\n",
    "    # do stemming\n",
    "    content = [stemmer.stem(token) for token in content]\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer('ms')\n",
    "def sort_by_time(df):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzSuMwzhPKnd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@timer('ms')\n",
    "def clean_data(json_file):\n",
    "  \"\"\"\n",
    "  json_file: 'train_reply.json'\n",
    "  \"\"\"\n",
    "    with open(json_file, 'r+') as file:\n",
    "        content=file.read()\n",
    "    content=json.loads(content)\n",
    "    df = pd.DataFrame(content)\n",
    "    df = df.T\n",
    "    df['text'] = df['text'].apply(lambda x: clean_tweet(x))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort raw data by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer('ms')\n",
    "def sort_by_time(raw_file, json_file):\n",
    "    with open(raw_file) as file:\n",
    "        ids = file.readlines()\n",
    "\n",
    "    with open(json_file, 'r+') as file:\n",
    "        content = file.read()\n",
    "        content=json.loads(content)\n",
    "        df = pd.DataFrame(content)\n",
    "        df = df.T\n",
    "\n",
    "    save_name = raw_file[:-4] + '_sorted.txt'\n",
    "    with open(save_name, 'w') as file:\n",
    "        date = pd.Series(pd.DatetimeIndex(df.iloc[:, 6]), index=df.index)\n",
    "        df.drop(['created_at'], axis=1, inplace=True)\n",
    "        df['time'] = date\n",
    "\n",
    "        for id_ in ids:\n",
    "            ids_ = id_.strip().split(',')\n",
    "            source_id = ids_[0]\n",
    "            file.write(source_id)\n",
    "            if len(ids_) > 1:\n",
    "                reply_ids = ids_[1:]\n",
    "                reply_ids[-1] = reply_ids[-1].replace('\\n', '')\n",
    "                valid_ids = [index for index in reply_ids if index in df.index]\n",
    "                sorted_replies = df.loc[valid_ids].sort_values(by='time')\n",
    "                if len(valid_ids) > 0:\n",
    "                    file.write(',')\n",
    "\n",
    "                for i, index in enumerate(sorted_replies.index):\n",
    "                    file.write(index)\n",
    "                    if i != len(sorted_replies.index) - 1:\n",
    "                        file.write(',')\n",
    "\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call sort_by_time in 2248.7099170684814 ms\n",
      "Call sort_by_time in 746.5729713439941 ms\n"
     ]
    }
   ],
   "source": [
    "raw_files = ['train.data.txt', 'dev.data.txt']\n",
    "json_files = ['./filtered data/train_reply.json', './filtered data/dev_reply.json']\n",
    "for raw_file, json_file in zip(raw_files, json_files):\n",
    "    sort_by_time(raw_file, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(raw_files[0]) as file:\n",
    "    train = file.readlines()\n",
    "with open('train.data_sorted.txt') as file:\n",
    "    s = file.readlines()\n",
    "assert len(train) == len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOajrT/dzdArYA2l9SrPGaw",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Pre-processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
