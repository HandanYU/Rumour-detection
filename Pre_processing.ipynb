{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre-processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOajrT/dzdArYA2l9SrPGaw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HandanYU/Rumour-detection/blob/main/Pre_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- replace_abbreviations\n",
        "- remove url \n",
        "- get the emoji and replace them as words\n",
        "- remove stop words\n",
        "- keep only english letters\n",
        "- stemming"
      ],
      "metadata": {
        "id": "6fY6now2OGTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYirQl2NPOE6",
        "outputId": "f1cb8b6b-ba64-4306-fb55-7f395c23cb76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 5.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=d92ce388b522979dfdc5fd450084cc9205015adc90439c093c847eca5a54a52f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk\n",
        "import json\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "d-oy_gqAPf32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOc33_ivOCT_"
      },
      "outputs": [],
      "source": [
        "\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "stopword = stopwords.words('english')\n",
        "def clean_tweet(content):\n",
        "    # replace_abbreviations\n",
        "    content = content.lower()\n",
        "    content = re.sub(r\"won\\'t\", \"will not\", content)\n",
        "    content = re.sub(r\"can\\'t\", \"can not\", content)\n",
        "    content = re.sub(r\"cannot\", \"can not\", content)\n",
        "    content = re.sub(r\"n\\'t\", \" not\", content)\n",
        "    content = re.sub(r\"\\'re\", \" are\", content)\n",
        "    content = re.sub(r\"\\'s\", \" is\", content)\n",
        "    content = re.sub(r\"\\'d\", \" would\", content)\n",
        "    content = re.sub(r\"\\'ll\", \" will\", content)\n",
        "    content = re.sub(r\"\\'t\", \" not\", content)\n",
        "    content = re.sub(r\"\\'ve\", \" have\", content)\n",
        "    content = re.sub(r\"\\'m\", \" am\", content)\n",
        "    content = re.sub(r\"\\\"\", \" am\", content)\n",
        "\n",
        "    content = re.sub(r'@[A-Za-z0-9_]+', '', content) # remove twitter ID\n",
        "    # remove url \n",
        "    ## http, https\n",
        "    content = re.sub(r'https?://[^ ]+', '', content) \n",
        "    ## www.\n",
        "    content = re.sub(r'www.[^ ]+', '', content)\n",
        "    # get the emoji and replace them as words\n",
        "    emojis = emoji.distinct_emoji_list(content)\n",
        "    for e in emojis:\n",
        "        content = re.sub(e, emoji.demojize(e), content)\n",
        "    content = re.sub('\\w+\\d+\\w+', '', content) # remove the word contains numbers\n",
        "    content = re.sub(r'[:_.!+-=——,$%^\\.\\?\\\\~\\\"\\'@#$%&*<>{}\\[\\]()/]', ' ', content) # remove punctuation\n",
        "    content = re.sub(r\"\\s+\", \" \", content) # conver multiple spaces as a single space\n",
        "    content = content.strip()\n",
        "    # remove stop words and keep only english letters\n",
        "    content = [c for c in content.split(' ') if c not in stopword and c.isalpha()]\n",
        "    # do stemming\n",
        "    content = [stemmer.stem(token) for token in content]\n",
        "    \n",
        "    return content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(json_file):\n",
        "  \"\"\"\n",
        "  json_file: 'train_reply.json'\n",
        "  \"\"\"\n",
        "  with open(json_file, 'r+') as file:\n",
        "      content=file.read()\n",
        "  content=json.loads(content)\n",
        "  df = pd.DataFrame(content)\n",
        "  df = df.stack()\n",
        "  df = df.unstack(0)\n",
        "  df['text'] = df['text'].apply(lambda x: clean_tweet(x))\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "QzSuMwzhPKnd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}