{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HandanYU/Rumour-detection/blob/main/Pre_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fY6now2OGTd"
   },
   "source": [
    "- replace_abbreviations\n",
    "- remove url \n",
    "- get the emoji and replace them as words\n",
    "- remove stop words\n",
    "- keep only english letters\n",
    "- stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PYirQl2NPOE6",
    "outputId": "f1cb8b6b-ba64-4306-fb55-7f395c23cb76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=5a92f91dcf134fc24011694b0e97f509edb85899e2f6e4aeb666514621a3bb94\n",
      "  Stored in directory: c:\\users\\trist\\appdata\\local\\pip\\cache\\wheels\\5e\\8c\\80\\c3646df8201ba6f5070297fe3779a4b70265d0bfd961c15302\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-1.7.0\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in c:\\users\\trist\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (8.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\trist\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (4.63.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\trist\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\trist\\anaconda3\\envs\\pytorch\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\trist\\anaconda3\\envs\\pytorch\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.7\n"
     ]
    }
   ],
   "source": [
    "# !pip install emoji\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert json to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json2df(json_file):\n",
    "    \"\"\"\n",
    "    json_file: 'train_reply.json'\n",
    "    \"\"\"\n",
    "    with open(json_file,'r+') as file:\n",
    "        content = file.read()\n",
    "    content = json.loads(content)#将json格式文件转化为python的字典文件\n",
    "    df = pd.DataFrame(content)\n",
    "    df = df.T\n",
    "    return df\n",
    "# df = json2df('train_reply.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort raw data by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer('ms')\n",
    "def sort_by_time(raw_file, json_file):\n",
    "    with open(raw_file) as file:\n",
    "        ids = file.readlines()\n",
    "\n",
    "    with open(json_file, 'r+') as file:\n",
    "        content = file.read()\n",
    "        content=json.loads(content)\n",
    "        df = pd.DataFrame(content)\n",
    "        df = df.T\n",
    "\n",
    "    save_name = raw_file[:-4] + '_sorted.txt'\n",
    "    with open(save_name, 'w') as file:\n",
    "        date = pd.Series(pd.DatetimeIndex(df.iloc[:, 6]), index=df.index)\n",
    "        df.drop(['created_at'], axis=1, inplace=True)\n",
    "        df['time'] = date\n",
    "\n",
    "        for id_ in ids:\n",
    "            ids_ = id_.strip().split(',')\n",
    "            source_id = ids_[0]\n",
    "            file.write(source_id)\n",
    "            if len(ids_) > 1:\n",
    "                reply_ids = ids_[1:]\n",
    "                reply_ids[-1] = reply_ids[-1].replace('\\n', '')\n",
    "                valid_ids = [index for index in reply_ids if index in df.index]\n",
    "                sorted_replies = df.loc[valid_ids].sort_values(by='time')\n",
    "                if len(valid_ids) > 0:\n",
    "                    file.write(',')\n",
    "\n",
    "                for i, index in enumerate(sorted_replies.index):\n",
    "                    file.write(index)\n",
    "                    if i != len(sorted_replies.index) - 1:\n",
    "                        file.write(',')\n",
    "\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files = ['train.data.txt', 'dev.data.txt']\n",
    "json_files = ['./filtered data/train_reply.json', './filtered data/dev_reply.json']\n",
    "for raw_file, json_file in zip(raw_files, json_files):\n",
    "    sort_by_time(raw_file, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(raw_files[0]) as file:\n",
    "    train = file.readlines()\n",
    "with open('train.data_sorted.txt') as file:\n",
    "    s = file.readlines()\n",
    "assert len(train) == len(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "d-oy_gqAPf32"
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "from utils import timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yuhandan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "stopword = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "NOc33_ivOCT_"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "@timer('ms')\n",
    "def clean_tweet(content):\n",
    "    from time import strftime\n",
    "    # def compute_num_month(content):\n",
    "    #     month_num = 0\n",
    "    #     month = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"octorber\", \"november\", \"december\", \n",
    "    #             \"jan.\", \"feb.\", \"mar.\", \"apr.\", \"may.\", \"jun.\", \"jul.\", \"aug.\", \"sept.\", \"oct.\", \"nov.\", \"dec.\"]\n",
    "    #     for i in content:\n",
    "    #         if i in month:\n",
    "    #             month_num += 1\n",
    "    #     return month_num\n",
    "    # replace_abbreviations\n",
    "    content = content.lower()\n",
    "    content = re.sub(r\"won't\", \"will not\", content)\n",
    "    content = re.sub(r\"can't\", \"can not\", content)\n",
    "    content = re.sub(r\"cannot\", \"can not\", content)\n",
    "    content = re.sub(r\"n't\", \" not\", content)\n",
    "    content = re.sub(r\"'re\", \" are\", content)\n",
    "    content = re.sub(r\"'s\", \" is\", content)\n",
    "    content = re.sub(r\"'d\", \" would\", content)\n",
    "    content = re.sub(r\"'ll\", \" will\", content)\n",
    "    content = re.sub(r\"'t\", \" not\", content)\n",
    "    content = re.sub(r\"'ve\", \" have\", content)\n",
    "    content = re.sub(r\"'m\", \" am\", content)\n",
    "    # get the number of month be mentioned\n",
    "    # month_num = compute_num_month(content)\n",
    "\n",
    "    # get the number of url\n",
    "    url_num = len(re.findall(r'https?://[^ ]+', content))\n",
    "    url_num += len(re.findall(r'www.[^ ]+', content))\n",
    "    # get the number of twitter ID be mentioned\n",
    "    id_num = len(re.findall(r'@[A-Za-z0-9_]+', content))\n",
    "    content = re.sub(r'@[A-Za-z0-9_]+', '', content) # remove twitter ID\n",
    "    # remove url \n",
    "    ## http, https\n",
    "    content = re.sub(r'https?://[^ ]+', '', content) \n",
    "    ## www.\n",
    "    content = re.sub(r'www.[^ ]+', '', content)\n",
    "    # get the emoji and replace them as words\n",
    "    emojis = emoji.distinct_emoji_list(content)\n",
    "    for e in emojis:\n",
    "        content = re.sub(e, emoji.demojize(e), content)\n",
    "    content = re.sub('\\w+\\d+\\w+', '', content) # remove the word contains numbers\n",
    "    content = re.sub(r'[:_.!+-=——,$%^\\.\\?\\\\~\\\"\\'@#$%&*<>{}\\[\\]()/]', ' ', content) # remove punctuation\n",
    "    content = re.sub(r\"\\s+\", \" \", content) # conver multiple spaces as a single space\n",
    "    content = content.strip()\n",
    "    # remove stop words and keep only english letters\n",
    "    content = [c for c in content.split(' ') if c not in stopword and c.isalpha()]\n",
    "    # do stemming\n",
    "    content = [stemmer.stem(token) for token in content]\n",
    "    \n",
    "    return content, url_num, id_num \n",
    "    #, month_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "QzSuMwzhPKnd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@timer('ms')\n",
    "def clean_data(data_type):\n",
    "  \"\"\"\n",
    "  Args: \n",
    "    data_type: 'dev', 'train'\n",
    "  Returns:\n",
    "    source_df, reply_df\n",
    "  \"\"\"\n",
    "  with open(f'./data/full data/{data_type}_source.json', 'r+') as file:\n",
    "      source_content = file.read()\n",
    "  source_content = json.loads(source_content)\n",
    "  source_df = pd.DataFrame(source_content)\n",
    "  source_df = source_df.T\n",
    "  source_df['text'] = source_df['text'].apply(lambda x: clean_tweet(x))\n",
    "  \n",
    "  with open(f'./data/full data/{data_type}_reply.json', 'r+') as file:\n",
    "      reply_content = file.read()\n",
    "  reply_content = json.loads(reply_content)\n",
    "  reply_df = pd.DataFrame(reply_content)\n",
    "  reply_df = reply_df.T\n",
    "  reply_df['text'] = reply_df['text'].apply(lambda x: clean_tweet(x))\n",
    "\n",
    "  return source_df, reply_df\n",
    "# train_source_df, train_reply_df = clean_data('train')\n",
    "\n",
    "# dev_source_df, dev_reply_df = clean_data('dev')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def concat_id_label(data_type, source_feature_df):\n",
    "    \"\"\"\n",
    "    data_type: 'dev', 'train'\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=['id', 'label'])\n",
    "    with open(f'./data/original_data/{data_type}_source.txt', 'r') as f:\n",
    "        ids = f.readlines()\n",
    "    with open(f'./data/original_data/{data_type}.label.txt', 'r') as f:\n",
    "        labels = f.readlines()\n",
    "    df['id'] = [id.strip() for id in ids]\n",
    "    df['label'] = [label.strip() for label in labels]\n",
    "    source_feature_df['id'] = source_feature_df.index\n",
    "    df_labels = pd.merge(source_feature_df, df, on='id', how='left')\n",
    "    return df_labels\n",
    "\n",
    "# dev_source_df = concat_id_label('dev', dev_source_df)\n",
    "# train_source_df = concat_id_label('train', train_source_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat Sorted reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_reply(data_type, source_df):\n",
    "    \"\"\"\n",
    "    data_type: 'dev', 'train'\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=['id', 'reply'])\n",
    "    with open(f'./data/original_data/{data_type}.data_sorted.txt', 'r') as f:\n",
    "        content = f.readlines()\n",
    "    df['id'] = [c.split(',')[0].strip() for c in content]\n",
    "    df['reply'] = [','.join([i.strip() for i in c.split(',')[1:]]) for c in content]\n",
    "    source_df = pd.merge(source_df, df, on='id', how='left')\n",
    "    return source_df\n",
    "\n",
    "# dev_source_df = concat_reply('dev', dev_source_df)\n",
    "# train_source_df = concat_reply('train', train_source_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source_df, train_reply_df = clean_data('train')\n",
    "train_source_df = concat_id_label('train', train_source_df)\n",
    "train_source_df = concat_reply('train', train_source_df)\n",
    "\n",
    "dev_source_df, dev_reply_df = clean_data('dev')\n",
    "dev_source_df = concat_id_label('dev', dev_source_df)\n",
    "dev_source_df = concat_reply('dev', dev_source_df)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOajrT/dzdArYA2l9SrPGaw",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Pre-processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
