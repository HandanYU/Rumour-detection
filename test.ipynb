{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diana:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel\n",
    "import torch.optim as optim\n",
    "import time\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, data_type, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # read pre-processed data\n",
    "        self.tweet_df = pd.read_csv(f'./data/{data_type}_tweet_df.csv', usecols=['text', 'reply_text', 'label'])\n",
    "        self.statistic_df = pd.read_csv(f'./data/{data_type}_stat_feat_df.csv').drop(columns='tweet_id')\n",
    "        self.tweet_df['text'] = self.tweet_df['text'].replace(np.nan, '')\n",
    "        self.tweet_df['reply_text'] = self.tweet_df['reply_text'].replace(np.nan, '')\n",
    "        # define tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # DistilBertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    def __len__(self):\n",
    "        return self.tweet_df.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        # source_token_mask = self.tokenizer(self.tweet_df.iloc[idx]['text'], truncation=True, padding='max_length', max_length=self.max_seq_len)\n",
    "        # source_token, source_mask = torch.tensor(source_token_mask['input_ids']), torch.tensor(source_token_mask['attention_mask'])\n",
    "        concat_text = '[CLS] '+ self.tweet_df.iloc[idx]['text'] + ' [SEP] ' + self.tweet_df.iloc[idx]['reply_text']\n",
    "        concat_tokens = self.tokenizer.tokenize(concat_text)\n",
    "        if len(concat_tokens) < self.max_seq_len:\n",
    "          concat_tokens_padded = concat_tokens + ['[PAD]' for _ in range(self.max_seq_len - len(concat_tokens))]\n",
    "        else:\n",
    "          concat_tokens_padded = concat_tokens[:self.max_seq_len-1] + ['[SEP]']\n",
    "        concat_token_ids = self.tokenizer.convert_tokens_to_ids(concat_tokens_padded)\n",
    "        concat_attn_masks = [1 if token != '[PAD]' else 0 for token in concat_tokens_padded]\n",
    "        concat_seg_ids = []\n",
    "        seg_idx = 0\n",
    "        for i in range(len(concat_token_ids)):\n",
    "            concat_seg_ids.append(seg_idx)\n",
    "            if concat_tokens_padded[i] == '[SEP]':\n",
    "                seg_idx += 1\n",
    "        concat_token_ids_tensor = torch.tensor(concat_token_ids)\n",
    "        concat_attn_masks_tensor = torch.tensor(concat_attn_masks)\n",
    "        concat_seg_ids_tensor = torch.tensor(concat_seg_ids)\n",
    "        # pair_token_mask = self.tokenizer(self.tweet_df.iloc[idx]['text'], self.tweet_df.iloc[idx]['reply_text'], truncation='only_second', padding='max_length', max_length=self.max_seq_len)\n",
    "        # pair_tokens_tensor, pair_mask_tensor = torch.tensor(pair_token_mask['input_ids']), torch.tensor(pair_token_mask['attention_mask'])\n",
    "        return concat_token_ids_tensor, concat_attn_masks_tensor, concat_seg_ids_tensor, self.tweet_df.iloc[idx]['label'], torch.Tensor(self.statistic_df.iloc[idx])\n",
    "\n",
    "class RumorClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RumorClassifier, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        self.ffnn = nn.Sequential(nn.Linear(791,128),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3),\n",
    "                                 nn.Linear(128,64),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(0.3),\n",
    "                                  nn.Linear(64,1),\n",
    "                                  nn.Sigmoid()\n",
    "                                 )\n",
    "\n",
    "    def forward(self, seq, attn_masks, seg, stats):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "        \n",
    "        x = torch.cat((cls_rep,stats),dim=1)\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.ffnn(x)\n",
    "\n",
    "        return logits\n",
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = logits.unsqueeze(-1)\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "def evaluate(net, criterion, dataloader, device):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, labels in dataloader:\n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            logits = net(seq)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count\n",
    "if __name__ == '__main__':\n",
    "  train_loader = DataLoader(TweetDataset('train', 256), shuffle=True, batch_size=64, drop_last=True)\n",
    "  dev_loader = DataLoader(TweetDataset('dev', 256), shuffle=True, batch_size=64, drop_last=True)\n",
    "  torch.cuda.empty_cache ()\n",
    "  net = RumorClassifier()\n",
    "  # net = net.to(device)\n",
    "  \n",
    "  criterion = nn.BCELoss()\n",
    "  opti = optim.Adam(net.parameters(), lr = 2e-5)\n",
    "\n",
    "  best_acc = 0\n",
    "  st = time.time()\n",
    "  eps = []\n",
    "  t_loss = []\n",
    "  d_loss = []\n",
    "  for ep in range(5):\n",
    "    eps.append(ep)\n",
    "    net.train()\n",
    "  \n",
    "    for it, (seq, mask, seg, labels,stats) in enumerate(train_loader):\n",
    "        \n",
    "        #Clear gradients\n",
    "        opti.zero_grad()\n",
    "        #Converting these to cuda tensors\n",
    "        # seq, mask, seg, labels = seq.to(device), mask.to(device), seg.to(device), labels.to(device)\n",
    "        \n",
    "        #Obtaining the logits from the model\n",
    "        print(stats.shape)\n",
    "        logits = net(seq, mask, seg, stats)\n",
    "        \n",
    "        #Computing loss\n",
    "        loss = criterion(logits.squeeze(), labels.float())\n",
    "\n",
    "        #Backpropagating the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #Optimization step\n",
    "        opti.step()\n",
    "\n",
    "        if it % 10 == 0:\n",
    "\n",
    "            acc = get_accuracy_from_logits(logits, labels)\n",
    "            print(\"Iteration {} of epoch {} complete. \\n Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "            st = time.time()\n",
    "\n",
    "        \n",
    "    dev_acc, dev_loss = evaluate(net, criterion, dev_loader, 'cpu')\n",
    "    t_loss.append(loss.item())\n",
    "    d_loss.append(dev_loss)\n",
    "    print(\"Development Accuracy: {}; Development Loss: {}\".format(dev_acc, dev_loss))\n",
    "    if dev_acc > best_acc:\n",
    "        # print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "        best_acc = dev_acc\n",
    "        torch.save(net.state_dict(), 'bertcls_{}.dat'.format(ep))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
